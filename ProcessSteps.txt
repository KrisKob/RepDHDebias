What is actually done?

Test effect of frequency on gender subspace:
need: GloVe, "One billion English word bechmark", set of gender word pairs P,
1. Train GloVe on "One billion english word benchmark"
2. for all embeddings of pairs in P: calculate difference vector
3. for all difference vectors: compute pairwise cosine similarity
Alter Dataset
4. sample one gender word pair & select single word
5. from "One billion English word benchmark" sample sentences with that word twice
6. Train GloVe on altered Corpus
7. for all pairs in P: calculate difference vector
8. for all difference vectors: compute pairwise cosine similarity
Compare both similarity matrices, focus on difference in sampled gender pair
9. calculate norm of individual difference vectors
10. calculate cosine similarity between difference vectors

Double Hard Debias:
need: Word embeddings, top 500 Male biased words set Wm, top 500 Female biased words set Wf
1. for all word embeddings: decentralize all words
2. for all decentralized embeddings: compute PCA
3. for all principal components:
   male embedding = decentralized embedding - projected original (?) embedding into direction of PC
   female embedding = decentralized embedding - projected original (?) embedding into direction of PC
   with all new male embeddings: HardDebias
   with all new female embeddings: HardDebias
   for all HardDebiased embeddings: KMeansClustering (2)
   for clustered embeddings: compute gender alignment accuracy
4. store evaluations for each principal components
5. evaluate which PC lead to most random cluster (evaluation smallest (close to 0.5), used second PC)
6. for all decentralized embeddings: remove that PC-direction
7. for all new embeddings: HardDebias

Gender alignment accuracy/ Neighborhood Metric:
need: k (=1000) most biased female and male word's embedding (cosine similarity embedding & gender direction),
1. assign ground truth gender labels: 0 = male, 1 = female
2. run KMeans on embeddings
3. compute alignment score: cluster assignment vs ground truth gender label
4. alignment score = max(a, 1-a)

Calculate k most biased male & female words:
need: Word2Vec, Corpus, set of gender word pairs P
1. Train Word2Vec on Corpus
2. normalize all vectors
3. for all embeddings of pairs in P: calculate difference vector
4. calculate first PC of difference vectors = gender direction g
5. for all word embeddings: calculate cosine similarity embedding and g
6. take k embeddings with max similarity (same direction - one gender ?)
7. take k embeddings with min similarity (opposed direction - other gender ?)

Hard Debias
need: set of gender neutral words N, set of male-female word pairs D (= set of gender word pairs P?),
300-dim GloVe, "2017 January dump of English Wikipedia",
1. Train GloVe on "2017 January dump of English Wikipedia"
2. Get Gender subspace: Singular Value Decomposition
-- Bolukbasi 2016: normalize all embeddings --
   2.1 for all embeddings of pairs in D: mü = embedding = embedding/ 2
   2.2 1/2 * for all computed mü: calculate sum of:
       for all all word embeddings: sum inner product of (embedding - mü : difference vector(?))
   2.3 take first row of SVD
3. for all neutral words' embeddings: projection onto gender direction
4. for all neutral words' embeddings: subtract projection from original vector
-- Bolukbasi 2016: Equalize words

Determine Gender Neutral Words:
needs: linear Classifier (Support Vector Machine), Dataset, dictionary Definitions (Wordnet), set of gender words
1. get word definitions including gender words
2. manually select words truly gender specific !subjective!
3. Train linear Classifier (with regularization parameter 1)
4. run Classifier on Dataset
5. add words to gender specific words that were newly labelled as such by Classifier
6. subtract gender specific words from dataset

Evaluation:
Comparison with: Original GloVe, debiased Gender-Neutral GN-GloVe, GN-GloVe without gender dimension,
Gender preserving GP-GloVe, GP-GN-GloVe, Hard-GloVe, Strong Hard-Glove without gender
Evaluation:
Coreference Resolution:
needs: WinoBias PRO & ANTI sentences, Coreference Resolution Model, OntoNodes 5.0 dataset
1. Train embeddings according to different algorithms
2. for each embeddings: train & test Coreference Resolution Model on OntoNodes data
3. obtain performance measure (accuracy)
4. for each embeddings: train & test Coreference Resolution Model on WinoBias data
5. obtain performance measure (accuracy) on average & for PRO & ANTI sentences individually
6. compute difference in performance between PRO & ANTI sentences
Word Embeddings Association Test:
needs: two sets of target words, two gender attribute sets (=names)
1. compute differential association of target words and gender attribute sets
2. calculate p values
Neighborhood Metric: (used to create Double Hard GloVe so unfair comparison?)
see above:
1. for all embeddings: perform Neighborhood Metric for k = 100/500/1000
2. for all embeddings: perform tNSE projection
Retaining Word Semantics:
Word Analogy:
needs: MSR word analogy task, Google word analogy
1. for all embeddings: perform MSR word analogy task
2. for all embeddings: perform Google word analogy task
3. for each task: compute accuracy (=correct answer assigned max score)
Concept Categorization:
needs: Almubaresh-Poesio, ESSLLI 2008, Battig 1969, BLESS datasets
1. for all embeddings: perform KMeansClustering
2. for each clustering: compute purity (= fraction correctly classified)
