---
title             : "Does Double-Hard Debias Keep its Promises? - A Replication Study"
shorttitle        : "Replication: Wang et al. (2020)"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    email         : "kkobrock@uni-osnabrueck.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - müssen wir natürlich nicht ausfüllen
     
      
  - name          : "Meike Korsten"
    affiliation   : "1"
    role:
      - ...
      
  - name          : "Sonja Börgerding"
    affiliation   : "1"
    role:
      - ...

affiliation:
  - id            : "1"
    institution   : "University of Osnabrück"

  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Just edit the text! There are several websites that help with (r)markdown, e.g. https://rmarkdown.rstudio.com/.
If you don't want to edit directly, but rather comment use:
<!-- KK: This is a comment. It won't be included in the exported file. -->
Cite using: Wang et al. [-@wang2020doublehard] said that... or simply blablabla [@wang2020doublehard]. References must be added to r-references.bib file. 

## Introduction
- shall we include an abstract?
Recent research has shown that word embeddings derived from language corpora inherit human biases. The first seminal study on this topic was by Bolukbasi et al. [-@bolukbasi_2016] who used the word analogy task developed by Mikolov et al. [-@mikolov_2013] to prove that "Man is to Computer Programmer as Woman is to Homemaker" - a clearly gender biased analogical relation derived from a Google News embedding. Caliskan, Bryson and Narayanan [-@caliskan_2017] complement this finding with a large study on human-like semantic biases in Natural Language Processing (NLP) tools. They show in more general that human biases as exhibited in psychological studies using, for example, the Implicit Association Test (IAT) are learned by NLP algorithms designed to construct meaningful word representations. According to Zhao et al. [-@zhao_2018a] these biases propagate to downstream tasks. As pre-trained word embeddings are often used for a lot of more complex NLP tasks and architectures of our everyday life, the biased embeddings bear the risk of proliferating and strengthening existing stereotypes in human cultures.

### Debiasing Embeddings
But how can the problem of biased embeddings be solved? Several researchers have proposed post-processing techniques or algorithm modifications that promise to "debias" the word embeddings obtained by algorithms like word2vec or GloVe [e.g. @bolukbasi_2016, @zhao_2018b, @kaneko_2019]. 
Bolukbasi et al. [-@bolukbasi_2016] have developed an algorithm called Hard Debias that is based on the idea of removing (biased) gender directions from the embedding whilst preserving desired gender relations. For example, the encoded gender of the words "king" and "queen" is given by definition and the relation between "male" and "king" and between "female" and "queen" is desired. On the other hand, words like "nurse" and "doctor" also tend to exhibit relations to a specific gender even though the words do not have a gender-specific meaning but can be used for all genders. The proposed Hard Debias algorithm first identifies a gender subspace of the embedding that best captures the bias. Then a neutralizing step ensures that gender neutral words (like "nurse" and "doctor") are indeed neutral, i.e. zero, in the gender subspace. An equalizing step is then applied in order to ensure that useful relations apply to words from both genders and are not biased towards one gender anymore. For example, the word "babysit" should be equally distant from both "she" and "he". 
Wang et al. [-@wang_2020] build on the Hard Debias algorithm. They have developed the most recent post-processing procedure for debiasing word embeddings called Double Hard Debias. In the following, we aim to replicate the method and results presented by Wang et al. [-@wang_2020]. 

### Motivation
- rough description of the specific study (Wang et al.) & motivation of replication attempt (maybe some general remarks on the need for replication studies)


## Implementation
- task description
- explanation of model and training choices

### Datasets and Preliminaries

### Hard Debias

### Double-Hard Debias


## Evaluation 
- stick to the paper

### Baselines

### Evaluation of Debiasing Performance
### Debiasing in Downstream Applications
#### Coreference Resolution
- no replication possible, no code provided

### Debiasing at Embedding Level
#### The Word Embeddings Association Test (WEAT)
#### Neighborhood Metric
- discuss that this is shady in the discussion part

## Analysis of Retaining Word Semantics
#### Word Analogy
#### Concept Categorization


## Discussion
- analysis of results and evaluation of performance evaluation
- ablation studies (not applicable)
- discuss the results and what could be (partly) replicated and what not


## Conclusion



\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
