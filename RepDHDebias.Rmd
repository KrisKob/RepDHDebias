---
title             : "Does Double-Hard Debias Keep its Promises? - A Replication Study"
shorttitle        : "Replication: Wang et al. (2020)"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
     
  - name          : "Meike Korsten"
    affiliation   : "1"
      
  - name          : "Sonja Börgerding"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Osnabrück"

  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(dplyr)
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

## Introduction
<!--shall we include an abstract?-->
Recent research has shown that word embeddings derived from natural language corpora inherit human biases. The first seminal study on this topic was by @bolukbasi_2016 who used the word analogy task developed by @mikolov2013MSR to prove that "Man is to Computer Programmer as Woman is to Homemaker" - a clearly gender biased analogical relation derived from a word2vec embedding trained on Google News. @caliskan_2017 complement this finding with a large study on human-like semantic biases in Natural Language Processing (NLP) tools. They have shown in more general that human biases as exhibited in psychological studies using, for example, the Implicit Association Test (IAT) [@greenwald_1998] are learned by NLP algorithms designed to construct meaningful word representations. According to @zhao_2018a these biases propagate to downstream tasks. As pre-trained word embeddings are often used for a lot of more complex NLP tasks and architectures of our everyday life, the biased embeddings bear the risk of proliferating and strengthening existing stereotypes in human cultures.

### Debiasing Embeddings
But how can the problem of biased embeddings be solved? Several researchers have proposed post-processing techniques or algorithm modifications that promise to "debias" the word embeddings obtained by algorithms like word2vec or GloVe [e.g. @bolukbasi_2016; @zhao_2018b; @kaneko_2019]. 
@bolukbasi_2016 have developed an algorithm called Hard Debias that is based on the idea of removing (biased) gender directions from the embedding whilst preserving desired gender relations. For example, the encoded gender of the words "king" and "queen" is given by definition. So the relation between the concept *male* and "king" and between *female* and "queen" is desired. On the other hand, words like "nurse" and "doctor" also tend to exhibit relations to a specific gender in semantic space even though the words do not have a gender-specific meaning but can be used for all genders. The proposed Hard Debias algorithm first identifies a gender subspace of the embedding that best captures the bias. Then a neutralizing step ensures that gender neutral words (like "nurse" and "doctor") are indeed neutral, i.e. zero, in the gender subspace. An equalizing step is then applied in order to ensure that useful relations apply to words from both genders and are not biased towards one gender anymore. For example, the word "babysit" should be equally distant from both "she" and "he". 
@wang_2020 build on Hard Debias in their newly proposed Double Hard Debias algorithm. The main idea is to not only remove the gender direction of the biased embedding, but also the frequency direction as it has been shown that word frequency has a significant impact on the geometry of the embedding space [e.g. @gong_2018; @mu_2018, more on this later].

### Motivation
In this project for the course "Implementing ANNs with Tensorflow", we aimed to replicate the debiasing method presented by @wang_2020 and to reproduce their experimental results. We chose the paper because it is the most recent one that proposes a post-processing technique for debiasing algorithms and because it builds on the seminal paper by @bolukbasi_2016.
In the following, we would like to quickly motivate our choice of topic by relating it to the course and stating a motivation for replication and reproduction attempts in general. 
The course covered a huge breadth of topics ranging from the simplest neural network architectures to advanced Convolutional Neural Networks (CNN) and recently proposed Transformer models [@vaswani_2017]. One of the topics covered that stroke us the most interesting was Deep Learning for Natural Language Processing (NLP) covering word embeddings, language models as well as Seq2seq models and preprocessing techniques. So our aim was to find a final project that would combine knowledge on NLP that we learned in the course with some interesting topic that would motivate our specific choice of project. The extra session on "Ethical aspects of ML" held by Pascal and Annie was a great starting point and we decided to settle on bias in word embeddings as a topic. Our search on papers proposing debiasing algorithms led us to the work described above and we decided to do re-implement the paper by @wang_2020. This gave us the opportunity to apply a variety of skills learned in the course ranging from preprocessing datasets for NLP tasks, over skillfully working with word embeddings, to understanding, fine-tuning and training a NN model that we did not know before (GloVe).
The reproduction of existing results is not only good practice in science, but it is also essential for gaining a deeper understanding on the methods used and it can help to validate existing results or to shed well-grounded doubt on them where needed. Recent studies of reproducibility in the field of Computer Science [e.g. @collberg_2015] and NLP [e.g. @fokkens_2013; @mieskes_2017; @cohen_2018; @belz_2021] explain why reproducibility endeavours are often failing and shed light on the exact nature of the 'reproducibility crisis' [term coined by e.g. @baker_2015] in NLP research. @belz_2021, for example, report that the community's interest in topics of reproducibility have risen even though reproduction attempts still tend to fail due to problems like missing data, missing code and incomplete documentation [see also @fokkens_2013; @mieskes_2019]. 
This project aims to make a contribution to the increasing body of reproducibility and replication attempts in NLP research.


## Implementation
- task description
- explanation of model and training choices

### Pilot Study: Impact of Word Frequency (Meike)
@wang_2020 claim, that the encoding of word frequency in embeddings also significantly influences the encoding of gender direction, thereby diminishing the effectiveness of debiasing algorithms. To ground their theory, a short pilot study is performed, in which word frequencies are artificially changed and changes in the resulting embeddings are investigated. 
Specifically, they look at the difference vectors of the "definitional pairs", which should approximate the gender direction [@wang_2020].
We could not replicate this study as closely to the paper as the other parts, due to constraints on our hardware, therefore leading to similar, but definitely less pronounced results. Ultimately though, even based on limited ressources, we could replicate that manually changing the word frequency statistics for a single word significantly influences the resulting gender direction of the embedding space. Thus, supporting the approach to put more emphasis on word frequency in the scope of debiasing.

### Datasets and Preliminaries (Sonja)
As dataset we used the pre-trained, 300-dimensional GloVe embedding used and provided via link on Github by @wang_2020. Further they also provided several more datasets that were obtained through different debiasing methods, some applied during training, some after. The authors] used these datasets as baselines for the evaluation to compare the perfomance of their Double-Hard Debias method to. Lastly they also provide the embedding that is the reult of applying their Double-Hard Debias alogrithm to the original, non-debiased GloVe embedding. We included all of these embeddings in our evaluation to compore to the result of our implementation of Double-Hard Debias.

The original files included 300-dimensional embeddings for 322.636 words. To avoid large downloads we decided to open the files via URL, however this was not possible for the Double-Hard Debiased embedding obtained by @wang_2020, therefor this embedding needs to be downloaded and open seperatly. From all the provided files we created for each embedding a vocabulary in form of a list, a dictionary mapping all words in the vocabulary to an ID and an embedding matrix that stores the 300-dimensional embedding of each word in the row corresponding to the ID of the word. During the pre-processing we restricted the vocabularies to the 50.000 most common words, which corresponds to the first 50.000 words in a GloVe embedding. At this point we also excluded any words that contain digits or special characters from our vocabularies. Both these actions follow the implementation provided by @wang_2020, however the restriction to 50.000 words is not documented and apparently only happens in the code demonstrating the Double-Hard debiasing method for computational reasons. We adopted this restriction for computational reasons as well as the hope to remove some less frequent words such as names from our embedding to obtain more general results. However unlike the authors we did not specifically remove any non lower case words as the GloVe embedding is lower-cased by default and we also refrained from excluding all words longer than 20 characters.

We made use of a number of word sets provided by @wang_2020. For the application of Hard Debias as proposed by @bolukbasi_2016 we need sets of male and female words as well as a set of neutral words. We will later explain how we obtained the sets of gendered words based on determining the gender direction. For this we used the supplied "definitional_pairs", a word set including pairs such as *he* and *she*. To obtain the neutral set, we removed all words found in the files provided by @wang_2020 from our vocabulary. These files included besides the aforementioned "definitional_pairs" also "equalize_pairs", "gender_specific_full", "female_word_file" and "male_word_file" in the assumption that all these files include definitionally gendered words  that should not be included in the neutral set. However it stays unclear for some of these sets where @wang_2020 obtained them, except for "definitional_pairs" and "equalize_pairs" which are the gender pairs and equality sets as suggested by @bolukbasi_2016. Since the "male_word_file" and the "female_word_file" include pairs of corresponding words such as *spokesman* and *spokeswoman* or *priest* and *nun* we also created a set of pairs based on these two files to be used with the debiasing algorithm.


### Hard Debias (Kristina)
The authors make use of the Hard Debias algorithm proposed by @bolukbasi_2016. The  paper at hand does not give much information on the exact implementation of Hard Debias used. The code uploaded to the authors' Github repository is not well documented, so we were not able to find the exact parts of the algorithm that should refer to the implementation of Hard Debias. That is why we sticked to the original paper from @bolukbasi_2016 in order to re-implement Hard Debias. 

The paper describes two steps: First, the gender direction (or, more generally, the subspace) has to be identified. This is achieved with the help of defining sets $D_1, D_2, ..., D_n \subset W$ which consist of *gender specific* words, i.e. words which are associated with a gender by definition like "girl, boy" or "she, he". These are the words that can help to identify the gender direction by capturing the concept *female, male* in the embedding $\{\vec{w}\in\mathbb{R}^d\}_{w\in W}$. Whereas in some simple implementations of Hard Debias, only one definitional pair might be used, @bolukbasi_2016 suggest to compute the gender direction $B$ across multiple pairs to more robustly estimate the bias. In our implementation we used the 10 word pairs suggested by @bolukbasi_2016 which were experimentally shown to agree with an intuitive concept of gender. For these 10 pairs the principal components (PCs) are calculated and the bias subspace $B$ is made of the first $k \geq 1$ rows of the decomposition SVD($C$). According to @bolukbasi_2016, the first eigenvalue is significantly larger than the rest and so the top PC is hypothesized to capture the gender subspace. So $k=1$ is chosen and our resulting gender subspace $B$ is thus simply a direction. C is calculated in the following way: $$C:=\sum_{i=1}^n \sum_{w\in D_i}(\vec{w}-\mu_i)^T(\vec{w}-\mu_i)/|D_i|$$ where $\mu_i := \sum_{w\in D_i}\vec{w}/|D_i|$ are the means of the defining sets $D_1, D_2, ..., D_n \subset W$.

As a second step Hard Debias neutralizes and equalizes the word embeddings. Neutralizing means to transform each word embedding $\vec{w}$ such that every word $w\in N$ has zero projection in the gender subspace. So for each word $w\in N$ in a set of neutral words $N \subseteq W$, we re-embed $\vec{w}$: $$\vec{w}:=(\vec{w}-\vec{w}_B)/||\vec{w}-\vec{w}_B||$$. The equalize step ensures that desired analogical properties hold for both female and male words contained in the equality sets $\mathcal{E}=\{E_1,E_2,...,E_m\}$ where each $E_i \subseteq W$. For example, after debiasing we would like the embeddings of the pair $E=\{grandmother, grandfather\}$ contained in the equality sets to be equidistant from the embedding of "babysit" [@bolukbasi_2016]. This is enforced by equating each set of words $E\in \mathcal{E}$ outside of $B$ to their simple average $\nu:=\mu-\mu_B$ where $\mu:=\sum_{w\in E}w/|E|$ before adjusting vectors so that they are unit length. So for each word $w\in E$, $\vec{w}$ is re-embedded to $$\vec{w}:=\nu+\sqrt{1-||\nu||^2}\frac{\vec{w}_B-\mu_B}{||\vec{w}_B-\mu_B||}$$.
With the help of the original paper, we were successfully able to re-implement Hard Debias.


### Double-Hard Debias (Meike)
As previously mentioned, the code provided by the authors' was rather unsatisfactory and superficial, which is why we were mostly guided by the Pseudocode provided in the paper [@wang_2020]. 

The Double-Hard Debias algorithm is about removing two different directions from the embeddings. The first one being one supposedly encoding frequency information, the second one being the gender direction, as proposed by Bolukbasi et al. [-@bolukbasi_2016].
In earlier work, Mu and Viswanath [-@mu_2018] have shown, that further dimension reduction of word embeddings actually increases the linguistic regularities they capture. To do so, they first subtracted the common mean vector and then identified a few top principal components of the embedding space. After removing both of those components, the resulting word embeddings proof to serve as stronger representations. During the application of their algorithm, Mu and Viswanath [-@mu_2018] noticed, that some the top PCA directions seemed to encode frequency to a significant degree. 
Wang et al. [-@wang_2020] base their algorithm mostly on those findings and also apply PCA to their algorithm, claiming that to be a feasible method to identify directions of the embedding space encoding word frequency. 
The first part of the algorithm represents picking the frequency direction that shall ultimatly be removed from all embeddings.This is done in a trial-based set up on only the most biased words (500 male and female). Following Mu and Viswanath's [-@mu_2018] work, the common mean vector is removed before PCA is applied.
In the Pseudocode presented, the authors depict the computation of the same number of principal components as the dimensionality that the embeddings in question have. In the provided code, on the other hand, they only compute the top 10 principal components, which is what we ultimately replicated.
For each of those PCs, the corresponding direction is removed, Hard Debias is applied on those projected embeddings, and performance of the resulting embeddings on the Neighborhood Metric, originally proposed by Gonen and Goldberg [-@gonen_2019], is measured. This Metric and its use is discussed in a more detailed manner in the evaluation part. 
Then, the PC is identified that lead to the best performance on the Neighborhood Metric, here meaning resulting in least biased embeddings. 
This direction, supposedly encoding frequency as well as gender information, is picked out of all candidate principal components for the second part of the algorithm.
This second part focuses on truly debiasing the full set of word embeddings, first by removing the recently hand-picked frequency direction and then applying Hard Debias to also remove the gender direction.

## Evaluation 
This part deals with the reproduction of @wang_2020's experimental results when evaluating their Double Hard debiased embedding compared to some baselines and on benchmark datasets using well-established tasks. Due to some code provided on the authors' Github repository, the task of re-implementing the evaluation part of the paper was much more straightforward than implementing the main part.

```{r load dataframes}
# load dataframes (computed in Evaluations.ipynb)
weat_results <- read.csv('evaluation_results/results_weat.csv')
nm_results <- read.csv('evaluation_results/results_nm.csv')
#tsne_results <- read.csv('evaluation_results/results_tsne.csv')
msr_results <- read.csv('evaluation_results/results_msr.csv')
gg_results <- read.csv('evaluation_results/results_gg.csv')
```

### Baselines (Sonja)
Following @wang_2020 we used GloVe embeddings that were obtained using several different debiasing methods as baselines for our evaluation along with the original, non-debiased embedding.

original GloVe: The original, non-debiased GloVe embedding used and provided by @wang_2020. It was obtained from training on the 2017 January dump of English Wikipedia.

GN-GloVe: Gender-Neutral GloVe embedding released by @zhao_2018b. This method restricts gender information in certain dimensions while neutralizing in the remaining dimensions.

GN-GloVe(a): A variant of the Gender-Neutral GloVe embedding obtained by @wang_2020. It was created by excluding the gender dimensions from the GN-GloVe embedding in a try to completly remove gender.

GP-GloVe: Gender preserving GloVe embedding released by @kaneko_2019. This method attempts to remove stereotypical gender bias and preserve non-discriminative gender information.

GP-GN-GloVe: Gender preserving, Gender-Neutral GloVe embedding provided by @kaneko_2019. This is the result of applying gender preserving debiasing to an already debiased GN-GloVe embedding.

Hard-GloVe: Hard debiased GloVe embedding obtained by @wang_2020 following the implementation of @bolukbasi_2016. This method aims to debias neutral words while preserving the gender specific words.

Strong-Hard-GloVe: Strong-Hard debiased GloVe embedding obtained by @wang_2020. A variant of Hard debias during which all words are debiased instead of only the neutral words.

Double-Hard-GloVe(Wang et al.): Double-Hard debiased GloVe embedding obtained by @wang_2020. This is the result of applying their proposed Double-Hard Debias method to the original GloVe embedding.

Double-Hard-GloVe(replication): Double-Hard debiased GloVe embedding obtained by us. This is the result of applying our implementation of the Double-Hard Debias method to the original GloVe embedding.


### Evaluation of Debiasing Performance 
### Debiasing in Downstream Applications
#### Coreference Resolution (Meike)
@wang_2020 also evaluated the performance of their resulting embeddings in Coreference systems. @zhao_2018a have shown that "The physician hired the secretary because he was overwhelmed with clients" is processed better, because "he" relates to "the physician", which is biased with a male gender association. In contrast, non-consistent sentences, such as "The physician hired the secretary because she was overwhelmed with clients" showed poorer performance.
As the code provided by @wang_2020 did not implement this evaluation and we failed to reimplement the original @zhao_2018a Coreference task due to its huge computational demands, we decided to redirect our efforts to other feasible evaluation methods.

### Debiasing at Embedding Level
#### The Word Embeddings Association Test (WEAT) (Sonja)
Following along the evaluation of @wang_2020 we tested the bias of different embeddings with the Word Embedding Association Test(@caliskan_2017). As a permutations test, WEAT takes four sets of words, two sets of so-called target words and two sets of attribute words. It calculates the relative similarity between the target words and the attribute words respectively and outputs in the end how likely it is that this result, the difference between the mean similarities for the different sets, could have been obtained from a non-biased distribution. The two values it returns are the effective size d and the p-value p. A p-value smaller than 0.05 indicates a significant bias and the bias is considered stronger for a larger effective size.

As part of the evaluation and not direct implementation of the paper we decided to use a pre-implemented version of WEAT for our evaluation. However, instead of re-using the code of @wang_2020 we decided for a different WEAT implementation that is oriented on the original paper by @caliskan_2017. 

@wang_2020 conducted WEAT three times for each embedding, once with the bias "Career & Family", once with "Math & Arts" and lastly with "Science & Arts". We replicated all three of these with the word lists taken directly from @caliskan_2017. These include:

1. "Career & Family":
Male names: *John, Paul, Mike, Kevin, Steve, Greg, Jeff, Bill*
Female names: *Amy, Joan, Lisa, Sarah, Diana, Kate, Ann, Donna*
Career words: *executive, management, professional, corporation, salary, office, business, career*
Family words: *home, parents, children, family, cousins, marriage, wedding, relatives*

2. "Math & Arts":
Math words: *math, algebra, geometry, calculus, equations, computation, numbers, addition*
Arts words: *poetry, art, dance, literature, novel, symphony, drama, sculpture*
Male attributes: *male, man, boy, brother, he, him, his, son*
Female attributes: *female, woman, girl, sister, she, her, hers, daughter*

3. "Science & Arts":
Science words: *science, technology, ohysics, chemistry, Einstein, NASA, experiment, astronomy*
Arts words: *poetry, art, Shakespeare, dance, literature, novel, symphony, drama*
Male attributes: *brother, father, uncle, grandfather, son, he, his, him*
Female attributes: *sister, mother, aunt, grandmother, daughter, she, hers, her*

Following @wang_2020, we made one alteration to this set, namely exchanging *Bill* in the male names of the "Career & Family" set for *Tom*. This is to avoid ambiguity due to the lower-casing of GloVe. The results we obtained can be seen in Table 1.

```{r}
options(digits=4)
knitr::kable(weat_results, col.names = c('Embeddings', 'C&F d', 'C&F p', 'M&A d', 'M&A p', 'S&A d', 'S&A p'), caption="WEAT Test")
```

With the WEAT implementation we sued we were able to almost perfectly recreate the results of @wang_2020 for the "Career & Family" word sets. Therefor we assume our implementation to be comparable to that of @wang_2020. Both the effective size and the p-value differ only slightly from the reported values and we can clearly seehat for the Double-Hard debiased embeddings the p-value is larger not only in comparison to the original embedding but also the other debiasing methods. Despite the bias still being significant(p-value > 0.05), it is the least significant for the Double-Hard debiased embedding, which can be inferred from the effective size being the smallest. We were also able to obtain somparable resutls for the Double-Hard debiased embeddings provided by @wang_2020 and for our self-debiased embedding.

However for both "Math & Arts" and "Science & Arts" we obtained values for the effective size and the p-value that partly differ significantly from the values reporter by the authors. However, as in @wang_2020, we can see that the bias in "Math & Arts" was already insignificant in the original GloVe embedding but became even less significant through debiasing. For both word sets the bias appears to be insignificant for both Double-Hard debiased embeddings.


#### Neighborhood Metric (Meike)
The Neighborhood Metric was originally introduced by @gonen_2019 and is based on the observation that even though debiased words no longer show their bias in form of a certain characteristic in the gender direction, the bias remains in the form of the grouping of the embeddings. The supposedly "removed" bias is still manifasted as those words socially-marked with the same gender will be situated closer to one another in the embedding space [@gonen_2019].
When applying K-means clustering on a number of most biased words, the clustering algorithm easily clusters "debiased" embeddings into the correct male and female categories.
@wang_2020 evaluate these clustering performances by simply counting the samples correctly assigned to the gender bias allegedly removed in debiasing. The alignment score $a$ is defined as $a = \frac{1}{2k}\sum_{i=1}^{2k}1[\hat{g_{i}}== g_{i}]$ and set to $a = max(a, 1-a)$ with $k$ resembling the number of samples for each gender, $\hat{g_{i}}$ the estimated and $g_{i}$ the correct label. According to this definition, an alignment value of 0.5 indicates perfectly unbiased embeddings, as the clustering algorithm failes to replicate the gender pattern [@wang_2020].
Even though @wang_2020 make use of this metric within their proposed debiasing algorithm, thereby adjusting their embeddings to obtimize the Neighborhood Metric, they also apply it again in the scope of evaluating their debiasing performance. As one would expect, the Double-Hard debiased embeddings therefore do particularly well in comparison to the other Baseline embeddings.


```{r table 2}
#options(digits=2)
nm_results <- mutate_if(nm_results, is.numeric, ~ . * 100) # get percentages
apa_table(nm_results, col.names = c('Embeddings', 'Top 100', 'Top 500', 'Top 1000'), caption="Neighborhood Metric Clustering Accuracy")
```

@wang_2020 also applied the tSNE reduction technique on the Top 500 female and male biased words to be able to plot the resulting clusters in two-dimensional space. In the graphic below you can see our plot that shows all the baseline datasets, as well as our Double-Hard debiased embedding and @wang_2020's embedding. For the original GloVe embedding, GN-GloVe, GP-GloVe and GP-GN-GloVe the clustering accuracy is still very good meaning that the Top 500 female (lighblue) and male (pink) biased words are still biased after applying the respective debiasing techniques. Our embedding performs equally to the Double-Hard debiased embedding reported by @wang_2020. Hard- and Strong-Hard-GloVe also show comparably little bias.
```{r tsne plot}
knitr::include_graphics("evaluation_results/results_tsne.png")
```

### Analysis of Retaining Word Semantics (Kristina)
One of the most important properties of embeddings is that they represent meaningful word semantics, for example in the form of analogies or concepts. In this section it is tested in how far the embeddings still meet this criterion after debiasing.

#### Word Analogy (Kristina)
The word analogy task was introduced by @mikolov2013MSR. The task is to find the word D such that "A is to B as C is to D". One example for an unbiased analogy is: "Man is to King as Woman is to Queen" whereas a biased analogy would be: "Man is to Computer Programmer as Woman is to Homemaker" [@bolukbasi_2016]. The debiased embeddings are evaluated on two word analogy test sets: the MSR [@mikolov2013MSR] and the Google word analogy task [@mikolov2013Google] in order to find out whether they preserve desired unbiased analogies.

The MSR word analogy dataset contains 8000 syntactic questions in the form presented above. The missing word D is computed by maximizing the cosine similarity between D and C - A + B. The evaluation metric is the percentage of correctly answered questions [see @wang_2020].

The Google word analogy dataset contains 19.544 (**Total**) questions, 8.869 of which are semantic (**Sem**) and 10.675 are syntactic (**Syn**) questions.

```{r table 3}
ana_results <- full_join(msr_results, gg_results)
ana_results <- mutate_if(ana_results, is.numeric, ~ . * 100) # get percentages
apa_table(ana_results, col.names = c('Embeddings', 'MSR', 'Sem', 'Syn', 'Total'), caption="Analogy Tasks")
```
The results can be inspected in Table 3 and are in line with the results reported by @wang_2020. What is interesting is that our replicated Double-Hard debiased embedding scores best in all word analogy tasks and even outperforms the original GloVe embedding.

#### Concept Categorization (Sonja)
@wang_2020 applied this evaluation method to all the different embedding baselines and their Double-Hard debiased embedding as well. However, we found the code provided by the authors to be incomplete. It was missing the application of the Kmeans algorithm as well as the evaluation of this and due to time constraints we decided not to implement this method as we are confident that we already proved the retaining of word semantics with the Word Analogy Task.


## Discussion
- analysis of results and evaluation of performance evaluation
- ablation studies (not applicable)
- discuss the results and what could be (partly) replicated and what not


## Conclusion



\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
