---
title             : "Does Double-Hard Debias Keep its Promises? - A Replication Study"
shorttitle        : "Replication: Wang et al. (2020)"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
     
  - name          : "Meike Korsten"
    affiliation   : "1"
      
  - name          : "Sonja Börgerding"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Osnabrück"

  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(dplyr)
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

## Introduction
Recent research has shown that word embeddings derived from natural language corpora inherit human biases. The first seminal study on this topic was by @bolukbasi_2016 who used the word analogy task developed by @mikolov2013MSR to prove that "Man is to Computer Programmer as Woman is to Homemaker" - a clearly gender biased analogical relation derived from a word2vec embedding trained on Google News. @caliskan_2017 complement this finding with a large study on human-like semantic biases in Natural Language Processing (NLP) tools. They have shown that human biases as exhibited in psychological studies using, for example, the Implicit Association Test (IAT) [@greenwald_1998] <!-- MK: wenn wir weiter kürzen wollen, find ich könnte man hier z.B. auch die Erwähnung von IAT weglassen -->are learned by NLP algorithms designed to construct meaningful word representations. According to @zhao_2018a these biases propagate to downstream tasks. As pre-trained word embeddings are frequently fed into NLP architectures used for more complex tasks  encountered in everyday life like Machine Translation, the biased embeddings bear the risk of proliferating and strengthening existing bias and stereotypes in human cultures.

### Debiasing Embeddings
But how can the problem of biased embeddings be solved? Several researchers have proposed post-processing techniques or algorithm modifications that promise to "debias" the word embeddings obtained by algorithms like word2vec or GloVe [e.g. @bolukbasi_2016; @zhao_2018b; @kaneko_2019]. 
@bolukbasi_2016 have developed an algorithm called Hard Debias that is based on the idea of removing biased gender directions from the embedding whilst preserving desired gender relations. For example, the encoded gender of the words "king" and "queen" is given by definition. So the relation between the concept *male* and "king" and between *female* and "queen" is desired. On the other hand, words like "nurse" and "doctor" also tend to exhibit relations to a specific gender in semantic space even though the words do not have a gender-specific meaning but can be used for all genders. The proposed Hard Debias algorithm first identifies a gender subspace of the embedding that best captures the bias. Subsequent neutralizing and equalizing steps ensure that the debiased embeddings satisfy neutrality constraints without impairing desired properties of the semantic space. 
@wang_2020 build on Hard Debias in their newly proposed Double Hard Debias algorithm. The main idea is to not only remove the gender direction of the biased embedding, but also the frequency direction as it has been shown that word frequency has a significant impact on the geometry of the embedding space [e.g. @gong_2018; @mu_2018, more on this later].

### Motivation
In this project for the course "Implementing ANNs with Tensorflow", we aimed to replicate the debiasing method presented by @wang_2020 and to reproduce their experimental results. We chose the paper because it is the most recent one that proposes a post-processing technique for debiasing algorithms and because it builds on the seminal paper by @bolukbasi_2016.
In the following, we would like to quickly motivate our choice of topic by relating it to the course and stating a motivation for replication and reproduction attempts in general. 
The course covered a huge breadth of topics ranging from the simplest neural network architectures to advanced Convolutional Neural Networks (CNN) and recently proposed Transformer models [@vaswani_2017]. One of the topics covered that stroke us the most interesting was Deep Learning for Natural Language Processing (NLP) covering word embeddings, language models, as well as Seq2seq models and preprocessing techniques. So our aim was to find a final project that would extend on the NLP knowledge gained in the course. The extra session on "Ethical aspects of ML" held by Pascal and Annie was a great starting point and we decided to settle on bias in word embeddings as a topic. Our search on papers proposing debiasing algorithms led us to the work described above and we decided to re-implement the paper by @wang_2020. This gave us the opportunity to apply a variety of skills learned in the course ranging from preprocessing datasets for NLP tasks, over skillfully working with word embeddings, to understanding, fine-tuning and training a NN model that we did not know before (GloVe [@pennington_2014]).
The reproduction of existing results is not only good practice in science, but it is also essential for gaining a deeper understanding on the methods used and it can help to validate existing results or to shed well-grounded doubt on them where needed. Recent studies of reproducibility in the field of Computer Science [e.g. @collberg_2015] and NLP [e.g. @fokkens_2013; @mieskes_2017; @cohen_2018; @belz_2021] explain why reproducibility endeavours are often failing and shed light on the exact nature of the 'reproducibility crisis' [term coined by e.g. @baker_2015] in NLP research. @belz_2021, for example, report that the community's interest in topics of reproducibility have risen even though reproduction attempts still tend to fail due to problems like missing data, missing code and incomplete documentation [see also @fokkens_2013; @mieskes_2019]. 
This project aims to contribute to the increasing body of reproducibility and replication attempts in NLP research.

## Implementation
Shortly after starting the replication attempt, we noticed that the code provided on the [Github repository](https://github.com/uvavision/Double-Hard-Debias) linked in the paper by @wang_2020 was poorly documented and seemed to deviate from the paper's suggestions in some aspects. This is why we sticked closely to the ideas developed in the paper when re-implementing the proposed algorithm. In the following, we will explain in detail what we did and how specific implementation choices were motivated.

### Pilot Study: Impact of Word Frequency
Double-Hard Debias is built on the claim that an embedding's encoding of word frequency significantly influences the same embedding's encoding of gender which can lead to a diminished efficacy of any debiasing algorithms [@wang_2020] <!--KK: Finde, das könnte nochmal näher ausgeführt werden. Inwiefern, warum und was sind hier "debiasing algorithms" - post-processing techniques oder algorithms in general?-->. As interest in debiasing algorithms increased, so did awareness of their weaknesses'. @gonen_2019, for example, noticed that the leading methods barely scrape the surface of removing the full underlying bias. @wang_2020 now propose that specifically the performance of post-processing debiasing methods, such as Hard Debias proposed by @bolukbasi_2016, can be significantly improved by mitigating frequency features before applying them.
To ground this assumption, @wang_2020 perform a short pilot study. They manipulate word frequencies in the original data to investigate the impact of frequency on gender bias contained in the embedding. This is done by sampling twice from specific words contained in the set of definitional pairs introduced by @bolukbasi_2016, i.e. words with a definitional gender whose difference vectors are assumed to approximate the gender direction [@wang_2020].
Word embeddings were trained on a GloVe [@pennington_2014] model implementation (original from [https://github.com/GradySimon/tensorflow-glove]), that was reworked by us. The training used a Minibatch size of 512, a learning rate of 0.05 and the Adam optimizer. We decided to train this model over 50 epochs to create 300-dimensional embeddings, as we work with embeddings of that size in the remaining paper as well. These hyperparameter settings depicted a good trade-off between computational needs and performance on our machines.<!-- Hier fehlt für mich noch: Wir haben GloVe trainiert mit den und den specific model choices etc.-->
We could not replicate the authors' approach as neatly as other parts of the paper due to hardware constraints. This left us with similar, but definitely less pronounced, results. Despite limited resources we were able to reproduce the effect that manually changing the word frequency statistics for a single word significantly influences the resulting gender direction of the embedding space. This supports @wang_2020's conclusion that enhanced emphasis needs to be put on word frequency in debiasing techniques.

### Datasets and Preliminaries
The pre-trained, 300-dimensional GloVe embedding used by @wang_2020 is provided via link on the authors' Github repository. <!--Further they also provided several more datasets that were obtained through different debiasing methods, some applied during training, some after. The authors used these datasets as baselines for the evaluation to compare the perfomance of their Double-Hard Debias method to. Lastly they also provide the embedding that is the result of applying their Double-Hard Debias alogrithm to the original, non-debiased GloVe embedding. We included all of these embeddings in our evaluation to compare to the result of our implementation of Double-Hard Debias.-->
<!--KK: Würde den auskommentierten Teil hier rausnehmen, das gehört mMn nicht direkt zur Implementation, sondern zur Evaluation. MK: und wird meiner Meinung nach da auch genügend erklärt-->
The original files included <!--KK: einmal "300-dimensional" reicht--> embeddings for 322.636 words. For access to embeddings troughout coding we created vocabularies and dictionaries mapping words to Ids. During the pre-processing we followed common steps such as the removal of words containing digits or special characters and a restriction of the vocabularies to the 50.000 most common words, which corresponds to the first 50.000 words in a GloVe embedding.
<!-- KK: Apropos kürzen: Den Teil hier oben finde ich viel zu lang und zu spezifisch. Da würde eigentlich so etwas reichen wie: "Wir haben die folgenden typischen preprocessing steps ausgeführt: ..., Genau den Download beschreiben finde ich zu detailliert. Falls einen das interessiert, findet man das ja schön in unserem Code.-->

We made use of a number of word sets provided by @wang_2020. For the application of Hard Debias as proposed by @bolukbasi_2016 we need sets of male and female words as well as a set of neutral words. We will later explain how we obtained the sets of gendered words based on determining the gender direction. For this we used the supplied "definitional_pairs", a word set including pairs such as *he* and *she*, corresponding to the gender pairs as suggested by @bolukbasi_2016. Further @wang_2020 provided the equality sets for the equalization step of the Hard Debias method(@bolukbasi_2016) in the file "equalize_pairs" To obtain the neutral set, we removed all words found in the files provided by @wang_2020 from our vocabulary. These files included besides the aforementioned "definitional_pairs" and "equalize_pairs" also "gender_specific_full", "female_word_file" and "male_word_file" based on the observation that all these files include definitionally gendered words  that should not be included in the neutral set. However it remains unclear for these remaining sets where @wang_2020 obtained them. Since the "male_word_file" and the "female_word_file" include pairs of corresponding words such as *spokesman* and *spokeswoman* or *priest* and *nun* we also created a set of pairs based on these two files to be used it as an alternative to the equality sets during the debiasing algorithm.
<!--KK: Für den hier oben stehenden Teil würde ich mir eine genaue Aufschlüsselung wünschen, welche Files nun eigentlich was enthalten (und zwar im Kontext der Methode) und wo gebraucht werden. -->
<!--MK: vielleicht könnte man dann den ganzen Abschnitt ein bisschen anders angehen, ala "Among the files used were ... We needed x, which is a set of , for y ..." -->

### Hard Debias
The authors make use of the Hard Debias algorithm proposed by @bolukbasi_2016. The  paper at hand does not give much information on the exact implementation of Hard Debias used. <!--KK: Das dann rausnehmen, weil wir das ja jetzt oben schon angesprochen haben?: The code uploaded to the authors' Github repository is not well documented, so we were not able to find the exact parts of the algorithm that should refer to the implementation of Hard Debias.--> That is why we sticked to the original paper from @bolukbasi_2016 in order to re-implement Hard Debias. 

The paper describes two steps: First, the gender direction (or, more generally, the subspace) has to be identified. This is achieved with the help of defining sets $D_1, D_2, ..., D_n \subset W$ which consist of *gender specific* words, i.e. words which are associated with a gender by definition like "girl, boy" or "she, he". These are the words that can help to identify the gender direction by capturing the concept *female, male* in the embedding $\{\vec{w}\in\mathbb{R}^d\}_{w\in W}$. Whereas in some simple implementations of Hard Debias, only one definitional pair might be used, @bolukbasi_2016 suggest to compute the gender direction $B$ across multiple pairs to more robustly estimate the bias. In our implementation we used the 10 word pairs suggested by @bolukbasi_2016 which were experimentally shown to agree with an intuitive concept of gender [@bolukbasi_2016]. For these 10 pairs the principal components (PCs) are calculated and the bias subspace $B$ is made of the first $k \geq 1$ rows of the decomposition SVD($C$). According to @bolukbasi_2016, the first eigenvalue is significantly larger than the rest and so the top PC is hypothesized to capture the gender subspace. So $k=1$ is chosen and our resulting gender subspace $B$ is thus simply a direction. $C$ is calculated in the following way: $$C:=\sum_{i=1}^n \sum_{w\in D_i}(\vec{w}-\mu_i)^T(\vec{w}-\mu_i)/|D_i|$$ where $\mu_i := \sum_{w\in D_i}\vec{w}/|D_i|$ are the means of the defining sets $D_1, D_2, ..., D_n \subset W$.

As a second step Hard Debias neutralizes and equalizes the word embeddings. Neutralizing means to transform each word embedding $\vec{w}$ such that every word $w\in N$ has zero projection in the gender subspace. So for each word $w\in N$ in a set of neutral words $N \subseteq W$, we re-embed $\vec{w}$: $$\vec{w}:=(\vec{w}-\vec{w}_B)/||\vec{w}-\vec{w}_B||$$. The equalize step ensures that desired analogical properties hold for both female and male words contained in the equality sets $\mathcal{E}=\{E_1,E_2,...,E_m\}$ where each $E_i \subseteq W$. For example, after debiasing we would like the embeddings of the pair $E=\{grandmother, grandfather\}$ contained in the equality sets to be equidistant from the embedding of "babysit". This is enforced by equating each set of words $E\in \mathcal{E}$ outside of $B$ to their simple average $\nu:=\mu-\mu_B$ where $\mu:=\sum_{w\in E}w/|E|$ before adjusting vectors so that they are unit length. So for each word $w\in E$, $\vec{w}$ is re-embedded to $$\vec{w}:=\nu+\sqrt{1-||\nu||^2}\frac{\vec{w}_B-\mu_B}{||\vec{w}_B-\mu_B||}$$.
With the help of the original paper, we were successfully able to re-implement Hard Debias.

### Double-Hard Debias
<!-- KK: Würde ich hier dann auch rausnehmen, weil wir es oben schon haben: As previously mentioned, the code provided by the authors was rather unsatisfactory,which is why we were guided by the Pseudocode provided in the paper [@wang_2020] in the implementation of Double-Hard Debias.-->
Our implementation of Double-Hard Debias was guided by the pseudocode presented in the paper [@wang_2020].
The main idea of the algorithm is to remove two different components from the embedding. The first one supposedly encodes frequency information, the second one captures the gender direction, as proposed already by @bolukbasi_2016.
In earlier work, @mu_2018 have shown that <!-- further -KK: Ich glaube, "increasing" wäre ein bessers Wort als "further" Aber dann haben wir ne Wortdopplung- dimension reduction of word embeddings --> reducing the dimensionality of word embeddings can increase the linguistic regularities they capture. Specifically, they first subtract the common mean vector $\mu = \frac{1}{|V|}\sum_{\vec{w} \in V}\vec{w}$, with $V$ being the set of word embeddings $\vec{w}$, from all embeddings. Then, a few top principal components of the embedding space are identified and removed. The resulting word embeddings actually prove to serve as stronger linguistic representations than the unprocessed ones. @mu_2018 noticed that some of the top PCA directions seemed to encode frequency to a significant degree. @wang_2020 base their debiasing technique on those findings and apply the described steps in their algorithm claiming to apply a method that identifies those directions of the embedding space which encode word frequency.

The first part of Double-Hard Debias is identifying the frequency direction that shall ultimatly be removed from all embeddings. This is done on a on a subset of 500 female and 500 male most biased words by examining different subspace projections and how these influence Hard Debias' results. <!--KK: Ich weiß nicht, was mit trial-based setup gemeint ist.-->. Following @mu_2018, the common mean vector is removed before PCA is applied. In the pseudocode presented, @wang_2020 calculate as many principal components (PCs) as denoted by the dimensionality of the embedding. In the provided code, on the other hand, they only compute the top 20 principal components. As the variance of the components does not encode much information after around 20 PCs, we also decided to compute only those.
For each of those PCs, the corresponding direction $\mathbf{u}$ is removed $w' = w - (\mathbf{u^{T}}w)\mathbf{u}$, Hard Debias is applied, and performance of the resulting embeddings on the Neighborhood Metric [@gonen_2019], is measured. This metric will be discussed in a more detailed manner in the evaluation part. Then, the PC that leads to the best performance on the Neighborhood Metric, i.e. results in least biased embeddings, is identified. This direction, which is hypothesized to encode frequency and to significantly affect gender information, is needed for the second part of the algorithm.
The second part focuses on debiasing the full set of word embeddings. The choosen frequency direction is removed from all embeddings before Hard Debias is applied. By first creating those frequency-mitigated projections removing the gender direction shows to be more successfull. <!-- hier war der komisch komplizierte Satz-->

## Evaluation 
This part deals with the reproduction of @wang_2020's experimental results when evaluating their Double Hard debiased embedding compared to some baselines and on benchmark datasets using well-established tasks. Due to some code provided on the authors' Github repository, the task of re-implementing the evaluation part of the paper was more straightforward than implementing the main part.

```{r load dataframes}
# load dataframes (computed in Evaluations.ipynb)
weat_results <- read.csv('evaluation_results/results_weat.csv')
nm_results <- read.csv('evaluation_results/results_nm.csv')
#tsne_results <- read.csv('evaluation_results/results_tsne.csv')
msr_results <- read.csv('evaluation_results/results_msr.csv')
gg_results <- read.csv('evaluation_results/results_gg.csv')
```

### Baselines
Following @wang_2020 we used GloVe embeddings that were obtained using several different debiasing methods, some applied during training some during post-processing, as baselines for our evaluation along with the original, non-debiased embedding and the Double-Hard debiased embedding obtained by @wang_2020.
<!--KK: Hier ggbfs. den Part von oben einfügen-->

**original GloVe:** the non-debiased GloVe embedding used and provided by @wang_2020, trained on the 2017 January dump of English Wikipedia

<!-- KK: sticking to the order of our implementation:-->
**Double-Hard-GloVe(Wang et al.):** Double-Hard debiased GloVe embedding obtained by @wang_2020 and reported to be the result of their implementation of Double-Hard Debias.

**Double-Hard-GloVe(replication):** Double-Hard debiased GloVe embedding obtained by us. This is the result of applying our implementation of Double-Hard Debias to the original GloVe embedding.

**GN-GloVe:** Gender-Neutral GloVe embedding released by @zhao_2018b. This method restricts gender information in certain dimensions while neutralizing in the remaining dimensions.

**GN-GloVe(a):** A variant of Gender-Neutral GloVe obtained by @wang_2020 created by excluding the gender dimensions from the GN-GloVe embedding in an attempt to completely remove gender.

**GP-GloVe:** Gender preserving GloVe embedding released by @kaneko_2019. This method attempts to remove stereotypical gender bias and preserve non-discriminative gender information.

**GP-GN-GloVe:** Gender preserving, Gender-Neutral GloVe embedding provided by @kaneko_2019. This is the result of applying gender preserving debiasing to an already debiased GN-GloVe embedding.

**Hard-GloVe:** Hard debiased GloVe embedding obtained by @wang_2020 following the implementation of @bolukbasi_2016. This method aims to debias neutral words while preserving the gender specific words.

**Strong-Hard-GloVe:** Strong-Hard debiased GloVe embedding obtained by @wang_2020. A variant of Hard debias which debiases all words instead of only neutral words.

### Evaluation of Debiasing Performance 
@wang_2020 evaluated their embeddings on multiple different tasks to test multiple of the characteristics that embeddings inherit. Even though we were unable to replicate all those tests, we focused on covering the main aspects.

### Debiasing in Downstream Applications
As shortly mentioned above, a main concern regarding bias in word embeddings regards the application of pre-trained word embeddings in downstream tasks. The following method was developed to measure in how far bias contained in a pre-trained embedding influences downstream task performance.
#### Coreference Resolution
@wang_2020 evaluated the performance of their Double-Hard debiased embedding in comparison to the Baselines<!--KK: und die anderen auch alle oder nicht?--> in coreference systems. Bias in coreference models has been shown by @zhao_2018a as models more successfully identify the semantic reference in  "The physician hired the secretary because **he** was overwhelmed with clients" <!--KK: maybe write: "processing is facilitated", by a model?--> due to the biased association of "the physician" to <!-- with?--> the male gender, allowing it to be more readily related to the concept *he*. In contrast, non-consistent sentences such as "The physician hired the secretary because **she** was overwhelmed with clients" showed poorer performance <!--KK: in terms of processing difficulty?-->. The idea is that the societal bias of "the physician" is implemented within its embedding, thereby influencing model's performance. Unbiased embeddings, on the other hand, should lead to equal model performance with both consistent and non-consistent samples.
@wang_2020 themselves neither provide any code for evaluations on the coreference task, neither relate to any external source of the results they present in their paper. Actually, the Coreference Resolution is not mentioned at all in the accompanying material. Trying to implement the original @zhao_2018a coreference task also showed to be not feasible due to its huge computational demands, which ultimately lead to us choosing to also leave it out. Instead, we redirected our efforts to other evaluation methods, that were not only feasible to us, but also documented by the authors.  <!--KK: Finde hier muss weniger drinstehen, dass wir halt kein Bock hatten, nochmal komplett coreference resolution zu programmieren und mehr, dass wang ihren Job nicht gemacht haben, indem sie ordentlichen und vollständigen Code zur Verfügung gestellt hätten.-->

### Debiasing at Embedding Level
Besides evaluating the performance on different NLP tasks, the authors also specifically investigated how much of a bias can be determined in the embeddings themselves.
<!--KK: Hier wird noch ein Satz gebraucht, der debiasing at embedding level erläutert.-->
#### The Word Embeddings Association Test (WEAT) 
Following along the evaluation of @wang_2020 we tested the bias of all embeddings with a permutation test. This Word Embedding Association Test [@caliskan_2017] takes four sets of words, two sets of so-called target words and two sets of attribute words. <!--KK: sehe in diesem Satz nicht, was permutations mit dem input zu tun haben--> It calculates the relative similarity between the target words and the attribute words respectively and outputs how likely it is that this result, the difference between the mean similarities for the different sets,<!--KK: der Einschub hier ist ne unnötige Dopplung--> could have been obtained from a non-biased distribution. The two values it returns are the effective size $d$ and the p-value $p$. A value of $p<0.05$ indicates a significant bias and the bias is considered more pronounced for larger effective sizes.

We used a pre-implemented version of WEAT for our evaluation that can be obtained [https://github.com/shivaomrani/HumanBiasInSemantics]. Instead of re-using the code of @wang_2020 <!--KK: Moment, wang hatte code dazu hochgeladen? Dann muss hier noch genauer erklärt werden, warum wir den nicht benutzt haben--> we opted for a different WEAT implementation that is oriented on the original paper by @caliskan_2017. We decided not to use the authors' original code since it was partly incomplete and poorly understandable. Instead we decided for an implementation that is more compact and where both returned values are obtained simultaniously to avoid potentially incorrect value pairs that could occur based on the fact that this test includes an element of chance and the results usually differ slightly.

@wang_2020 conducted WEAT three times for each embedding, once with the bias "Career & Family", once with "Math & Arts" and lastly with "Science & Arts". We replicated all three of these with the word lists taken directly from @caliskan_2017. Following @wang_2020, we made one alteration to the sets, namely exchanging *Bill* in the male names of the "Career & Family" set for *Tom*. This is to avoid ambiguity due to the lower-casing of GloVe. The exact lists can be found in our implementation of the evaluation. <!-- maybe rather include these in the appendix? KK: Würde ich auch sagen. Beziehungsweise reicht es vielleicht auch, wenn die in unserer implementation sind. -->

 The results we obtained can be seen in Table 1.

```{r}
options(digits=4)
knitr::kable(weat_results, col.names = c('Embeddings', 'C&F d', 'C&F p', 'M&A d', 'M&A p', 'S&A d', 'S&A p'), caption="WEAT Test")
```

With the WEAT implementation we used we were able to almost perfectly reproduce the results of @wang_2020 for the "Career & Family" word sets. <!-- Therefore we assume our implementation to be comparable to that of @wang_2020.--><!--KK: Weiß nicht, ob ich da so genau drauf eingehen würde. Wir haben einen WEAT Test benutzt und fertig. Wir gehen bei der Implementierung ja eh nicht ins Detail--> Both $d$ and $p$ differ only slightly from the reported values and we can clearly see that for the Double-Hard debiased embeddings $p$ is larger not only in comparison to the original embedding but also the other debiasing methods. Despite the bias still being significant ($p$-value > 0.05), it appears to be the least significant for the Double-Hard debiased embedding,<!--KK: sowas gibt es beim Hypothesentesten mit p-values nicht. Entweder es ist significant oder nicht. Damit Rückschlüsse auf den "Grad der Signifikanz" zu ziehen, wäre ich sehr vorsichtig, besonders, wenn man den angewendeten Test nicht voll und ganz durchdrungen hat und versteht. Über die effect size kann man spekulieren, da stimme ich zu. Aber auch am besten als Spekulation formulieren.--> as suggested by the effective size being the smallest for this embedding. We were also able to obtain comparable results for the Double-Hard debiased embedding provided by @wang_2020 and for our self-debiased embedding.

However for both "Math & Arts" and "Science & Arts" we obtained values for the effective size and the p-value that <!-- partly MK: find ich hier ein bisschen doppelt gemoppelt --> differ noticebly <!--KK: Hast du das getestet? :D Nein Spaß. Aber in diesem statistischen Kontext würde ich das Wort "significantly" durch ein anderes ersetzen.--> from the values reported by the authors. However, as in @wang_2020, we can see that the bias in "Math & Arts" was already insignificant in the original GloVe embedding but became even less significant through debiasing. For both word sets the bias appears to be insignificant for both Double-Hard debiased embeddings.


#### Neighborhood Metric
The Neighborhood Metric was originally introduced by @gonen_2019 and is based on the observation that even though debiased words no longer inherit their bias in form of a specific gender direction, it remains in how embeddings are grouped in semantic space. The supposedly "removed" bias is still manifested as one can observe those words socially-marked with the same gender situated closer to one another in the embedding space [@gonen_2019].
K-means clustering on a number of most biased words can often straightforwardly cluster "debiased" embeddings into the correct female and male categories.
@wang_2020 evaluate K-means clustering performance by simply counting the samples correctly assigned to the gender bias allegedly removed in debiasing. The alignment score $a$ is defined as $a = \frac{1}{2k}\sum_{i=1}^{2k}1[\hat{g_{i}}== g_{i}]$ and set to $a = max(a, 1-a)$ with $k$ resembling the number of samples for each gender, $\hat{g_{i}}$ the estimated and $g_{i}$ the correct label. According to this definition, an alignment score value of 0.5 indicates perfectly unbiased embeddings, as the clustering algorithm failed to replicate the original gender pattern [@wang_2020].
Even though @wang_2020 make use of this metric within their debiasing algorithm, adjusting their embeddings to optimize performance on the Neighborhood Metric, they also apply it again in the scope of evaluating their debiasing performance in comparison to other embeddings. As one would expect, the Double-Hard debiased embeddings therefore do particularly well in comparison to the other Baseline embeddings.
<!-- Hard-Glove, Strong-Hard-Glove and both @wang_2020 and our Double-Hard-GloVe embeddings show to lead to nearly bias-free clustering performances, whereby our double-hard debiased embeddings actually slightly outperform the author's ones. The remaining Baseline embeddings on the other hand can nearly perfectly be clustered, indicating remaining bias. Where our results deviate considerably to the ones from the paper is in the performance of (Strong-)Hard-Glove. In our evaluation these also show clustering accuracies alluding to basically no remaining bias, while @wang_2020 present results that would indicate significantly more remaining bias. MK: wäre jetzt erstmal ein Vorschlag, fand' aber eigentlich, dass das im Groben durch die tnse-results erklärt ist -->
<!--KK: Noch ein Satz dazu, wie unsere und die anderen embeddings abschneiden-->


```{r table 2}
nm_results <- mutate_if(nm_results, is.numeric, ~ . * 100) # get percentages
apa_table(nm_results, col.names = c('Embeddings', 'Top 100', 'Top 500', 'Top 1000'), caption="Neighborhood Metric Clustering Accuracy")
```

@wang_2020 also applied the tSNE reduction technique to the top 500 female and male biased words to be able to plot the resulting clusters in two-dimensional space. In the graphic below you can see our plot that shows all the baseline datasets, as well as our Double-Hard debiased embedding and @wang_2020's embedding. For the original GloVe embedding, GN-GloVe, GP-GloVe and GP-GN-GloVe the clustering accuracy is still very high, meaning that the top 500 female (lighblue) and male (pink) biased words are still biased after applying the respective debiasing techniques. Our embedding performs equally to the Double-Hard debiased embedding reported by @wang_2020. Hard- and Strong-Hard-GloVe also show comparably little bias.
```{r tsne plot}
knitr::include_graphics("evaluation_results/results_tsne.png")
```

### Analysis of Retaining Word Semantics
One of the most important properties of embeddings is that they represent meaningful word semantics, for example in the form of analogies or concepts. In this section it is tested in how far the embeddings still meet this criterion after debiasing.

#### Word Analogy (Kristina)
The word analogy task was introduced by @mikolov2013MSR. The task is to find the word D such that "A is to B as C is to D". One example for an unbiased analogy is: "Man is to King as Woman is to Queen" whereas a biased analogy would be: "Man is to Computer Programmer as Woman is to Homemaker" [@bolukbasi_2016]. The debiased embeddings are evaluated on two word analogy test sets, the MSR [@mikolov2013MSR] and the Google word analogy task [@mikolov2013Google], in order to find out whether they preserve desired unbiased analogies.

The MSR word analogy dataset contains 8000 syntactic questions in the form presented above. The missing word D is computed by maximizing the cosine similarity between D and C - A + B. The evaluation metric is the percentage of correctly answered questions [see @wang_2020].

The Google word analogy dataset contains 19.544 (**Total**) questions, 8.869 of which are semantic (**Sem**) and 10.675 are syntactic (**Syn**) questions.

```{r table 3}
ana_results <- full_join(msr_results, gg_results)
ana_results <- mutate_if(ana_results, is.numeric, ~ . * 100) # get percentages
apa_table(ana_results, col.names = c('Embeddings', 'MSR', 'Sem', 'Syn', 'Total'), caption="Analogy Tasks")
```
The results can be inspected in Table 3 and are in line with the results reported by @wang_2020. What is interesting is that our replicated Double-Hard debiased embedding scores best in all word analogy tasks and even outperforms the original GloVe embedding.

#### Concept Categorization
@wang_2020 applied this evaluation method to all the different embedding baselines and their Double-Hard debiased embedding as well. However, we found the code provided by the authors to be incomplete. It was missing major parts <!-- thereby hindering us from adequately replicating its application ? -->  and due to time constraints we decided not to implement this method. <!--KK: letzten Satz würde ich auch rauslassen. Uns ging es ja ums re-implementen, da ist es vollkommen egal, ob wir glauben, dass man das schon mit dem word analogy task gezeigt hat (und im Übrigen zeigen die beiden Tests ja auch völlig unterschiedliche Eigenschaften des semantic space)-->


## Discussion
- analysis of results and evaluation of performance evaluation
- ablation studies (not applicable)
- discuss the results and what could be (partly) replicated and what not


## Conclusion



\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
