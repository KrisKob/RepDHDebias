---
title             : "Does Double-Hard Debias Keep its Promises? - A Replication Study"
shorttitle        : "Replication: Wang et al. (2020)"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
     
  - name          : "Meike Korsten"
    affiliation   : "1"
      
  - name          : "Sonja Börgerding"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Osnabrück"

  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(dplyr)
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

## Introduction
Recent research has shown that word embeddings derived from natural language corpora inherit human biases. The first seminal study on this topic was by @bolukbasi_2016 who used the word analogy task developed by @mikolov2013MSR to prove that "Man is to Computer Programmer as Woman is to Homemaker" - a clearly gender biased analogical relation derived from a word2vec embedding trained on Google News. @caliskan_2017 complement this finding with a large study on human-like semantic biases in Natural Language Processing (NLP) tools. They have shown that human biases as exhibited in psychological studies using, for example, the Implicit Association Test (IAT) [@greenwald_1998] are learned by NLP algorithms designed to construct meaningful word representations. According to @zhao_2018a these biases propagate to downstream tasks. As pre-trained word embeddings are frequently fed into NLP architectures used for more complex tasks  encountered in everyday life like Machine Translation, the biased embeddings bear the risk of proliferating and strengthening existing bias and stereotypes in human cultures.

### Debiasing Embeddings
But how can the problem of biased embeddings be solved? Several researchers have proposed post-processing techniques or algorithm modifications that promise to "debias" the word embeddings obtained by algorithms like word2vec or GloVe [e.g. @bolukbasi_2016; @zhao_2018b; @kaneko_2019]. 
@bolukbasi_2016 have developed an algorithm called Hard Debias that is based on the idea of removing biased gender directions from the embedding whilst preserving desired gender relations. For example, the encoded gender of the words "king" and "queen" is given by definition. So a relation between the concept *male* and "king" and between *female* and "queen" is desired. On the other hand, words like "nurse" and "doctor" also tend to exhibit relations to a specific gender in semantic space, even though the words do not have a gender-specific meaning but can be used for all genders. The proposed Hard Debias algorithm first identifies a gender subspace of the embedding that best captures the bias. Subsequent neutralizing and equalizing steps ensure that the debiased embeddings satisfy neutrality constraints without impairing desired properties of the semantic space. 
@wang_2020 build on Hard Debias in their newly proposed Double-Hard Debias algorithm. The main idea is to not only remove the gender direction of the biased embedding, but also the frequency direction as it has been shown that word frequency has a significant impact on the geometry of the embedding space [e.g. @gong_2018; @mu_2018, more on this later].

### Motivation
In this project for the course "Implementing ANNs with Tensorflow", we aimed to replicate the debiasing method presented by @wang_2020 and to reproduce their experimental results. We chose the paper because it is the most recent one that proposes a post-processing technique for debiasing algorithms and because it builds on the seminal paper by @bolukbasi_2016.
In the following, we would like to quickly motivate our choice of topic by relating it to the course and stating a motivation for replication and reproduction attempts in general. 
The course covered a huge breadth of topics ranging from the simplest neural network architectures to advanced Convolutional Neural Networks (CNN) and recently proposed Transformer models [@vaswani_2017]. One of the topics covered that stroke us the most interesting was Deep Learning for Natural Language Processing (NLP) covering word embeddings, language models, as well as Seq2seq models and preprocessing techniques. So our aim was to find a final project that would extend on the NLP knowledge gained in the course. The extra session on "Ethical aspects of ML" held by Pascal and Annie was a great starting point and we decided to settle on bias in word embeddings as a topic. Our search for papers proposing debiasing algorithms led us to the work described above and we decided to re-implement the paper by @wang_2020. This gave us the opportunity to apply a variety of skills learned in the course ranging from preprocessing datasets for NLP tasks, over skillfully working with word embeddings, to understanding, fine-tuning and training a NN model that we did not know before (GloVe [@pennington_2014]).
The reproduction of existing results is not only good practice in science, but it is also essential for gaining a deeper understanding on the methods used and it can help to validate existing results or to shed well-grounded doubt on them where needed. Recent studies of reproducibility in the field of Computer Science [e.g. @collberg_2015] and NLP [e.g. @fokkens_2013; @mieskes_2017; @cohen_2018; @belz_2021] explain why reproducibility endeavours are often failing and shed light on the exact nature of the 'reproducibility crisis' [term coined by e.g. @baker_2015] in NLP research. @belz_2021, for example, report that the community's interest in topics of reproducibility have risen even though reproduction attempts still tend to fail due to problems like missing data, missing code and incomplete documentation [see also @fokkens_2013; @mieskes_2019]. 
This project aims to contribute to the increasing body of reproducibility and replication attempts in NLP research.

## Implementation
Shortly after starting the replication attempt, we noticed that the code provided on the [Github repository](https://github.com/uvavision/Double-Hard-Debias) linked in the paper by @wang_2020 was poorly documented and seemed to deviate from the paper's suggestions in some aspects. This is why we sticked closely to the ideas developed in the paper when re-implementing the proposed algorithm. In the following, we will explain in detail what we did and how specific implementation choices were motivated.

### Pilot Study: Impact of Word Frequency
Double-Hard Debias is built on the claim that an embedding's encoding of word frequency significantly influences the same embedding's encoding of gender which can lead to a diminished efficacy of any debiasing algorithm [@wang_2020]. As interest in debiasing embeddings increased, so did awareness of the method's weaknesses. @gonen_2019, for example, noticed that the leading methods barely scrape the surface of removing the full bias. @wang_2020 now propose that the performance of debiasing methods, specifically of post-processnig methods such as Hard Debias proposed by @bolukbasi_2016, can be significantly improved by mitigating frequency features before applying them.
To ground this assumption, @wang_2020 perform a short pilot study. They manipulate word frequencies in the original data to investigate the impact of frequency on gender bias contained in the embedding. This is done by sampling certain sentences twice containing specific words of the set of definitional pairs introduced by @bolukbasi_2016, i.e. words with a definitional gender whose difference vectors are assumed to approximate the gender direction [@wang_2020].
Word embeddings were trained on a GloVe [@pennington_2014] model implementation ([original from GradySimon](https://github.com/GradySimon/tensorflow-glove)), that was reworked by us. The training used a minibatch size of 512, a learning rate of 0.05 and the Adam optimizer. We decided to train this model over 50 epochs to create 300-dimensional embeddings, as we work with embeddings of that size in the remaining paper as well. These hyperparameter settings depicted a good trade-off between computational needs and performance on our machines.
We could not replicate the authors' approach as neatly as other parts of the paper due to hardware constraints, which led us to working with a significantly smaller corpus. This left us with similar, but definitely less pronounced, results. Despite limited resources we were able to reproduce the effect that manually changing the word frequency statistics for a single word significantly influences the resulting gender direction of the embedding space. This supports @wang_2020's conclusion that enhanced emphasis needs to be put on word frequency in debiasing techniques.

### Datasets and Preliminaries
The pre-trained, 300-dimensional GloVe embedding used by @wang_2020 is provided via link on the authors' Github repository. The original files included embeddings for 322.636 words. For access to embeddings troughout coding we created vocabularies and dictionaries mapping words to Ids. During the pre-processing we followed common steps such as the removal of words containing digits or special characters and a restriction of the vocabularies to the 50.000 most common words, which corresponds to the first 50.000 words in a GloVe embedding.

We made use of a number of word sets provided by @wang_2020. Among the files used were `definitional_pairs`, `equalize_pairs`, `gender_specific_full`, `male_word_file` and `female_word_file`. The two files `definitional_pairs` and `equalize_pairs` are directly based on the work of @bolukbasi_2016, however for the remaining files it is unclear where the word sets were obtained. The gender pairs needed for the determination of the gender direction during the Hard Debias method [@bolukbasi_2016], were provided in the `definitional_pairs`, i.e. words which are associated with a gender by definition like "girl, boy" or "she, he". The file `equalize_pairs` includes what @bolukbasi_2016 called the equality sets (word pairs such as "Men" and "Women" or "prince" and "princess"), which are needed for the equalization step. As an alternative to this, we created another set of pairs from the `male_word_file` and the `female_word_file` since they also include corresponding words such as "priest" and "nun" or "spokesman" and "spokeswoman". Lastly the Hard Debias algorithm expects to be passed a set of neutral words as well as sets of male and female words [@bolukbasi_2016]. We will later show how the two gendered sets are obtained using the gender direction. To obtain the neutral set, we removed all words found in the five files provided by @wang_2020 from our vocabulary, since these files have shown to include definitionally gendered words.

### Hard Debias
The authors make use of the Hard Debias algorithm proposed by @bolukbasi_2016. The  paper at hand does not give much information on the exact implementation of Hard Debias used, which is why we sticked to the original paper from @bolukbasi_2016 in order to re-implement Hard Debias. 

The paper describes two steps: First, the gender direction (or, more generally, the subspace) has to be identified. This is achieved with the help of defining sets $D_1, D_2, ..., D_n \subset W$ which consist of *gender specific* words. These are the words that can help to identify the gender direction by capturing the concept *female, male* in the embedding $\{\vec{w}\in\mathbb{R}^d\}_{w\in W}$. Whereas in some simple implementations of Hard Debias, only one definitional pair might be used, @bolukbasi_2016 suggest to compute the gender direction $B$ across multiple pairs to more robustly estimate the bias. In our implementation we used the 10 word pairs suggested by @bolukbasi_2016 which were experimentally shown to agree with an intuitive concept of gender [@bolukbasi_2016]. For these 10 pairs the principal components (PCs) are calculated and the bias subspace $B$ is made of the first $k \geq 1$ rows of the decomposition SVD($C$). According to @bolukbasi_2016, the first eigenvalue is significantly larger than the rest and so the top PC is hypothesized to capture the gender subspace. So $k=1$ is chosen and our resulting gender subspace $B$ is thus simply a direction. $C$ is calculated in the following way: $$C:=\sum_{i=1}^n \sum_{w\in D_i}(\vec{w}-\mu_i)^T(\vec{w}-\mu_i)/|D_i|$$ where $\mu_i := \sum_{w\in D_i}\vec{w}/|D_i|$ are the means of the defining sets $D_1, D_2, ..., D_n \subset W$.

As a second step Hard Debias neutralizes and equalizes the word embeddings. Neutralizing means to transform each word embedding $\vec{w}$ such that every word $w\in N$ has zero projection in the gender subspace. So for each word $w\in N$ in a set of neutral words $N \subseteq W$, we re-embed $\vec{w}$: $$\vec{w}:=(\vec{w}-\vec{w}_B)/||\vec{w}-\vec{w}_B||$$. The equalize step ensures that desired analogical properties hold for both female and male words contained in the equality sets $\mathcal{E}=\{E_1,E_2,...,E_m\}$ where each $E_i \subseteq W$. For example, after debiasing we would like the embeddings of the pair $E=\{grandmother, grandfather\}$ contained in the equality sets to be equidistant from the embedding of "babysit". This is enforced by equating each set of words $E\in \mathcal{E}$ outside of $B$ to their simple average $\nu:=\mu-\mu_B$ where $\mu:=\sum_{w\in E}w/|E|$ before adjusting vectors such that they are of unit length. So for each word $w\in E$, $\vec{w}$ is re-embedded to $$\vec{w}:=\nu+\sqrt{1-||\nu||^2}\frac{\vec{w}_B-\mu_B}{||\vec{w}_B-\mu_B||}$$.
With the help of the original @bolukbasi_2016 paper, we were successfully able to re-implement Hard Debias.

### Double-Hard Debias
Our implementation of Double-Hard Debias was guided by the pseudocode presented in the paper [@wang_2020].
The main idea of the algorithm is to remove two different components from the embedding. The first one supposedly encodes frequency information, the second one captures the gender direction, as proposed already by @bolukbasi_2016.
In earlier work, @mu_2018 have shown that reducing the dimensionality of word embeddings can increase the linguistic regularities they capture. Specifically, they first subtract the common mean vector $\mu = \frac{1}{|V|}\sum_{\vec{w} \in V}\vec{w}$, with $V$ being the set of word embeddings $\vec{w}$, from all embeddings. Then, a few top principal components of the embedding space are identified and removed. The resulting word embeddings actually prove to serve as stronger linguistic representations than the unprocessed ones. @mu_2018 noticed that some of the top PCA directions seemed to encode frequency to a significant degree. @wang_2020 base their debiasing technique on those findings and apply the described steps in their algorithm as a method that identifies those directions of the embedding space which encode word frequency.

The first part of Double-Hard Debias is identifying the frequency direction that shall ultimatly be removed from all embeddings. This is done on a on a subset of 500 female and 500 male most biased words by examining different subspace projections and how these influence Hard Debias' results. Following @mu_2018, the common mean vector is removed before PCA is applied. In the pseudocode presented, @wang_2020 calculate as many principal components (PCs) as denoted by the dimensionality of the embedding. In the provided code, on the other hand, they only compute the top 20 principal components. As the components do not encode much variance of the data after around 20 PCs, we also decided to compute only those.
For each of those PCs, the corresponding direction $\mathbf{u}$ is removed $w' = w - (\mathbf{u^{T}}w)\mathbf{u}$, Hard Debias is applied, and performance of the resulting embeddings on the Neighborhood Metric [@gonen_2019], is measured. This metric will be discussed in a more detailed manner in the evaluation part. Then, the PC that leads to the best performance on the Neighborhood Metric, i.e. results in least biased embeddings, is identified. This direction, which is hypothesized to encode frequency and to significantly affect gender information, is needed for the second part of the algorithm.
The second part focuses on debiasing the full set of word embeddings. The choosen frequency direction is removed from all embeddings before Hard Debias is applied. By first creating those frequency-mitigated projections, removing the gender direction shows to be more successful.

## Evaluation 
This part deals with the reproduction of @wang_2020's experimental results when evaluating their Double Hard debiased embedding compared to some baselines and on benchmark datasets using well-established tasks. We could reuse more of the code written by @wang_2020 and provided on the authors' Github repository for the evaluations. 

```{r load dataframes}
# load dataframes (computed in Evaluations.ipynb)
weat_results <- read.csv('evaluation_results/results_weat.csv')
nm_results <- read.csv('evaluation_results/results_nm.csv')
msr_results <- read.csv('evaluation_results/results_msr.csv')
gg_results <- read.csv('evaluation_results/results_gg.csv')
```

### Baselines
Following @wang_2020 we used GloVe embeddings that were obtained using several different debiasing methods, some applied during training some during post-processing, as baselines for our evaluation along with the original, non-debiased embedding and the Double-Hard debiased embedding obtained by @wang_2020. The embeddings were [linked](http://www.cs.virginia.edu/~tw8cb/word_embeddings/) on the Github repository of @wang_2020.

**original GloVe:** The non-debiased GloVe embedding used and provided by @wang_2020, trained on the 2017 January dump of English Wikipedia.

**Double-Hard-GloVe(Wang et al.):** Double-Hard debiased GloVe embedding obtained by @wang_2020 and reported to be the result of their implementation of Double-Hard Debias.

**Double-Hard-GloVe(replication):** Double-Hard debiased GloVe embedding obtained by us. This is the result of applying our implementation of Double-Hard Debias to the original GloVe embedding.

**GN-GloVe:** Gender-Neutral GloVe embedding released by @zhao_2018b. This method restricts gender information in certain dimensions while neutralizing in the remaining dimensions.

**GN-GloVe(a):** A variant of Gender-Neutral GloVe obtained by @wang_2020. Created by excluding the gender dimensions from the GN-GloVe embedding in an attempt to completely remove gender.

**GP-GloVe:** Gender preserving GloVe embedding released by @kaneko_2019. This method attempts to remove stereotypical gender bias and preserve non-discriminative gender information.

**GP-GN-GloVe:** Gender preserving, Gender-Neutral GloVe embedding provided by @kaneko_2019. This is the result of applying gender preserving debiasing to an already debiased GN-GloVe embedding.

**Hard-GloVe:** Hard debiased GloVe embedding obtained by @wang_2020 following the implementation of @bolukbasi_2016. This method aims to debias neutral words while preserving the gender specific words.

**Strong-Hard-GloVe:** Strong-Hard debiased GloVe embedding obtained by @wang_2020. A variant of Hard debias which debiases all words instead of only neutral words.

### Evaluation of Debiasing Performance 
@wang_2020 evaluated their embeddings on different tasks to test multiple of the characteristics that embeddings inherit. Even though we were unable to replicate all those tests, we focused on covering the main aspects.

### Debiasing in Downstream Applications
As shortly mentioned above, a main concern regarding bias in word embeddings regards the application of pre-trained word embeddings in downstream tasks. The following method was developed to measure in how far bias contained in a pre-trained embedding influences downstream task performance.
#### Coreference Resolution
<!--mention vs. readme-->
@wang_2020 evaluated the performance of their Double-Hard debiased embedding in comparison to the baselines in coreference systems. Bias in coreference models has been shown by @zhao_2018a as models more successfully identify the semantic reference in  "The physician hired the secretary because **he** was overwhelmed with clients" due to the biased association of "the physician" with the male gender, allowing it to be more readily related to the concept *he*. In contrast, non-consistent sentences such as "The physician hired the secretary because **she** was overwhelmed with clients" showed poorer performance. The idea is that the societal bias of "the physician" is implemented within its embedding, thereby influencing model's performance. Unbiased embeddings, on the other hand, should lead to equal model performance with both consistent and non-consistent samples.
@wang_2020 themselves neither provide any code for evaluations on the coreference task, nor relate to any external source of the results they present in their paper. Actually, the Coreference Resolution is not mentioned at all in the accompanying material. Trying to implement the original @zhao_2018a coreference task also showed to be not feasible due to its huge computational demands, which ultimately lead to us choosing to leave it out. 

### Debiasing at Embedding Level
Besides evaluating the performance on different downstream NLP tasks, the authors also specifically investigated how much bias can be determined in the embeddings themselves.

#### The Word Embeddings Association Test (WEAT) 
Following along the evaluation of @wang_2020 we tested the bias of all embeddings with a permutation test. This Word Embedding Association Test [@caliskan_2017] takes four sets of words, two sets of so-called target words and two sets of attribute words. It calculates the relative similarity between the target words and the attribute words respectively and outputs how likely it is that this result could have been obtained from a non-biased distribution. The two values it returns are the effective size $d$ and the p-value $p$. A value of $p<0.05$ indicates a significant bias and the bias is considered more pronounced for larger effective sizes.
We used a pre-implemented version of WEAT for our evaluation that can be obtained [here](https://github.com/shivaomrani/HumanBiasInSemantics). Instead of re-using the code of @wang_2020 we opted for a different WEAT implementation that is oriented on the original paper by @caliskan_2017. We decided not to use the authors' original code since it was partly incomplete and poorly understandable. Instead we decided for a more compact implementation where $d$ and $p$ are obtained simultanously. 
@wang_2020 conducted WEAT three times for each embedding, once with the bias *Career & Family*, once with *Math & Arts* and lastly with *Science & Arts*. We replicated all three of these with the word lists taken directly from @caliskan_2017. Following @wang_2020, we made one alteration to the sets, namely exchanging "Bill" in the male names of the *Career & Family* set for "Tom". This is to avoid ambiguity due to the lower-casing of GloVe. The exact lists can be found in our implementation of the evaluation.
 The results we obtained can be seen in Table 1.

```{r}
options(digits=4)
knitr::kable(weat_results, col.names = c('Embeddings', 'C&F d', 'C&F p', 'M&A d', 'M&A p', 'S&A d', 'S&A p'), caption="WEAT Test")
```

With the WEAT implementation we used we were able to almost perfectly reproduce the results of @wang_2020 for the *Career & Family* word sets. Both $d$ and $p$ differ only slightly from the reported values and we can clearly see that for the Double-Hard debiased embeddings $p$ is larger not only in comparison to the original embedding but also the other debiasing methods. Despite the bias still being significant ($p<0.05$), the large $p$ and small $d$ compared to the baselines suggest a more effective debiasing. We were also able to obtain comparable results for the Double-Hard debiased embedding provided by @wang_2020 and for our self-debiased embedding.
For both *Math & Arts* and *Science & Arts* we obtained values for $d$ and the $p$ that differ noticebly from the values reported by the authors. However, as in @wang_2020, we can see that the bias in *Math & Arts* was already insignificant in the original GloVe embedding. 

#### Neighborhood Metric
The Neighborhood Metric was originally introduced by @gonen_2019 and is based on the observation that even though debiased words no longer inherit their bias in form of a specific gender direction, it remains in embeddings' spacial grouping. The supposedly "removed" bias is still manifested as one can observe those words socially-marked with the same gender situated closer to one another in the embedding space [@gonen_2019].
K-means clustering on a number of most biased words can often straightforwardly cluster "debiased" embeddings into the correct female and male categories, uncovering the remaining bias.
@wang_2020 evaluate K-means clustering performance on such most biased words by simply counting the samples correctly assigned to the gender bias allegedly removed in debiasing. The alignment score $a$ is defined as $a = \frac{1}{2k}\sum_{i=1}^{2k}1[\hat{g_{i}}== g_{i}]$ and set to $a = max(a, 1-a)$ with $k$ resembling the number of samples for each gender, $\hat{g_{i}}$ the estimated and $g_{i}$ the correct label. According to this definition, an alignment score value of 0.5 indicates perfectly unbiased embeddings, as the clustering algorithm failed to replicate the original gender pattern [@wang_2020].
Even though @wang_2020 make use of this metric within their debiasing algorithm, adjusting their embeddings to optimize performance on the Neighborhood Metric, they also apply it again in the scope of evaluating their debiasing performance in comparison to other embeddings. As one would expect, the Double-Hard debiased embeddings therefore do particularly well in comparison to the other Baseline embeddings.
Our results can be observed in Table 2, and are visualized with the help of tSNE below. @wang_2020 applied the tSNE reduction technique to the top 500 female and male biased words to be able to plot the resulting clusters in two-dimensional space. Both values in Table 2 and the tSNE visualization show that Hard-Glove, Strong-Hard-Glove and both Double-Hard-GloVe embeddings obtained by us and by @wang_2020 lead to nearly bias-free clustering performances, whereby our Double-Hard debiased embeddings actually slightly outperform the authors' ones. The remaining baseline embeddings on the other hand can nearly perfectly be clustered, indicating remaining bias. 
Where our results deviate considerably to the ones from the paper is in the performance of (Strong-)Hard-Glove. In our evaluation these embeddings also show clustering accuracies alluding to basically no remaining bias, while @wang_2020 present results that would indicate significantly more remaining bias.

```{r table 2}
nm_results <- mutate_if(nm_results, is.numeric, ~ . * 100) # get percentages
apa_table(nm_results, col.names = c('Embeddings', 'Top 100', 'Top 500', 'Top 1000'), caption="Neighborhood Metric Clustering Accuracy")
```

```{r tsne plot}
knitr::include_graphics("evaluation_results/results_tsne.png")
```
<!--KK: als Grafikbeschreibung:?-->
In the graphic above you can see a plot that shows all the baseline datasets, as well as our Double-Hard debiased embedding and @wang_2020's embedding. For the original GloVe embedding, GN-GloVe, GP-GloVe and GP-GN-GloVe the clustering accuracy is still very high, meaning that the top 500 female (lighblue) and male (pink) biased words are still biased after applying the respective debiasing techniques.

### Analysis of Retaining Word Semantics
One of the most important properties of embeddings is that they represent meaningful word semantics, for example in the form of analogies or concepts. In this section it is tested in how far the embeddings still meet this criterion after debiasing.

#### Word Analogy
The word analogy task was introduced by @mikolov2013MSR. The task is to find the word D such that "A is to B as C is to D". One example for an unbiased analogy is: "Man is to King as Woman is to Queen" whereas a biased analogy would be: "Man is to Computer Programmer as Woman is to Homemaker" [@bolukbasi_2016]. The debiased embeddings are evaluated on two word analogy test sets, the MSR [@mikolov2013MSR] and the Google word analogy task [@mikolov2013Google], in order to find out whether they preserve desired unbiased analogies.

The MSR word analogy dataset contains 8000 syntactic questions in the form presented above. The missing word D is computed by maximizing the cosine similarity between D and C - A + B. The evaluation metric is the percentage of correctly answered questions [see @wang_2020].

The Google word analogy dataset contains 19.544 (**Total**) questions, 8.869 of which are semantic (**Sem**) and 10.675 are syntactic (**Syn**) questions.

```{r table 3}
ana_results <- full_join(msr_results, gg_results)
ana_results <- mutate_if(ana_results, is.numeric, ~ . * 100) # get percentages
apa_table(ana_results, col.names = c('Embeddings', 'MSR', 'Sem', 'Syn', 'Total'), caption="Analogy Tasks")
```
The results can be inspected in Table 3 and are in line with the results reported by @wang_2020. What is interesting is that our replicated Double-Hard debiased embedding scores best in all word analogy tasks and even outperforms the original GloVe embedding.

#### Concept Categorization
<!-- MK: kurz erklären was die Idee hinter Concept Categorization ist? Allein, weil wir es bei den anderen Evaluation methods (und auch bei Coreference) auch machen?-->
@wang_2020 applied this evaluation method to all the different embedding baselines and their Double-Hard debiased embedding as well. However, we found the code provided by the authors to be incomplete. It was missing major parts, thereby hindering us from adequately replicating its application and due to time constraints we decided not to implement this method.


## Discussion
<!-- notes
- Methods (Sonja):
  - discuss the results and what could be (partly) replicated and what not (list the respective parts): coreference resolution not mentioned in their uploaded code, only link in readme which is not really helpful; concept categorization not replicable
  - poor code quality, documentation
  - did they use the equality_pairs or female_male pairs?? -> doesn't make a difference, but still ill documented code etc. blabla
  - Word Analogy tasks: needed to change code (due to web module??) -> datasets were not provided and not linked (needed to search it on our own and preprocess it to bring it in the form expected by wang's code)
  - "web" module not uploaded to github repo
  - uploaded notebook only exemplary
  - preprocessing not mentioned in paper (we did it due to rational reasoning and the notebook provided in their github)
- Content (Meike):
  - claiming they remove frequency, even though there is no real proof that the choosen PC depicts that
  - relate to pilot study and papers (gong & mu)
- Meta (Krissi):
  - missing code etc. relates to the results found by metastudies described in the introduction
  - very poor code quality: readability, mismatchs to paper; kind of an example why replications often fail, we could not replicate everything either
  - uploaded notebook only exemplary
-->

## Conclusion (kris)
<!-- summarize the main discussion points-->


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
