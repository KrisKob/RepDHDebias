---
title             : "Does Double-Hard Debias Keep its Promises? - A Replication Study"
shorttitle        : "Replication: Wang et al. (2020)"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    email         : "kkobrock@uni-osnabrueck.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - müssen wir natürlich nicht ausfüllen
     
      
  - name          : "Meike Korsten"
    affiliation   : "1"
    role:
      - ...
      
  - name          : "Sonja Börgerding"
    affiliation   : "1"
    role:
      - ...

affiliation:
  - id            : "1"
    institution   : "University of Osnabrück"

  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Just edit the text! There are several websites that help with (r)markdown, e.g. https://rmarkdown.rstudio.com/.
If you don't want to edit directly, but rather comment use:
<!-- KK: This is a comment. It won't be included in the exported file. -->
Cite using: Wang et al. [-@wang_2020] said that... or simply blablabla [@wang_2020]. References must be added to r-references.bib file. 

## Introduction
- shall we include an abstract?
Recent research has shown that word embeddings derived from language corpora inherit human biases. The first seminal study on this topic was by Bolukbasi et al. [-@bolukbasi_2016] who used the word analogy task developed by Mikolov et al. [-@mikolov2013MSR] to prove that "Man is to Computer Programmer as Woman is to Homemaker" - a clearly gender biased analogical relation derived from a Google News embedding. Caliskan et al. [-@caliskan_2017] complement this finding with a large study on human-like semantic biases in Natural Language Processing (NLP) tools. They have shown in more general that human biases as exhibited in psychological studies using, for example, the Implicit Association Test (IAT) are learned by NLP algorithms designed to construct meaningful word representations. According to Zhao et al. [-@zhao_2018a] these biases propagate to downstream tasks. As pre-trained word embeddings are often used for a lot of more complex NLP tasks and architectures of our everyday life, the biased embeddings bear the risk of proliferating and strengthening existing stereotypes in human cultures.

### Debiasing Embeddings
But how can the problem of biased embeddings be solved? Several researchers have proposed post-processing techniques or algorithm modifications that promise to "debias" the word embeddings obtained by algorithms like word2vec or GloVe [e.g. @bolukbasi_2016; @zhao_2018b; @kaneko_2019]. 
Bolukbasi et al. [-@bolukbasi_2016] have developed an algorithm called Hard Debias that is based on the idea of removing (biased) gender directions from the embedding whilst preserving desired gender relations. For example, the encoded gender of the words "king" and "queen" is given by definition. So the relation between "male" and "king" and between "female" and "queen" is desired. On the other hand, words like "nurse" and "doctor" also tend to exhibit relations to a specific gender even though the words do not have a gender-specific meaning but can be used for all genders. The proposed Hard Debias algorithm first identifies a gender subspace of the embedding that best captures the bias. Then a neutralizing step ensures that gender neutral words (like "nurse" and "doctor") are indeed neutral, i.e. zero, in the gender subspace. An equalizing step is then applied in order to ensure that useful relations apply to words from both genders and are not biased towards one gender anymore. For example, the word "babysit" should be equally distant from both "she" and "he". 
Wang et al. [-@wang_2020] build on Hard Debias in their newly proposed Double Hard Debias algorithm. The main idea is to not only remove the gender direction of the biased embedding, but also the frequency direction as it has been shown that word frequency has a significant impact on the geometry of the embedding space [e.g. @gong_2018; @mu_2018, more on this later].

### Motivation
In this project, we aimed to replicate the debiasing method presented by Wang et al. [-@wang_2020] and to reproduce their experimental results as it is the most recent paper that proposes a post-processing technique for debiasing algorithms. 
<!-- KK: Brauchen wir noch eine Definition von reproducibility bzw. replication? -->
The reproduction of existing results is not only good practice in science, but it is also essential for gaining a deeper understanding on the methods used and it can help to validate existing results or to shed well-grounded doubt on them. Recent studies of reproducibility in the field of Computer Science [e.g. @collberg_2015] and NLP [e.g. @fokkens_2013; @mieskes_2017; @cohen_2018; @belz_2021] explain why reproducibility endeavours are often failing and shed light on the exact nature of the 'reproducibility crisis' [term coined by e.g. @baker_2015] in NLP research. Belz et al. [-@belz_2021], for example, report that the community's interest in topics of reproducibility have risen even though reproduction attempts still tend to fail due to problems like missing data, missing code and incomplete documentation [see also @fokkens_2013; @mieskes_2019]. 
This project aims to make a contribution to the increasing body of reproducibility and replication attempts in NLP research.


## Implementation
- task description
- explanation of model and training choices

### Datasets and Preliminaries

### Hard Debias

### Double-Hard Debias


## Evaluation 
- stick to the paper

### Baselines

### Evaluation of Debiasing Performance
### Debiasing in Downstream Applications
#### Coreference Resolution
- no replication possible, no code provided

### Debiasing at Embedding Level
#### The Word Embeddings Association Test (WEAT)
#### Neighborhood Metric
- discuss that this is shady in the discussion part

## Analysis of Retaining Word Semantics
#### Word Analogy
#### Concept Categorization


## Discussion
- analysis of results and evaluation of performance evaluation
- ablation studies (not applicable)
- discuss the results and what could be (partly) replicated and what not


## Conclusion



\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
