---
title             : "Does Double-Hard Debias Keep its Promises? - A Replication Study"
shorttitle        : "Replication: Wang et al. (2020)"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
     
  - name          : "Meike Korsten"
    affiliation   : "1"
      
  - name          : "Sonja Börgerding"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Osnabrück"

  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Just edit the text! There are several websites that help with (r)markdown, e.g. https://rmarkdown.rstudio.com/.
If you don't want to edit directly, but rather comment use:
<!-- KK: This is a comment. It won't be included in the exported file. -->
Cite using: Wang et al. [-@wang_2020] said that... or simply blablabla [@wang_2020]. References must be added to r-references.bib file. 

## Introduction
- shall we include an abstract?
Recent research has shown that word embeddings derived from language corpora inherit human biases. The first seminal study on this topic was by Bolukbasi et al. [-@bolukbasi_2016] who used the word analogy task developed by Mikolov et al. [-@mikolov2013MSR] to prove that "Man is to Computer Programmer as Woman is to Homemaker" - a clearly gender biased analogical relation derived from a Google News embedding. Caliskan et al. [-@caliskan_2017] complement this finding with a large study on human-like semantic biases in Natural Language Processing (NLP) tools. They have shown in more general that human biases as exhibited in psychological studies using, for example, the Implicit Association Test (IAT) are learned by NLP algorithms designed to construct meaningful word representations. According to Zhao et al. [-@zhao_2018a] these biases propagate to downstream tasks. As pre-trained word embeddings are often used for a lot of more complex NLP tasks and architectures of our everyday life, the biased embeddings bear the risk of proliferating and strengthening existing stereotypes in human cultures.

### Debiasing Embeddings
But how can the problem of biased embeddings be solved? Several researchers have proposed post-processing techniques or algorithm modifications that promise to "debias" the word embeddings obtained by algorithms like word2vec or GloVe [e.g. @bolukbasi_2016; @zhao_2018b; @kaneko_2019]. 
Bolukbasi et al. [-@bolukbasi_2016] have developed an algorithm called Hard Debias that is based on the idea of removing (biased) gender directions from the embedding whilst preserving desired gender relations. For example, the encoded gender of the words "king" and "queen" is given by definition. So the relation between "male" and "king" and between "female" and "queen" is desired. On the other hand, words like "nurse" and "doctor" also tend to exhibit relations to a specific gender even though the words do not have a gender-specific meaning but can be used for all genders. The proposed Hard Debias algorithm first identifies a gender subspace of the embedding that best captures the bias. Then a neutralizing step ensures that gender neutral words (like "nurse" and "doctor") are indeed neutral, i.e. zero, in the gender subspace. An equalizing step is then applied in order to ensure that useful relations apply to words from both genders and are not biased towards one gender anymore. For example, the word "babysit" should be equally distant from both "she" and "he". 
Wang et al. [-@wang_2020] build on Hard Debias in their newly proposed Double Hard Debias algorithm. The main idea is to not only remove the gender direction of the biased embedding, but also the frequency direction as it has been shown that word frequency has a significant impact on the geometry of the embedding space [e.g. @gong_2018; @mu_2018, more on this later].

### Motivation
In this project, we aimed to replicate the debiasing method presented by Wang et al. [-@wang_2020] and to reproduce their experimental results as it is the most recent paper that proposes a post-processing technique for debiasing algorithms. 
<!-- KK: Brauchen wir noch eine Definition von reproducibility bzw. replication? -->
The reproduction of existing results is not only good practice in science, but it is also essential for gaining a deeper understanding on the methods used and it can help to validate existing results or to shed well-grounded doubt on them. Recent studies of reproducibility in the field of Computer Science [e.g. @collberg_2015] and NLP [e.g. @fokkens_2013; @mieskes_2017; @cohen_2018; @belz_2021] explain why reproducibility endeavours are often failing and shed light on the exact nature of the 'reproducibility crisis' [term coined by e.g. @baker_2015] in NLP research. Belz et al. [-@belz_2021], for example, report that the community's interest in topics of reproducibility have risen even though reproduction attempts still tend to fail due to problems like missing data, missing code and incomplete documentation [see also @fokkens_2013; @mieskes_2019]. 
This project aims to make a contribution to the increasing body of reproducibility and replication attempts in NLP research.


## Implementation
- task description
- explanation of model and training choices

### Pilot Study: Impact of Word Frequency (Meike)

### Datasets and Preliminaries (Sonja)

### Hard Debias (Kristina)
The authors make use of the Hard Debias algorithm proposed by Bolukbasi et al. [-@bolukbasi_2016]. The  paper at hand does not give much information on the exact implementation of Hard Debias used. The code uploaded to the authors' Github repository is not well documented, so we were not able to find the exact parts of the algorithm that should refer to the implementation of Hard Debias. That is why we sticked to the original paper from Bolukbasi et al. [-@bolukbasi_2016] in order to re-implement Hard Debias. 

The paper describes two steps: First, the gender direction (or, more generally, the subspace) has to be identified. This is achieved with the help of defining sets $D_1, D_2, ..., D_n \subset W$ which consist of *gender specific* words, i.e. words which are associated with a gender by definition like *girl, boy* or *she, he*. These are the words that can help to identify the gender direction by capturing the concept *female, male* in the embedding $\{\vec{w}\in\mathbb{R}^d\}_{w\in W}$. Whereas in some simple implementations of Hard Debias, only one definitional pair might be used, [@bolukbasi_2016] suggest to compute the gender direction $B$ across multiple pairs to more robustly estimate the bias. In our implementation we used the 10 word pairs suggested by @bolukbasi_2016 which were experimentally shown to agree with an intuitive concept of gender. For these 10 pairs the principal components (PCs) are calculated and the bias subspace $B$ is made of the first $k \geq 1$ rows of the decomposition SVD($C$). According to @bolukbasi_2016, the first eigenvalue is significantly larger than the rest and so the top PC is hypothesized to capture the gender subspace. So $k=1$ is chosen and our resulting gender subspace $B$ is thus simply a direction. C is calculated in the following way: $$C:=\sum_{i=1}^n \sum_{w\in D_i}(\vec{w}-\mu_i)^T(\vec{w}-\mu_i)/|D_i|$$ where $\mu_i := \sum_{w\in D_i}\vec{w}/|D_i|$ are the means of the defining sets $D_1, D_2, ..., D_n \subset W$.

As a second step Hard Debias neutralizes and equalizes the word embeddings. Neutralizing means to transform each word embedding $\vec{w}$ such that every word $w\in N$ has zero projection in the gender subspace. So for each word $w\in N$ in a set of neutral words $N \subseteq W$, we re-embed $\vec{w}$: $$\vec{w}:=(\vec{w}-\vec{w}_B)/||\vec{w}-\vec{w}_B||$$. The equalize step ensures that desired analogical properties hold for both female and male words contained in the equality sets $\mathcal{E}=\{E_1,E_2,...,E_m\}$ where each $E_i \subseteq W$. For example, after debiasing we would like the embeddings of the pair $E=\{grandmother, grandfather\}$ contained in the equality sets to be equidistant from the embedding of *babysit* [@bolukbasi_2016]. This is enforced by equating each set of words $E\in \mathcal{E}$ outside of $B$ to their simple average $\nu:=\mu-\mu_B$ where $\mu:=\sum_{w\in E}w/|E|$ before adjusting vectors so that they are unit length. So for each word $w\in E$, $\vec{w}$ is re-embedded to $$\vec{w}:=\nu+\sqrt{1-||\nu||^2}\frac{\vec{w}_B-\mu_B}{||\vec{w}_B-\mu_B||}$$.

With the help of the original paper, we were successfully able to re-implement Hard Debias.


### Double-Hard Debias (Meike)


## Evaluation 
- stick to the paper

```{r load dataframes}
# load dataframes (computed in Evaluations.ipynb)
weat_results <- read.csv('evaluation_results/results_weat.csv')
nm_results <- read.csv('evaluation_results/results_nm.csv')
tsne_results <- read.csv('evaluation_results/results_tsne.csv')
msr_results <- read.csv('evaluation_results/results_msr.csv')
gg_results <- read.csv('evaluation_results/results_gg.csv')
```


### Baselines (Sonja)

### Evaluation of Debiasing Performance 
### Debiasing in Downstream Applications
#### Coreference Resolution (Meike)
- no replication possible, no code provided

### Debiasing at Embedding Level
#### The Word Embeddings Association Test (WEAT) (Sonja)

```{r table 1}
knitr::kable(weat_results, caption="WEAT Test")
```


#### Neighborhood Metric (Meike)
- discuss that this is shady in the discussion part

```{r table 2}
knitr::kable(nm_results, caption="Neighborhood Metric Clustering Accuracy")
```

```{r tsne plot}
knitr::include_graphics("evaluation_results/results_tsne.png")
```


### Analysis of Retaining Word Semantics (Kristina)
One of the most important properties of embeddings is that they represent meaningful word semantics. In this section it is tested whether the debiased embeddings still have this property.

#### Word Analogy (Kristina)
The word analogy task was introduced by Mikolov et al. [-@mikolov2013MSR]. The task is to find the word D such that "A is to B as C is to D". One example for an unbiased analogy is: "Man is to King as Woman is to Queen" whereas a biased analogy would be: "Man is to Computer Programmer as Woman is to Homemaker" [@bolukbasi_2016]. The debiased embeddings are evaluated on two word analogy test sets: the MSR [@mikolov2013MSR] and the Google word analogy task [@mikolov2013Google] in order to find out whether they preserve desired unbiased analogies.

The MSR word analogy dataset contains 8000 syntactic questions in the form presented above. The missing word D is computed by maximizing the cosine similarity between D and C - A + B. The evaluation metric is the percentage of correctly answered questions [see @wang_2020].

The Google word analogy dataset contains 19.544 (**Total**) questions, 8.869 of which are semantic (**Sem**) and 10.675 are syntactic (**Syn**) questions.

```{r tables 3 and 4}
#ana_results <- full_join(msr_results, gg_results)
#knitr::kable(ana_results, caption="Analogy Tasks")
knitr::kable(msr_results, caption="MSR Word Analogy Task")
knitr::kable(gg_results, caption="Google Word Analogy Task")
```

#### Concept Categorization (Sonja)


## Discussion
- analysis of results and evaluation of performance evaluation
- ablation studies (not applicable)
- discuss the results and what could be (partly) replicated and what not


## Conclusion



\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
