---
title             : "Does Double-Hard Debias Keep its Promises? - A Replication Study"
shorttitle        : "Replication: Wang et al. (2020)"

author: 
  - name          : "Kristina Kobrock"
    affiliation   : "1"
     
  - name          : "Meike Korsten"
    affiliation   : "1"
      
  - name          : "Sonja Börgerding"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "University of Osnabrück"

  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

Just edit the text! There are several websites that help with (r)markdown, e.g. https://rmarkdown.rstudio.com/.
If you don't want to edit directly, but rather comment use:
<!-- KK: This is a comment. It won't be included in the exported file. -->
Cite using: Wang et al. [-@wang_2020] said that... or simply blablabla [@wang_2020]. References must be added to r-references.bib file. 

## Introduction
- shall we include an abstract?
Recent research has shown that word embeddings derived from language corpora inherit human biases. The first seminal study on this topic was by Bolukbasi et al. [-@bolukbasi_2016] who used the word analogy task developed by Mikolov et al. [-@mikolov2013MSR] to prove that "Man is to Computer Programmer as Woman is to Homemaker" - a clearly gender biased analogical relation derived from a Google News embedding. Caliskan et al. [-@caliskan_2017] complement this finding with a large study on human-like semantic biases in Natural Language Processing (NLP) tools. They have shown in more general that human biases as exhibited in psychological studies using, for example, the Implicit Association Test (IAT) are learned by NLP algorithms designed to construct meaningful word representations. According to Zhao et al. [-@zhao_2018a] these biases propagate to downstream tasks. As pre-trained word embeddings are often used for a lot of more complex NLP tasks and architectures of our everyday life, the biased embeddings bear the risk of proliferating and strengthening existing stereotypes in human cultures.

### Debiasing Embeddings
But how can the problem of biased embeddings be solved? Several researchers have proposed post-processing techniques or algorithm modifications that promise to "debias" the word embeddings obtained by algorithms like word2vec or GloVe [e.g. @bolukbasi_2016; @zhao_2018b; @kaneko_2019]. 
Bolukbasi et al. [-@bolukbasi_2016] have developed an algorithm called Hard Debias that is based on the idea of removing (biased) gender directions from the embedding whilst preserving desired gender relations. For example, the encoded gender of the words "king" and "queen" is given by definition. So the relation between "male" and "king" and between "female" and "queen" is desired. On the other hand, words like "nurse" and "doctor" also tend to exhibit relations to a specific gender even though the words do not have a gender-specific meaning but can be used for all genders. The proposed Hard Debias algorithm first identifies a gender subspace of the embedding that best captures the bias. Then a neutralizing step ensures that gender neutral words (like "nurse" and "doctor") are indeed neutral, i.e. zero, in the gender subspace. An equalizing step is then applied in order to ensure that useful relations apply to words from both genders and are not biased towards one gender anymore. For example, the word "babysit" should be equally distant from both "she" and "he". 
Wang et al. [-@wang_2020] build on Hard Debias in their newly proposed Double Hard Debias algorithm. The main idea is to not only remove the gender direction of the biased embedding, but also the frequency direction as it has been shown that word frequency has a significant impact on the geometry of the embedding space [e.g. @gong_2018; @mu_2018, more on this later].

### Motivation
In this project, we aimed to replicate the debiasing method presented by Wang et al. [-@wang_2020] and to reproduce their experimental results as it is the most recent paper that proposes a post-processing technique for debiasing algorithms. 
<!-- KK: Brauchen wir noch eine Definition von reproducibility bzw. replication? -->
The reproduction of existing results is not only good practice in science, but it is also essential for gaining a deeper understanding on the methods used and it can help to validate existing results or to shed well-grounded doubt on them. Recent studies of reproducibility in the field of Computer Science [e.g. @collberg_2015] and NLP [e.g. @fokkens_2013; @mieskes_2017; @cohen_2018; @belz_2021] explain why reproducibility endeavours are often failing and shed light on the exact nature of the 'reproducibility crisis' [term coined by e.g. @baker_2015] in NLP research. Belz et al. [-@belz_2021], for example, report that the community's interest in topics of reproducibility have risen even though reproduction attempts still tend to fail due to problems like missing data, missing code and incomplete documentation [see also @fokkens_2013; @mieskes_2019]. 
This project aims to make a contribution to the increasing body of reproducibility and replication attempts in NLP research.


## Implementation
- task description
- explanation of model and training choices

### Pilot Study: Impact of Word Frequency (Meike)

### Datasets and Preliminaries (Sonja)
As dataset we used the pre-trained, 300-dimensional GloVe embedding used and provided via link on Github by Wang et al. [-@wang_2020]. Further they also provided several more datasets that were obtained through different debiasing methods, some applied during training, some after. The authors] used these datasets as baselines for the evaluation to compare the perfomance of their Double-Hard Debias method to. Lastly they also provide the embedding that is the reult of applying their Double-Hard Debias alogrithm to the original, non-debiased GloVe embedding. We included all of these embeddings in our evaluation to compore to the result of our implementation of Double-Hard Debias.

The original files included 300-dimensional embeddings for 322.636 words. To avoid large downloads we decided to open the files via URL, however this was not possible for the Double-Hard Debiased embedding obtained by Wang et al. [-@wang_2020], therefor this embedding needs to be downloaded and open seperatly. From all the provided files we created for each embedding a vocabulary in form of a list, a dictionary mapping all words in the vocabulary to an ID and an embedding matrix that stores the 300-dimensional embedding of each word in the row corresponding to the ID of the word. During the pre-processing we restricted the vocabularies to the 50.000 most common words, which corresponds to the first 50.000 words in a GloVe embedding. At this point we also excluded any words that contain digits or special characters from our vocabularies. Both these actions follow the implementation provided by Wang et al. [-@wang_2020], however the restriction to 50.000 words is not documented and apparently only happens in the code demonstrating the Double-Hard debiasing method for computational reasons. We adopted this restriction for computational reasons as well as the hope to remove some less frequent words such as names from our embedding to obtain more general results. However unlike the authors we did not specifically remove any non lower case words as the GloVe embedding is lower-cased by default and we also refrained from excluding all words longer than 20 characters.

We made use of a number of word sets provided by Wang et al. [-@wang_2020]. For the application of Hard Debias as proposed by Bolukbasi et al. [-@bolukbasi_2016] we need sets of male and female words as well as a set of neutral words. We will later explain how we obtained the sets of gendered words based on determining the gender direction. For this we used the supplied "definitional_pairs", a word set including pairs such as "he" and "she". To obtain the neutral set, we removed all words found in the files provided by Wang et al. [-@wang_2020] from our vocabulary. These files included besides the aforementioned "definitional_pairs" also "equalize_pairs", "gender_specific_full", "female_word_file" and "male_word_file" in the assumption that all these files include definitionally gendered words  that should not be included in the neutral set. However it stays unclear for some of these sets where Wang et al. [-@wang_2020] obtained them, except for "definitional_pairs" and "equalize_pairs" which are the gender pairs and equality sets as suggested by Bolukbasi et al. [-@bolukbasi_2016]. Since the "male_word_file" and the "female_word_file" include pairs of corresponding words such as "spokesman" and "spokeswoman" or "priest" and "nun" we also created a set of pairs based on these two files to be used with the debiasing algorithm.


### Hard Debias (Kristina)
The authors make use of the Hard Debias algorithm proposed by Bolukbasi et al. [-@bolukbasi_2016]. The  paper at hand does not give much information on the exact implementation of Hard Debias used. The code uploaded to the authors' Github repository is not well documented, so we were not able to find the exact parts of the algorithm that should refer to the implementation of Hard Debias. That is why we sticked to the original paper from Bolukbasi et al. [-@bolukbasi_2016] in order to re-implement Hard Debias. 

The paper describes two steps: First, the gender direction (or, more generally, the subspace) has to be identified. This is achieved with the help of defining sets $D_1, D_2, ..., D_n \subset W$ which consist of *gender specific* words, i.e. words which are associated with a gender by definition like *girl, boy* or *she, he*. These are the words that can help to identify the gender direction by capturing the concept *female, male* in the embedding $\{\vec{w}\in\mathbb{R}^d\}_{w\in W}$. Whereas in some simple implementations of Hard Debias, only one definitional pair might be used, [@bolukbasi_2016] suggest to compute the gender direction $B$ across multiple pairs to more robustly estimate the bias. In our implementation we used the 10 word pairs suggested by @bolukbasi_2016 which were experimentally shown to agree with an intuitive concept of gender. For these 10 pairs the principal components (PCs) are calculated and the bias subspace $B$ is made of the first $k \geq 1$ rows of the decomposition SVD($C$). According to @bolukbasi_2016, the first eigenvalue is significantly larger than the rest and so the top PC is hypothesized to capture the gender subspace. So $k=1$ is chosen and our resulting gender subspace $B$ is thus simply a direction. C is calculated in the following way: $$C:=\sum_{i=1}^n \sum_{w\in D_i}(\vec{w}-\mu_i)^T(\vec{w}-\mu_i)/|D_i|$$ where $\mu_i := \sum_{w\in D_i}\vec{w}/|D_i|$ are the means of the defining sets $D_1, D_2, ..., D_n \subset W$.

As a second step Hard Debias neutralizes and equalizes the word embeddings. Neutralizing means to transform each word embedding $\vec{w}$ such that every word $w\in N$ has zero projection in the gender subspace. So for each word $w\in N$ in a set of neutral words $N \subseteq W$, we re-embed $\vec{w}$: $$\vec{w}:=(\vec{w}-\vec{w}_B)/||\vec{w}-\vec{w}_B||$$. The equalize step ensures that desired analogical properties hold for both female and male words contained in the equality sets $\mathcal{E}=\{E_1,E_2,...,E_m\}$ where each $E_i \subseteq W$. For example, after debiasing we would like the embeddings of the pair $E=\{grandmother, grandfather\}$ contained in the equality sets to be equidistant from the embedding of *babysit* [@bolukbasi_2016]. This is enforced by equating each set of words $E\in \mathcal{E}$ outside of $B$ to their simple average $\nu:=\mu-\mu_B$ where $\mu:=\sum_{w\in E}w/|E|$ before adjusting vectors so that they are unit length. So for each word $w\in E$, $\vec{w}$ is re-embedded to $$\vec{w}:=\nu+\sqrt{1-||\nu||^2}\frac{\vec{w}_B-\mu_B}{||\vec{w}_B-\mu_B||}$$.

With the help of the original paper, we were successfully able to re-implement Hard Debias.


### Double-Hard Debias (Meike)


## Evaluation 
- stick to the paper

```{r load dataframes}
# load dataframes (computed in Evaluations.ipynb)
weat_results <- read.csv('evaluation_results/results_weat.csv')
nm_results <- read.csv('evaluation_results/results_nm.csv')
tsne_results <- read.csv('evaluation_results/results_tsne.csv')
msr_results <- read.csv('evaluation_results/results_msr.csv')
gg_results <- read.csv('evaluation_results/results_gg.csv')
```


### Baselines (Sonja)
Following Wang et al. [-@wang_2020] we used GloVe embeddings that were obtained using several different debiasing methods as baselines for our evaluation along with the original, non-debiased embedding.

original GloVe: The original, non-debiased GloVe embedding used and provided by Wang et al. [-@wang_2020]. It was obtained from training on the 2017 January dump of English Wikipedia.

GN-GloVe: Gender-Neutral GloVe embedding released by Zhao et al. [-@zhao_2018b]. This method restricts gender information in certain dimensions while neutralizing in the remaining dimensions.

GN-GloVe(a): A variant of the Gender-Neutral GloVe embedding obtained by Wang et al. [-@wang_2020]. It was created by excluding the gender dimensions from the GN-GloVe embedding in a try to completly remove gender.

GP-GloVe: Gender preserving GloVe embedding released by Kaneko and Bollegala [-@kaneko_2019]. This method attempts to remove stereotypical gender bias and preserve non-discriminative gender information.

GP-GN-GloVe: Gender preserving, Gender-Neutral GloVe embedding provided by Kaneko and Bollegala [-@kaneko_2019]. This is the result of applying gender preserving debiasing to an already debiased GN-GloVe embedding.

Hard-GloVe: Hard debiased GloVe embedding obtained by Wang et al. [-@wang_2020] following the implementation of Bolukbasi et al. [-@bolukbasi_2016]. This method aims to debias neutral words while preserving the gender specific words.

Strong-Hard-GloVe: Strong-Hard debiased GloVe embedding obtained by Wang et al. [-@wang_2020]. A variant of Hard debias during which all words are debiased instead of only the neutral words.

Double-Hard-GloVe(Wang et al.): Double-Hard debiased GloVe embedding obtained by Wang et al. [-@wang_2020]. This is the result of applying their proposed Double-Hard Debias method to the original GloVe embedding.

Double-Hard-GloVe(replication): Double-Hard debiased GloVe embedding obtained by us. This is the result of applying our implementation of the Double-Hard Debias method to the original GloVe embedding.

### Evaluation of Debiasing Performance 
### Debiasing in Downstream Applications
#### Coreference Resolution (Meike)
- no replication possible, no code provided 

### Debiasing at Embedding Level
#### The Word Embeddings Association Test (WEAT) (Sonja)

```{r table 1}
knitr::kable(weat_results, caption="WEAT Test")
```


#### Neighborhood Metric (Meike)
- discuss that this is shady in the discussion part

```{r table 2}
knitr::kable(nm_results, caption="Neighborhood Metric Clustering Accuracy")
```

```{r tsne plot}
knitr::include_graphics("evaluation_results/results_tsne.png")
```


### Analysis of Retaining Word Semantics (Kristina)
One of the most important properties of embeddings is that they represent meaningful word semantics. In this section it is tested whether the debiased embeddings still have this property.

#### Word Analogy (Kristina)
The word analogy task was introduced by Mikolov et al. [-@mikolov2013MSR]. The task is to find the word D such that "A is to B as C is to D". One example for an unbiased analogy is: "Man is to King as Woman is to Queen" whereas a biased analogy would be: "Man is to Computer Programmer as Woman is to Homemaker" [@bolukbasi_2016]. The debiased embeddings are evaluated on two word analogy test sets: the MSR [@mikolov2013MSR] and the Google word analogy task [@mikolov2013Google] in order to find out whether they preserve desired unbiased analogies.

The MSR word analogy dataset contains 8000 syntactic questions in the form presented above. The missing word D is computed by maximizing the cosine similarity between D and C - A + B. The evaluation metric is the percentage of correctly answered questions [see @wang_2020].

The Google word analogy dataset contains 19.544 (**Total**) questions, 8.869 of which are semantic (**Sem**) and 10.675 are syntactic (**Syn**) questions.

```{r tables 3 and 4}
#ana_results <- full_join(msr_results, gg_results)
#knitr::kable(ana_results, caption="Analogy Tasks")
knitr::kable(msr_results, caption="MSR Word Analogy Task")
knitr::kable(gg_results, caption="Google Word Analogy Task")
```

#### Concept Categorization (Sonja)


## Discussion
- analysis of results and evaluation of performance evaluation
- ablation studies (not applicable)
- discuss the results and what could be (partly) replicated and what not


## Conclusion



\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
