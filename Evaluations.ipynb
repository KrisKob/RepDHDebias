{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded utils\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import utils\n",
    "import further_embeddings as data # takes a while because it loads all embedding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the two debiased embeddings obtained by our implementation of Double-Hard Debias\n",
    "file_1 = open('debiased_1/debiased_1.p', 'rb')\n",
    "debiased_equal = pickle.load(file_1)\n",
    "file_1.close()\n",
    "file_2 = open('debiased_2/debiased_2.p', 'rb')\n",
    "debiased_fem_male = pickle.load(file_2)\n",
    "file_2.close()\n",
    "# load the Double-Hard debiased embeddings obtained by Wang et al. (2020)\n",
    "file_3 = open('glove_dhd.p', 'rb')\n",
    "glove_dhd_wang = pickle.load(file_3)\n",
    "file_3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_embeddings = {'original_glove': (embedding, vocab, w2id), 'glove_dhd_wang': (glove_dhd_wang, vocab, w2id), \n",
    "                   'debiased_equal': (debiased_equal, vocab, w2id), 'debiased_fem_male': (debiased_fem_male, vocab, w2id)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Association Test\n",
    "Implementation taken from https://github.com/shivaomrani/HumanBiasInSemantics with minor adjustments such as variable names for readability. See file `weat.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weat import weat\n",
    "# auf embedding zugreifen, wenn wir das als Datei haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Career and family\n",
    "# Change from Bill to Tom as in paper to avoid ambiguity\n",
    "male_names = [\"john\", \"paul\", \"mike\", \"kevin\", \"steve\", \"greg\", \"jeff\", \"tom\"]\n",
    "female_names = [\"amy\", \"joan\", \"lisa\", \"sarah\", \"diana\", \"kate\", \"ann\", \"donna\"]\n",
    "career_attributes = [\"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"office\", \"business\", \"career\"]\n",
    "family_attributes = [\"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math and arts\n",
    "math_words = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\"]\n",
    "arts_words1 = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"sculpture\"]\n",
    "male_attributes1 = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
    "female_attributes1 = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science and arts\n",
    "science_words = [\"science\", \"technology\", \"physics\", \"chemistry\", \"einstein\", \"nasa\", \"experiment\", \"astronomy\"]\n",
    "arts_words2 = [\"poetry\", \"art\", \"shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\"]\n",
    "male_attributes2 = [\"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\"]\n",
    "female_attributes2 = [\"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w2ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d941bfa94033>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mwea_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcept1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcept2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstereotype1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstereotype2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mpvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meffect_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwea_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPValueAndEffect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meffect_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w2ids' is not defined"
     ]
    }
   ],
   "source": [
    "iterations = 100000\n",
    "embeddings = [data.embedding, data.embedding_gn, data.embedding_gn_a, data.embedding_gp, data.embedding_gp_gn, data.embedding_hd, data.embedding_hd_a, debiased_equal, debiased_fem_male, glove_dhd_wang]\n",
    "w2ids = [data.w2id, data.w2id_gn, data.w2id_gn_a, data.w2id_gp, data.w2id_gp_gn, data.w2id_hd, data.w2id_hd_a, data.w2id, data.w2id, data.w2id]\n",
    "concept1 = [male_names, math_words, science_words]\n",
    "concept2 = [female_names, arts_words1, arts_words2]\n",
    "stereotype1 = [career_attributes, male_attributes1, male_attributes2]\n",
    "stereotype2 = [family_attributes, female_attributes1, female_attributes2]\n",
    "row_names = [\"GloVe\", \"GN-GloVe\", \"GN-GloVe(a)\", \"GP-GloVe\", \"GP-GN-GloVe\", \"Hard-GloVe\", \"Strong-Hard-GloVe\", \"Double-Hard-GloVe(Equal)\", \"Double-Hard-GloVe(Fem-Male)\", \"Double-Hard-Glove(Wang)\"]\n",
    "column_names = [\"C & F: d\", \"C & F: p\", \"M & A: d\", \"M & A: p\", \"S & A: d\", \"S & A: p\"]\n",
    "test_results = []\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    result = []\n",
    "    print(row_names[i])\n",
    "    for j in range(3):\n",
    "        wea_test = weat(concept1[j], concept2[j], stereotype1[j], stereotype2[j], iterations, embedding, w2ids[i])\n",
    "        pvalue, effect_size, _ = wea_test.getPValueAndEffect()\n",
    "        result.append(effect_size)\n",
    "        result.append(pvalue)\n",
    "    test_results.append(result)\n",
    "    \n",
    "test_results = np.array(test_results).astype(float)\n",
    "df = pd.DataFrame(data = test_results, index = row_names, columns = column_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy\n",
    "The word analogy task is to find a word *D* such that \"*A* is to *B* as *C* is to *D*\". Wang et al. (2020) have evaluated all non-debiased and debiased embeddings on the MSR word analogy task [Mikolov et al., 2013a](https://www.aclweb.org/anthology/N13-1090/) as well as on a second Google word analogy dataset [Mikolov et al. 2013b](https://arxiv.org/abs/1301.3781v3). The evaluation metric is the percentage of questions for which the correct answer is assigned the maximum score by the algorithm. The analogy task is used to show whether a debiasing method is capable of preserving desired distance relations between words. The implementation was taken from Wang et al. (2020) with some adjustments to get it running: `analogy_tasks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analogy_tasks as ana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4884\n",
      "ACCURACY TOP1-MSR: 54.40% (2657/4884)\n"
     ]
    }
   ],
   "source": [
    "# MSR on all embeddings:\n",
    "for key, (embedding, vocab, w2id) in dict_embeddings.items():\n",
    "    print(key, \": \")\n",
    "    ana.evaluate_analogy_msr(embedding, vocab, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ana.evaluate_analogy_msr(debiased_1, data.vocab, data.w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 98.95% (283/286)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 94.69% (1409/1488)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 7.63% (18/236)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 77.49% (1855/2394)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 71.67% (301/420)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 9.25% (86/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 29.22% (135/462)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 78.68% (1048/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 46.55% (378/812)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 47.42% (441/930)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 93.23% (1418/1521)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 34.87% (544/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 75.46% (898/1190)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 45.57% (370/812)\n",
      "Questions seen/total: 73.54% (14373/19544)\n",
      "Semantic accuracy: 80.14%  (3866/4824)\n",
      "Syntactic accuracy: 55.69%  (5318/9549)\n",
      "Total accuracy: 63.90%  (9184/14373)\n"
     ]
    }
   ],
   "source": [
    "ana.evaluate_analogy_google(data.embedding, data.vocab, data.w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ana.evaluate_analogy_google(debiased_2, data.vocab, data.w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
