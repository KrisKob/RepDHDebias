{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load further embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding(url, skip_first = False):\n",
    "    \"\"\"Function to read out an embedding\n",
    "    Input: url: url to embedding\n",
    "    \n",
    "    Returns: vocab: list of words in the embedding\n",
    "             w2id: dictionary mapping words to ids\n",
    "             embedding: array storing the word vectors,\n",
    "                           row corresponds to word id\"\"\"\n",
    "    # Open url\n",
    "    data = urllib.request.urlopen(url)\n",
    "    vocab = []\n",
    "    embedding = []\n",
    "    \n",
    "    # Each line contains one word and its embedding\n",
    "    for i, line in enumerate(data):\n",
    "        if skip_first:\n",
    "            if i == 0:\n",
    "                continue\n",
    "        #if len(line) == 301:\n",
    "        line = line.decode()\n",
    "        # Split by spaces\n",
    "        split = line.split()\n",
    "        # First element(== the word) is added to vocabulary\n",
    "        vocab.append(split[0])\n",
    "        # All other elements(embedding vectors) are added to vectors\n",
    "        embedding.append([float(elem) for elem in split[1:]])\n",
    "    \n",
    "    # Create a dictionary with word-id pairs based on the order\n",
    "    w2id = {w: i for i, w in enumerate(vocab)}\n",
    "    # Vectors are converted into an array\n",
    "    embedding = np.array(embedding).astype(float)\n",
    "    \n",
    "    return vocab, w2id, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ae7f549e58ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0membedding_gn_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors300.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvocab_gn_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2id_gn_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_gn_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_gn_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvocab_gn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2id_gn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_gn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_gn_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2id_gn_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_gn_original\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-6ca00ef6ad40>\u001b[0m in \u001b[0;36mread_embedding\u001b[1;34m(url, skip_first)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mw2id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Vectors are converted into an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "embedding_gn_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors300.txt\"\n",
    "vocab_gn_original, w2id_gn_original, embedding_gn_original = read_embedding(embedding_gn_url)\n",
    "vocab_gn, w2id_gn, embedding_gn = restrict_vocab(vocab_gn_original, w2id_gn_original, embedding_gn_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias_gn(wv):\n",
    "    for v in wv:\n",
    "        assert(len(v) == 300)\n",
    "    \n",
    "    wv = wv[:,:-1]\n",
    "\n",
    "    for v in wv:\n",
    "        assert(len(v) == 299)\n",
    "    return wv\n",
    "\n",
    "vocab_gn_a = vocab_gn\n",
    "w2id_gn_a = w2id_gn\n",
    "embedding_gn_a = debias_gn(embedding_gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_hd_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors_hd.txt\"\n",
    "vocab_hd_original, w2id_hd_original, embedding_hd_original = read_embedding(embedding_hd_url)\n",
    "vocab_hd, w2id_hd, embedding_hd = restrict_vocab(vocab_hd_original, w2id_hd_original, embedding_hd_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_hd_a_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors_hd_a.txt\"\n",
    "vocab_hd_a_original, w2id_hd_a_original, embedding_hd_a_original = read_embedding(embedding_hd_a_url)\n",
    "vocab_hd_a, w2id_hd_a, embedding_hd_a = restrict_vocab(vocab_hd_a_original, w2id_hd_a_original, embedding_hd_a_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gp_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/gp_glove.txt\"\n",
    "vocab_gp_original, w2id_gp_original, embedding_gp_original = read_embedding(embedding_gp_url, skip_first = True)\n",
    "vocab_gp, w2id_gp, embedding_gp = restrict_vocab(vocab_gp_original, w2id_gp_original, embedding_gp_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size:  322636\n",
      "Restricted vocab size:  314952\n"
     ]
    }
   ],
   "source": [
    "embedding_gp_gn_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/gp_gn_glove.txt\"\n",
    "vocab_gp_gn_original, w2id_gp_gn_original, embedding_gp_gn_original = read_embedding(embedding_gp_gn_url, skip_first = True)\n",
    "vocab_gp_gn, w2id_gp_gn, embedding_gp_gn = restrict_vocab(vocab_gp_gn_original, w2id_gp_gn_original, embedding_gp_gn_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Association Test\n",
    "Implementation taken from \"https://github.com/shivaomrani/HumanBiasInSemantics\". \n",
    "With minor adjustments such as variable names for readability, see file \"weat.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import weat\n",
    "# auf embedding zugreifen, wenn wir das als .py haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Career and family\n",
    "# Change from Bill to Tom as in paper to avoid ambiguity\n",
    "male_names = [\"john\", \"paul\", \"mike\", \"kevin\", \"steve\", \"greg\", \"jeff\", \"tom\"]\n",
    "female_names = [\"amy\", \"joan\", \"lisa\", \"sarah\", \"diana\", \"kate\", \"ann\", \"donna\"]\n",
    "career_attributes = [\"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"office\", \"business\", \"career\"]\n",
    "family_attributes = [\"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math and arts\n",
    "math_words = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\"]\n",
    "arts_words1 = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"sculpture\"]\n",
    "male_attributes1 = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
    "female_attributes1 = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science and arts\n",
    "science_words = [\"science\", \"technology\", \"pyhsics\", \"chemistry\", \"einstein\", \"nasa\", \"experiment\", \"astronomy\"]\n",
    "arts_words2 = [\"poetry\", \"art\", \"shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\"]\n",
    "male_attributes2 = [\"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\"]\n",
    "female_attributes2 = [\"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100000\n",
    "wea_test = weat(male_names, female_names, career_attributes, family_attributes, iterations, embedding, w2id)\n",
    "pvalue, effect_size, sd = wea_test.getPValueAndEffect()\n",
    "print(\"p-value: \", pvalue)\n",
    "print(\"effect size: \", effect_size)\n",
    "print(\"standard deviation: \", sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analogy\n",
    "The word analogy task is to find a word *D* such that \"*A* is to *B* as *C* is to *D*\". Wang et al. (2020) have evaluated all non-debiased and debiased embeddings on the MSR word analogy task (Mikolov et al., 2013 Ling) as well as on a second Google word analogy dataset (Mikolov et al. 2013 Eff). The evaluation metric is the percentage of questions for which the correct answer is assigned the maximum score by the algorithm. The analogy task is used to show whether a debiasing method is capable of preserving desired distance relations between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'web'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2fb7b889ed50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mword_analogy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0manalogy_tasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Kristina\\Documents\\Studium\\2020_21WiSe\\Implementing ANNs with TensorFlow\\RepDSDebias\\word_analogy.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mweb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalogy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0miteritems\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mweb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'web'"
     ]
    }
   ],
   "source": [
    "from word_analogy import analogy_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
