{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded utils\n",
      "successfully loaded further_embeddings\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import utils\n",
    "import further_embeddings as data # takes a while because it loads all embedding datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'debiased_1/debiased_1.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b315d422bda0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load the two debiased embeddings obtained by our implementation of Double-Hard Debias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfile_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'debiased_1/debiased_1.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdebiased_equal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfile_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfile_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'debiased_2/debiased_2.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'debiased_1/debiased_1.p'"
     ]
    }
   ],
   "source": [
    "# load the two debiased embeddings obtained by our implementation of Double-Hard Debias\n",
    "file_1 = open('debiased_1/debiased_1.p', 'wb')\n",
    "debiased_equal = pickle.load(file_1)\n",
    "file_1.close()\n",
    "file_2 = open('debiased_2/debiased_2.p', 'wb')\n",
    "debiased_fem_male = pickle.load(file_2)\n",
    "file_2.close()\n",
    "# load the Double-Hard debiased embeddings obtained by Wang et al. (2020)\n",
    "file_3 = open('glove_dhd.p', 'wb')\n",
    "glove_dhd_wang = pickle.load(file_3)\n",
    "file_3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_embeddings = {'original_glove': (embedding, vocab, w2id), 'glove_dhd_wang': (glove_dhd_wang, vocab, w2id), \n",
    "                   'debiased_equal': (debiased_equal, vocab, w2id), 'debiased_fem_male': (debiased_fem_male, vocab, w2id)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Association Test\n",
    "Implementation taken from https://github.com/shivaomrani/HumanBiasInSemantics with minor adjustments such as variable names for readability. See file `weat.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weat import weat\n",
    "# auf embedding zugreifen, wenn wir das als Datei haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Career and family\n",
    "# Change from Bill to Tom as in paper to avoid ambiguity\n",
    "male_names = [\"john\", \"paul\", \"mike\", \"kevin\", \"steve\", \"greg\", \"jeff\", \"tom\"]\n",
    "female_names = [\"amy\", \"joan\", \"lisa\", \"sarah\", \"diana\", \"kate\", \"ann\", \"donna\"]\n",
    "career_attributes = [\"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"office\", \"business\", \"career\"]\n",
    "family_attributes = [\"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math and arts\n",
    "math_words = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\"]\n",
    "arts_words1 = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"sculpture\"]\n",
    "male_attributes1 = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
    "female_attributes1 = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science and arts\n",
    "science_words = [\"science\", \"technology\", \"physics\", \"chemistry\", \"einstein\", \"nasa\", \"experiment\", \"astronomy\"]\n",
    "arts_words2 = [\"poetry\", \"art\", \"shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\"]\n",
    "male_attributes2 = [\"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\"]\n",
    "female_attributes2 = [\"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe\n",
      "The difference of means is  0.14427443788736127\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  0.017384010507612402\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  0.031252206121280324\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "GN-GloVe\n",
      "The difference of means is  0.15499514781367907\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  -0.007112939686408026\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  0.03482070643258339\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "GN-GloVe(a)\n",
      "The difference of means is  0.12253738935760339\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  0.0195871276773687\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  0.03415733362726314\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "GP-GloVe\n",
      "The difference of means is  0.131777030757803\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  0.03140624985462637\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  0.029088766721542925\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "GP-GN-GloVe\n",
      "The difference of means is  0.14423039544817584\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  -0.0004306505088607082\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  0.02560682726766572\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "Hard-GloVe\n",
      "The difference of means is  0.07614666222798405\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  -0.014647437440999056\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  -0.005444271584565286\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "Strong-Hard-GloVe\n",
      "The difference of means is  0.07614665308574331\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  -0.015041389436191821\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "The difference of means is  -0.005642129216539615\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C &amp; F: d</th>\n",
       "      <th>C &amp; F: p</th>\n",
       "      <th>M &amp; A: d</th>\n",
       "      <th>M &amp; A: p</th>\n",
       "      <th>S &amp; A: d</th>\n",
       "      <th>S &amp; A: p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GloVe</th>\n",
       "      <td>1.805996</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.688595</td>\n",
       "      <td>0.084412</td>\n",
       "      <td>1.129869</td>\n",
       "      <td>0.011739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GN-GloVe</th>\n",
       "      <td>1.821105</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>-0.256358</td>\n",
       "      <td>0.697437</td>\n",
       "      <td>1.068954</td>\n",
       "      <td>0.016255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GN-GloVe(a)</th>\n",
       "      <td>1.755476</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.503003</td>\n",
       "      <td>0.157309</td>\n",
       "      <td>0.879733</td>\n",
       "      <td>0.038715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GP-GloVe</th>\n",
       "      <td>1.805885</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>1.208529</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>1.106440</td>\n",
       "      <td>0.013490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GP-GN-GloVe</th>\n",
       "      <td>1.797431</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>-0.012642</td>\n",
       "      <td>0.508839</td>\n",
       "      <td>0.846007</td>\n",
       "      <td>0.045448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hard-GloVe</th>\n",
       "      <td>1.546646</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>-0.982605</td>\n",
       "      <td>0.975429</td>\n",
       "      <td>-0.538376</td>\n",
       "      <td>0.859721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strong-Hard-GloVe</th>\n",
       "      <td>1.546646</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>-0.985681</td>\n",
       "      <td>0.976156</td>\n",
       "      <td>-0.547118</td>\n",
       "      <td>0.863028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   C & F: d  C & F: p  M & A: d  M & A: p  S & A: d  S & A: p\n",
       "GloVe              1.805996  0.000149  0.688595  0.084412  1.129869  0.011739\n",
       "GN-GloVe           1.821105  0.000132 -0.256358  0.697437  1.068954  0.016255\n",
       "GN-GloVe(a)        1.755476  0.000230  0.503003  0.157309  0.879733  0.038715\n",
       "GP-GloVe           1.805885  0.000146  1.208529  0.007693  1.106440  0.013490\n",
       "GP-GN-GloVe        1.797431  0.000166 -0.012642  0.508839  0.846007  0.045448\n",
       "Hard-GloVe         1.546646  0.000991 -0.982605  0.975429 -0.538376  0.859721\n",
       "Strong-Hard-GloVe  1.546646  0.001003 -0.985681  0.976156 -0.547118  0.863028"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations = 100000\n",
    "embeddings = [data.embedding, data.embedding_gn, data.embedding_gn_a, data.embedding_gp, data.embedding_gp_gn, data.embedding_hd, data.embedding_hd_a]#, glove_dhd_wang, debiased_equal, debiased_fem_male]\n",
    "w2ids = [data.w2id, data.w2id_gn, data.w2id_gn_a, data.w2id_gp, data.w2id_gp_gn, data.w2id_hd, data.w2id_hd_a]#, data.w2id, data.w2id, data.w2id]\n",
    "concept1 = [male_names, math_words, science_words]\n",
    "concept2 = [female_names, arts_words1, arts_words2]\n",
    "stereotype1 = [career_attributes, male_attributes1, male_attributes2]\n",
    "stereotype2 = [family_attributes, female_attributes1, female_attributes2]\n",
    "row_names = [\"GloVe\", \"GN-GloVe\", \"GN-GloVe(a)\", \"GP-GloVe\", \"GP-GN-GloVe\", \"Hard-GloVe\", \"Strong-Hard-GloVe\"]#, \"Double-Hard-Glove(Authors)\", \"Double-Hard-GloVe(Equal)\", \"Double-Hard-GloVe(Fem-Male)\"]\n",
    "column_names = [\"C & F: d\", \"C & F: p\", \"M & A: d\", \"M & A: p\", \"S & A: d\", \"S & A: p\"]\n",
    "test_results = []\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    result = []\n",
    "    print(row_names[i])\n",
    "    for j in range(3):\n",
    "        wea_test = weat(concept1[j], concept2[j], stereotype1[j], stereotype2[j], iterations, embedding, w2ids[i])\n",
    "        pvalue, effect_size, _ = wea_test.getPValueAndEffect()\n",
    "        result.append(effect_size)\n",
    "        result.append(pvalue)\n",
    "    test_results.append(result)\n",
    "    \n",
    "test_results = np.array(test_results).astype(float)\n",
    "df = pd.DataFrame(data = test_results, index = row_names, columns = column_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogy\n",
    "The word analogy task is to find a word *D* such that \"*A* is to *B* as *C* is to *D*\". Wang et al. (2020) have evaluated all non-debiased and debiased embeddings on the MSR word analogy task [Mikolov et al., 2013a](https://www.aclweb.org/anthology/N13-1090/) as well as on a second Google word analogy dataset [Mikolov et al. 2013b](https://arxiv.org/abs/1301.3781v3). The evaluation metric is the percentage of questions for which the correct answer is assigned the maximum score by the algorithm. The analogy task is used to show whether a debiasing method is capable of preserving desired distance relations between words. The implementation was taken from Wang et al. (2020) with some adjustments to get it running: `analogy_tasks.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analogy_tasks as ana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4884\n",
      "ACCURACY TOP1-MSR: 54.40% (2657/4884)\n"
     ]
    }
   ],
   "source": [
    "# MSR on all embeddings:\n",
    "for key, (embedding, vocab, w2id) in dict_embeddings.items():\n",
    "    print(key, \": \")\n",
    "    ana.evaluate_analogy_msr(embedding, vocab, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ana.evaluate_analogy_msr(debiased_1, data.vocab, data.w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 98.95% (283/286)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 94.69% (1409/1488)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 7.63% (18/236)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 77.49% (1855/2394)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 71.67% (301/420)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 9.25% (86/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 29.22% (135/462)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 78.68% (1048/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 46.55% (378/812)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 47.42% (441/930)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 93.23% (1418/1521)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 34.87% (544/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 75.46% (898/1190)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 45.57% (370/812)\n",
      "Questions seen/total: 73.54% (14373/19544)\n",
      "Semantic accuracy: 80.14%  (3866/4824)\n",
      "Syntactic accuracy: 55.69%  (5318/9549)\n",
      "Total accuracy: 63.90%  (9184/14373)\n"
     ]
    }
   ],
   "source": [
    "ana.evaluate_analogy_google(data.embedding, data.vocab, data.w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ana.evaluate_analogy_google(debiased_2, data.vocab, data.w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
