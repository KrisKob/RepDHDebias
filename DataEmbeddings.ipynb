{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "from sklearn import metrics as sk_m\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to retrive pre-trained 300 dimensional gloVe embedding\n",
    "embedding_300_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors.txt\"\n",
    "\n",
    "def read_embedding(url):\n",
    "    \"\"\"Function to read out an embedding\n",
    "    Input: url: url to embedding\n",
    "    \n",
    "    Returns: vocab: list of words in the embedding\n",
    "             w2id: dictionary mapping words to ids\n",
    "             embedding: array storing the word vectors,\n",
    "                           row corresponds to word id\"\"\"\n",
    "    # Open url\n",
    "    data = urllib.request.urlopen(url)\n",
    "    vocab = []\n",
    "    embedding = []\n",
    "    \n",
    "    # Each line contains one word and its embedding\n",
    "    for line in data:\n",
    "        if len(line) == 301:\n",
    "            line = line.decode()\n",
    "            # Split by spaces\n",
    "            split = line.split()\n",
    "            # First element(== the word) is added to vocabulary\n",
    "            vocab.append(split[0])\n",
    "            # All other elements(embedding vectors) are added to vectors\n",
    "            embedding.append([float(elem) for elem in split[1:]])\n",
    "    \n",
    "    # Create a dictionary with word-id pairs based on the order\n",
    "    w2id = {w: i for i, w in enumerate(vocab)}\n",
    "    # Vectors are converted into an array\n",
    "    embedding = np.array(embedding).astype(float)\n",
    "    \n",
    "    return vocab, w2id, embedding\n",
    "    \n",
    "vocab_original, w2id_original, embedding_original = read_embedding(embedding_300_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasDigit(word):\n",
    "    \"\"\"Checks if a string contains any digits\"\"\"\n",
    "    return any(char.isdigit() for char in word)\n",
    "\n",
    "def hasSpecialChar(word):\n",
    "    \"\"\"Checks if a string contains special characters(except \"_\")\"\"\"\n",
    "    special_characters = \"!@#$%^&*()-+?=,<>/.\"\n",
    "    return any(char in special_characters for char in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_vocab(vocab, w2id, embedding):\n",
    "    \"\"\"Limits the vocab by removing words containing digits or special characters\n",
    "    Input: vocab: list of words in the embedding\n",
    "           w2id: dictionary mapping words to ids\n",
    "           embedding: array storing the word vectors\n",
    "           \n",
    "    Returns: limit_vocab: list of words in vocab that do not include digits or special characters\n",
    "             limit_w2id: dictionary mapping words in limit_vocab to new ids\n",
    "             limit_embedding: array storing the word vectors of the words in limit_vocab only\"\"\"\n",
    "    limit_vocab = []\n",
    "    limit_embedding = []\n",
    "    \n",
    "    for word in vocab:\n",
    "        # If word includes either a digit or a special character move on to next word\n",
    "        if hasDigit(word) or hasSpecialChar(word):\n",
    "            continue\n",
    "        # Else add word to limit_vocab and its embedding to limit_embedding    \n",
    "        limit_vocab.append(word)\n",
    "        limit_embedding.append(embedding[w2id[word]])\n",
    "        \n",
    "    # Convert embedding into an array    \n",
    "    limit_embedding = np.array(limit_embedding).astype(float)\n",
    "    # Create new dictionary containing only the words in limit_vocab and their new ids\n",
    "    limit_w2id = {word: i for i, word in enumerate(limit_vocab)}\n",
    "    \n",
    "    return limit_vocab, limit_w2id, limit_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size:  322636\n",
      "Restricted vocab size:  314952\n"
     ]
    }
   ],
   "source": [
    "vocab, w2id, embedding = restrict_vocab(vocab_original, w2id_original, embedding_original)\n",
    "print(\"Original vocab size: \", len(vocab_original))\n",
    "print(\"Restricted vocab size: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_vocab(vocab, exclude):\n",
    "    \"\"\"Function to exclude specific words from vocabulary\n",
    "    Input: vocab: list of words in the embedding\n",
    "           exclude: list of words to exclude from the vocabulary\n",
    "           \n",
    "    Returns: limited_vocab: vocab without the words in exclude\"\"\"\n",
    "    # Create copy of vocab\n",
    "    limited_vocab = vocab.copy()\n",
    "    # For all words that are in exclude and vocab\n",
    "    for word in exclude:\n",
    "        if word in limited_vocab:\n",
    "            # Remove word from vocab\n",
    "            limited_vocab.remove(word)\n",
    "            \n",
    "    return limited_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to female specific words as listed by the authors\n",
    "female_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/female_word_file.txt\"\n",
    "female_words_data = urllib.request.urlopen(female_words_url)\n",
    "\n",
    "# List of female words\n",
    "female_words = []\n",
    "for line in female_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    female_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to male specific words as listed by the authors\n",
    "male_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/male_word_file.txt\"\n",
    "male_words_data = urllib.request.urlopen(male_words_url)\n",
    "\n",
    "# List of male words\n",
    "male_words = []\n",
    "for line in male_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    male_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List with female - male pairs from female-male specific words\n",
    "female_male_pairs = []\n",
    "for i, female in enumerate(female_words):\n",
    "    female_male_pairs.append([female, male_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to the files storing gender specific words as listed by the authors\n",
    "gender_specific_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/gender_specific_full.json\"\n",
    "\n",
    "# Empty list to accumulate gender specific words plus additional list after lowercasing\n",
    "gender_specific_original = []\n",
    "gender_specific = []\n",
    "\n",
    "\n",
    "# Read out URL and add further gender specific words\n",
    "with urllib.request.urlopen(gender_specific_url) as f:\n",
    "    gender_specific_original.extend(json.load(f))\n",
    "\n",
    "# Add lower case words to second list\n",
    "for word in gender_specific_original:\n",
    "    gender_specific.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing definitional pairs as listed by the authors\n",
    "definitial_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/definitional_pairs.json\"\n",
    "\n",
    "# Empty list to store definitional pairs plus additional list after lowercasing\n",
    "definitional_pairs_original = []\n",
    "definitional_pairs = []\n",
    "\n",
    "\n",
    "# Read out url and add pairs in list\n",
    "with urllib.request.urlopen(definitial_pairs_url) as f:\n",
    "    definitional_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in definitional_pairs_original:\n",
    "    definitional_pairs.append([w1.lower(), w2.lower()])\n",
    "\n",
    "\n",
    "# Create list of single words instead of pairs  \n",
    "definitional_words = []\n",
    "for pair in definitional_pairs:\n",
    "    for word in pair:\n",
    "        definitional_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing the equalize pairs as listed by the authors\n",
    "equalize_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/equalize_pairs.json\"\n",
    "\n",
    "# Empty list to store equalize pairs plus additional list after lowercasing\n",
    "equalize_pairs_original = []\n",
    "equalize_pairs = []\n",
    "\n",
    "# Read out URL and add pairs to list\n",
    "with urllib.request.urlopen(equalize_pairs_url) as f:\n",
    "    equalize_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in equalize_pairs_original:\n",
    "    equalize_pairs.append([w1.lower(), w2.lower()])\n",
    "    \n",
    "# Create list of single words instead of pairs\n",
    "equalize_words = []\n",
    "for pair in equalize_pairs:\n",
    "    for word in pair:\n",
    "        equalize_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all gender specific words included in \n",
    "# female words, male words, gender specific words, equalize words and definitional words\n",
    "exclude_words = list(set(female_words + male_words + gender_specific + definitional_words + equalize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  314952\n",
      "Neutral vocab size:  314293\n"
     ]
    }
   ],
   "source": [
    "# Remove gender specific words from the embedding to obtain vocabulary of neutral words\n",
    "vocab_neutral = exclude_vocab(vocab, exclude_words)\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "print(\"Neutral vocab size: \", len(vocab_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(word, w2id=w2id, embedding=embedding):\n",
    "    return embedding[w2id[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load further embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gn_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors300.txt\"\n",
    "vocab_gn_original, w2id_gn_original, embedding_gn_original = read_embedding(embedding_gn_url)\n",
    "vocab_gn, w2id_gn, embedding_gn = restrict_vocab(vocab_gn_original, w2id_gn_original, embedding_gn_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_hd_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors_hd.txt\"\n",
    "vocab_hd_original, w2id_hd_original, embedding_hd_original = read_embedding(embedding_hd_url)\n",
    "vocab_hd, w2id_hd, embedding_hd = restrict_vocab(vocab_hd_original, w2id_hd_original, embedding_hd_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_hd_a_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors_hd_a.txt\"\n",
    "vocab_hd_a_original, w2id_hd_a_original, embedding_hd_a_original = read_embedding(embedding_hd_a_url)\n",
    "vocab_hd_a, w2id_hd_a, embedding_hd_a = restrict_vocab(vocab_hd_a_original, w2id_hd_a_original, embedding_hd_a_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gp_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/gp_glove.txt\"\n",
    "vocab_gp_original, w2id_gp_original, embedding_gp_original = read_embedding(embedding_gp_url)\n",
    "vocab_gp, w2id_gp, embedding_gp = restrict_vocab(vocab_gp_original, w2id_gp_original, embedding_gp_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gp_gn_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/gp_gn_glove.txt\"\n",
    "vocab_gp_gn_original, w2id_gp_gn_original, embedding_gp_gn_original = read_embedding(embedding_gp_gn_url)\n",
    "vocab_gp_gn, w2id_gp_gn, embedding_gp_gn = restrict_vocab(vocab_gp_gn_original, w2id_gp_gn_original, embedding_gp_gn_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idtfy_gender_subspace(word_sets, w2id, defining_sets, embedding, k=1):\n",
    "    \"\"\"\n",
    "    identifies the bias (gender) subspace following Bolukbasi et al. 2016\n",
    "    \n",
    "    takes\n",
    "    word_sets: vocabulary\n",
    "    w2id: a dictionary to translate words contained in the vocabulary into their corresponding IDs\n",
    "    defining_sets: N defining sets (pairs if I=2) consisting of I words that differ mainly on the bias (gender) direction\n",
    "    embedding: the embedding of the vocabulary\n",
    "    k: an integer parameters that defines how many rows of SVD(C) constitute the bias (gender) subspace B, bias (gender) direction if k=1\n",
    "    \n",
    "    returns\n",
    "    bias_subspace: linear bias (gender) subspace (direction) that is assumed to capture most of the gender bias (denoted as B in Bolukbasi et al. 2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(defining_sets) # we have 10 pairs in our defining_sets\n",
    "    I = len(defining_sets[0]) # = 2 in case of pairs (as for gender bias)\n",
    "    embedding_size = len(embedding[0]) # our embedding size is 300\n",
    "    \n",
    "    C = np.zeros((N, I, embedding_size))\n",
    "        \n",
    "    for n, d_set in enumerate(defining_sets):\n",
    "        mean_vector = np.zeros((I, embedding_size))\n",
    "        for i, word in enumerate(d_set):\n",
    "            mean_vector[i] = embed(word) / I\n",
    "            C[n][i] = (embed(word) - mean_vector[i]) * np.transpose((embed(word) - mean_vector[i])) / I\n",
    "            \n",
    "    #C = []\n",
    "    #for female_word, male_word in defining_sets:\n",
    "    #    mean = (embed(female_word) + embed(male_word)) / 2\n",
    "    #    C.append(embed(female_word) - mean)\n",
    "    #    C.append(embed(male_word) - mean)\n",
    "    \n",
    "    #C = np.array(C)\n",
    "    \n",
    "    print(\"C\",C.shape)\n",
    "    #print(C)\n",
    "    #_, SVD_C, _ = np.linalg.svd(C)\n",
    "    SVD_C = np.linalg.svd(C, compute_uv = False)\n",
    "    print(\"no uv\", SVD_C.shape)\n",
    "    B = SVD_C[:k]\n",
    "    print(\"B\",B)\n",
    "    \n",
    "    \n",
    "    print(\"cov\")\n",
    "    # as proven by Vargas & Cotterell 2020 the matrix C from Bolukbasi et al. 2016 can be written as an empirical covariance matrix for I = 2\n",
    "    matrix = []\n",
    "    for a, b in defining_sets:\n",
    "        center = (embed(a) + embed(b))/2\n",
    "        matrix.append(embed(a)-center)\n",
    "        matrix.append(embed(b)-center)\n",
    "    matrix = np.array(matrix)\n",
    "    print(\"matrix\", matrix.shape)\n",
    "    #print(matrix)\n",
    "    \n",
    "    # have to apply PCA when interpreting C as covariance matrix\n",
    "    pca = PCA(n_components = 10)\n",
    "    pca.fit(matrix)\n",
    "    print(\"pca\",pca.components_[0].shape)\n",
    "    # take the first pca\n",
    "    pca = pca.components_[0]\n",
    "    \n",
    "    SVD_matrix = np.linalg.svd(C, compute_uv=False)\n",
    "    svd = SVD_matrix[:k]\n",
    "    print(\"svd\",svd)\n",
    "\n",
    "    \n",
    "    print(\"new_B\")\n",
    "    array = np.ndarray((10,2,300))\n",
    "    #i=0\n",
    "    array_two = np.zeros((10,300,300))\n",
    "    for j, d_pair in enumerate(definitional_pairs):\n",
    "        for i, word in enumerate(d_pair):\n",
    "            # fill array with embeddings\n",
    "            array[j][i] = embedding[w2id[word]]\n",
    "            #i = i+1\n",
    "        # print(array[j][0].shape)\n",
    "        # calculate covariance between embeddings of same definitional pair?\n",
    "        array_two[j]=np.cov(np.transpose(array[j]))\n",
    "        \n",
    "    print(array_two.shape)\n",
    "    #new_C = np.cov(array)\n",
    "    #print(np.shape(new_C))\n",
    "    _, new_SVD_C, _ = np.linalg.svd(array_two)\n",
    "    print(np.shape(new_SVD_C))\n",
    "    new_B = new_SVD_C[:k]\n",
    "    print(new_B.shape)\n",
    "    \n",
    "    return B, new_B, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C (10, 2, 300)\n",
      "no uv (10, 2)\n",
      "B [[1.41470925 0.24232465]]\n",
      "cov\n",
      "matrix (20, 300)\n",
      "pca (300,)\n",
      "svd [[1.41470925 0.24232465]]\n",
      "new_B\n",
      "(10, 300, 300)\n",
      "(10, 300)\n",
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "B, new_B, pca = idtfy_gender_subspace(vocab, w2id, definitional_pairs, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most biased 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most biased male and female words\n",
    "def most_biased(embedding, B, k=500):\n",
    "    # small x, else memory issues\n",
    "    x = 50000\n",
    "    all_biased = np.ndarray((x,1))\n",
    "    for i, word in enumerate(embedding):\n",
    "        if i < x:\n",
    "            all_biased[i] = (sk_m.pairwise.cosine_similarity(word.reshape(1, 300), B.reshape(1, 300)))[0]\n",
    "            # print(sk_m.pairwise.cosine_similarity(word.reshape(1,300), B)[0])\n",
    "    #print(all_biased)\n",
    "    most_biased_f = []\n",
    "    most_biased_m = []\n",
    "    for word in range(k):\n",
    "        # female words\n",
    "        fb_index = np.argmin(all_biased)\n",
    "        most_biased_f.append(fb_index)\n",
    "        all_biased[fb_index] = 0\n",
    "        # male words\n",
    "        mb_index = np.argmax(all_biased)\n",
    "        most_biased_m.append(mb_index)\n",
    "        all_biased[mb_index] = 0\n",
    "    #print(most_biased_f, most_biased_m)\n",
    "    return most_biased_f, most_biased_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_f, index_m = most_biased(embedding, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female ['actress', 'pregnant', 'louise', 'therese', 'abbess', 'sister', 'chairwoman', 'alumna', 'princess', 'ballerina', 'maid', 'headmistress', 'pregnancy', 'josephine', 'olga', 'spinster', 'businesswoman', 'socialite', 'woman', 'heroine', 'congresswoman', 'matron', 'emmeline', 'seductive', 'uterus', 'feminist', 'actresses', 'feisty', 'princesses', 'mary', 'herself', 'suffragette', 'suffragist', 'louisa', 'ellen', 'countess', 'waitress', 'emily', 'goddess', 'girl', 'aunt', 'menstruation', 'sisters', 'governess', 'agnes', 'laura', 'duchess', 'filipina', 'archduchess', 'kuznetsova', 'daughters', 'svetlana', 'menstrual', 'noblewoman', 'katherine', 'mother', 'grandmother', 'elsa', 'nun', 'handbag', 'rebecca', 'prostitute', 'marchioness', 'sophie', 'valentina', 'irene', 'girlfriend', 'niece', 'glamorous', 'devi', 'francisca', 'manuela', 'millicent', 'nina', 'housewife', 'henriette', 'sultry', 'hilda', 'doreen', 'virgen', 'doña', 'inna', 'helene', 'thérèse', 'edith', 'baroness', 'schoolgirl', 'sophia', 'courtesan', 'tatiana', 'karolina', 'esther', 'betty', 'née', 'phoebe', 'lydia', 'miscarriage', 'emma', 'margaret', 'sheila', 'anna', 'seamstress', 'helen', 'godmother', 'patricia', 'lillian', 'breasts', 'supermodel', 'elisabeth', 'irina', 'womb', 'susan', 'christina', 'elizabeth', 'daniela', 'eliza', 'latina', 'yelena', 'needlework', 'amalia', 'frau', 'mademoiselle', 'tomboy', 'her', 'kathleen', 'lady', 'margareta', 'housekeeper', 'sorceress', 'diva', 'miss', 'consuelo', 'fatima', 'naomi', 'motherhood', 'jane', 'wollstonecraft', 'teresa', 'genevieve', 'alejandra', 'ann', 'caroline', 'lavinia', 'antonia', 'matriarch', 'bernadette', 'natalya', 'amelia', 'wilhelmina', 'lucy', 'constance', 'nurse', 'rachel', 'midwife', 'antoinette', 'magdalene', 'katie', 'alicia', 'eunice', 'cora', 'celeste', 'ekaterina', 'klara', 'eugenia', 'widowed', 'claudia', 'mildred', 'ingeborg', 'christine', 'martha', 'stepdaughter', 'blige', 'gabriela', 'annabelle', 'melanie', 'ursuline', 'diana', 'elena', 'juanita', 'rhoda', 'becky', 'eileen', 'ursula', 'marie', 'angelique', 'karin', 'patroness', 'madeleine', 'granddaughter', 'infanta', 'natalia', 'cervix', 'leona', 'dagmar', 'eva', 'fetus', 'rosalie', 'gertrude', 'lizzie', 'albertine', 'henrietta', 'bessie', 'nadezhda', 'postpartum', 'anne', 'selina', 'women', 'virginity', 'bombshell', 'maría', 'amalie', 'luise', 'frederica', 'katharina', 'tatyana', 'petrova', 'priestess', 'paulina', 'miriam', 'tamara', 'alina', 'irena', 'hildegard', 'angelica', 'magdalena', 'theresa', 'dorothea', 'julia', 'johanna', 'luisa', 'childbirth', 'femininity', 'hostess', 'blonde', 'minerva', 'erika', 'begum', 'sabrina', 'ida', 'mathilde', 'cheerleader', 'blouse', 'galina', 'sultana', 'lina', 'funnels', 'lesbian', 'nancy', 'sarah', 'amélie', 'aleksandra', 'maud', 'megan', 'elise', 'amy', 'monika', 'jenny', 'lilian', 'radwańska', 'carlotta', 'tanya', 'jeanne', 'marianne', 'sofía', 'ana', 'leah', 'contralto', 'beautiful', 'elaine', 'georgina', 'beatrice', 'harriet', 'emilie', 'earrings', 'amanda', 'ovaries', 'pankhurst', 'maharani', 'she', 'rani', 'goddesses', 'gabriella', 'cheryl', 'arabella', 'nuns', 'katharine', 'lorna', 'kumari', 'juana', 'maryam', 'thelma', 'astrid', 'edna', 'georgiana', 'ilse', 'lena', 'isabel', 'cécile', 'spokeswoman', 'bint', 'queen', 'theotokos', 'rms', 'hortense', 'sharapova', 'jessica', 'winifred', 'maternity', 'moorings', 'anita', 'dowager', 'womanhood', 'amina', 'flirtatious', 'charlotte', 'ethel', 'queenie', 'tania', 'bardot', 'lorena', 'jennifer', 'isabelle', 'hitomi', 'sorority', 'ada', 'pregnancies', 'secondly', 'heather', 'ywca', 'mothers', 'poppins', 'madame', 'vagina', 'necklace', 'susanne', 'leyla', 'beatriz', 'jacqueline', 'yvonne', 'viktoria', 'katerina', 'ramona', 'frances', 'odette', 'beaded', 'nathalie', 'topless', 'glamour', 'hadassah', 'cynthia', 'bethany', 'eleonora', 'yulia', 'augusta', 'kom', 'michelle', 'isobel', 'lucretia', 'maria', 'caterina', 'betsy', 'stepmother', 'joanne', 'hélène', 'cecilia', 'samantha', 'janie', 'femina', 'homemaker', 'aline', 'girls', 'eleanor', 'rita', 'marguerite', 'wilhelmine', 'gisela', 'eloise', 'beatrix', 'gowns', 'mme', 'valerie', 'bianca', 'lesbianism', 'annika', 'ulithi', 'bridget', 'sonia', 'maids', 'fatale', 'margarete', 'nadine', 'ilona', 'veronica', 'auntie', 'liza', 'catherine', 'adeline', 'brunette', 'lilith', 'madeline', 'faye', 'hysterical', 'kate', 'sara', 'dbe', 'amma', 'daria', 'necklaces', 'estrogen', 'rihanna', 'paola', 'jana', 'renee', 'daughter', 'alexandra', 'mrs', 'elsie', 'marija', 'pamela', 'hijab', 'alumnae', 'heiress', 'vanessa', 'receptionist', 'elisa', 'claudine', 'hera', 'marjorie', 'agatha', 'celia', 'lourdes', 'clarice', 'denise', 'ní', 'maggie', 'jelena', 'germaine', 'promiscuous', 'salome', 'ovulation', 'menopause', 'mollie', 'breastfeeding', 'ciara', 'hedwig', 'mistress', 'margrethe', 'polly', 'lynette', 'allure', 'monique', 'justine', 'donna', 'sassy', 'leda', 'majuro', 'cristina', 'chiara', 'liz', 'akiko', 'anja', 'claire', 'eugenie', 'charlene', 'fallin', 'lenore', 'gayatri', 'karen', 'susanna', 'gabrielle', 'athena', 'felicity', 'joanna', 'nora', 'inés', 'leila', 'margherita', 'fernanda', 'aurelia', 'ruth', 'susannah', 'brenda', 'feminism', 'karina', 'angela', 'debbie', 'chaste', 'faustina', 'nanny', 'kayla', 'landlady', 'ulrika', 'mimi', 'molly', 'wife', 'kristin', 'nymph', 'fiancée', 'lola', 'ayesha']\n",
      "male ['john', 'himself', 'his', 'brother', 'led', 'son', 'colonel', 'successor', 'nephew', 'footballing', 'sir', 'uncle', 'general', 'brothers', 'elway', 'he', 'tackle', 'linemen', 'nhl', 'deere', 'journeyman', 'governorship', 'brigadier', 'apprenticed', 'father', 'hooker', 'punter', 'marshal', 'captaincy', 'daud', 'generals', 'man', 'mayall', 'succeeded', 'henry', 'balliol', 'julius', 'leaguer', 'william', 'appointed', 'godfrey', 'captain', 'wingman', 'successors', 'trombonist', 'predecessor', 'mahdi', 'dibiase', 'fullback', 'domnall', 'papacy', 'inventor', 'engineer', 'military', 'defensive', 'linebacker', 'grandson', 'mcculloch', 'pontificate', 'dawkins', 'legate', 'james', 'mechanic', 'invented', 'ernest', 'surveyor', 'iii', 'muhammed', 'commander', 'czw', 'cardinal', 'irvin', 'receiver', 'sulayman', 'bulls', 'command', 'ibn', 'sayyid', 'dso', 'greats', 'under', 'plow', 'sax', 'robert', 'manager', 'explorer', 'decepticon', 'archbishop', 'grandfather', 'quarterback', 'klitschko', 'charles', 'commanded', 'sefer', 'pitchers', 'players', 'voivode', 'popper', 'antiquary', 'nfl', 'george', 'commentary', 'commanders', 'tt', 'haydn', 'usmc', 'andré', 'cricketing', 'joseph', 'hasidim', 'luthier', 'milton', 'racking', 'acumen', 'hijo', 'shocker', 'óscar', 'johann', 'associates', 'striker', 'shogun', 'jr', 'donald', 'setup', 'proto', 'ordnance', 'hannibal', 'englishman', 'xxii', 'abd', 'ernie', 'reliever', 'drafted', 'suspensions', 'playfair', 'praetorian', 'richard', 'contractor', 'episcopacy', 'snr', 'topographical', 'thomas', 'david', 'enforcers', 'starting', 'samuel', 'dewey', 'vasco', 'pacification', 'chairman', 'maestro', 'veteran', 'wayne', 'trombone', 'fought', 'bsk', 'chairmanship', 'king', 'enforcer', 'shapur', 'mercenaries', 'dalton', 'magister', 'adl', 'boyhood', 'philip', 'scoring', 'leader', 'halfback', 'shortstop', 'megatron', 'alexander', 'corps', 'ioan', 'mustapha', 'messrs', 'mullan', 'batchelor', 'army', 'drummer', 'johannes', 'relativity', 'abbot', 'khwaja', 'kamel', 'gough', 'hector', 'tex', 'franjo', 'famer', 'irl', 'edward', 'jovan', 'strongman', 'corinthians', 'fealty', 'antipope', 'linebackers', 'stonemason', 'preached', 'muhammad', 'sergeant', 'predecessors', 'positional', 'gary', 'apprenticeship', 'gottfried', 'galvatron', 'dooley', 'rebelled', 'bradman', 'electronic', 'aerospace', 'congressman', 'builder', 'stephen', 'hardline', 'ioannis', 'rule', 'johnny', 'chief', 'frusciante', 'foundry', 'albert', 'hoare', 'defensively', 'reverend', 'khl', 'midfield', 'dfc', 'mr', 'zahir', 'firm', 'corporal', 'returner', 'jonson', 'bruce', 'ironworks', 'architect', 'mechanics', 'saladin', 'frederick', 'pope', 'expeditions', 'stephenson', 'drums', 'gaspar', 'attila', 'dave', 'lieutenant', 'blacksmith', 'tsang', 'augustus', 'reginald', 'treatise', 'stuntman', 'dionysius', 'abbas', 'sons', 'mainz', 'militia', 'oberst', 'papal', 'tyndall', 'signings', 'nikola', 'wallace', 'agricola', 'abdallah', 'hap', 'galbraith', 'sabres', 'reorganizing', 'caliphate', 'carpentry', 'trumpet', 'olivares', 'businessman', 'cb', 'brian', 'beal', 'steve', 'khurasan', 'nll', 'magruder', 'muscovy', 'qasim', 'soult', 'mike', 'nexus', 'plato', 'flavius', 'antiquarian', 'mathias', 'xizong', 'monro', 'major', 'yazid', 'joe', 'backfield', 'nba', 'expedition', 'gregg', 'tim', 'leadership', 'carl', 'tenure', 'reputation', 'bass', 'partnership', 'iv', 'unix', 'saxophonist', 'marmaduke', 'capo', 'pershing', 'quintus', 'patriarch', 'resigned', 'ritter', 'benjamin', 'antigonus', 'ewald', 'byzantines', 'paul', 'ken', 'hugh', 'martyn', 'dempsey', 'jürgen', 'catcher', 'imperium', 'jeremiah', 'heenan', 'harold', 'franchises', 'maimonides', 'horsemen', 'bahadur', 'almagro', 'him', 'bud', 'virgil', 'redman', 'clement', 'aldo', 'councilman', 'bull', 'made', 'karl', 'adjutant', 'redskins', 'ahmad', 'foreman', 'appointment', 'mlb', 'harmonica', 'forwards', 'aforesaid', 'sheriff', 'governor', 'bartram', 'pipe', 'printer', 'herbert', 'lefebvre', 'contractors', 'yards', 'agro', 'colossal', 'cowboys', 'lefty', 'punting', 'cena', 'ludwig', 'johnson', 'cristiano', 'heavyweight', 'bench', 'vader', 'méliès', 'percival', 'socrates', 'horace', 'hirst', 'bassist', 'chiefs', 'gm', 'rhino', 'edwin', 'microscope', 'peter', 'burgoyne', 'kirby', 'armagh', 'goalie', 'sauron', 'óg', 'ruckus', 'signing', 'xi', 'gar', 'commentaries', 'guy', 'proconsul', 'racing', 'platoon', 'cuvier', 'aron', 'saxophone', 'babbitt', 'fairbairn', 'titus', 'hyder', 'clemons', 'career', 'marwan', 'gnaeus', 'guitar', 'taco', 'stadtholder', 'speight', 'master', 'replaced', 'xviii', 'greco', 'stearns', 'llewellyn', 'lytle', 'hisham', 'goc', 'kinsman', 'cigar', 'georg', 'shaykh', 'blaney', 'keith', 'pierre', 'rickard', 'vc', 'elwood', 'vern', 'steuart', 'rouse', 'rookie', 'leafs', 'wars', 'louis', 'bernard', 'elihu', 'dick', 'christopher', 'nasl', 'historian', 'arnold', 'máscara', 'header', 'athanasius', 'scored', 'reorganized', 'was', 'belushi', 'spengler', 'ivan', 'pistons', 'contender', 'pemberton', 'jim', 'knopfler', 'numan', 'league', 'team', 'battlefield', 'ashikaga', 'gotti', 'sturgis', 'subordinates', 'guard', 'whig', 'nishapur', 'frémont', 'rewarded', 'promoted', 'adrian', 'feudalism', 'fr', 'gottlob', 'friedrich', 'rookies', 'lineman', 'doc', 'eton', 'cymbal', 'epc', 'gotthard', 'nebuchadnezzar', 'enterprises', 'sawmill', 'outfielder', 'baronetcy']\n"
     ]
    }
   ],
   "source": [
    "female_most_biased = [vocab[i] for i in index_f]\n",
    "print(\"female\", female_most_biased)\n",
    "male_most_biased = [vocab[i] for i in index_m]\n",
    "print(\"male\", male_most_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Hard Debias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector projection of a vector a on (or onto) a nonzero vector b, (also known as the vector component or vector resolution of a in the direction of b), is the orthogonal projection of a onto a straight line parallel to b. It is a vector parallel to b, defined as:\n",
    "\n",
    "${\\displaystyle \\mathbf a_{1}} =  a_{1} {\\mathbf {\\hat {b}} \\,} $\n",
    "\n",
    "where $a_{1}$ is a scalar, called the scalar projection of a onto b, and ${\\mathbf {\\hat {b}} \\,}$ is the unit vector in the direction of b.\n",
    "\n",
    "In turn, the scalar projection is defined as:[2]\n",
    "\n",
    "$    a_{1} =  a ⋅ {\\mathbf {\\hat {b}} \\,}= a ⋅ \\frac{b}{‖ b ‖} $\n",
    "\n",
    "where the operator ⋅ denotes a dot product, ‖b‖ is the length of b.\n",
    "\n",
    "The **vector component** or vector resolute of a perpendicular to b, sometimes also called the **vector rejection of a from b** is the orthogonal projection of a onto the plane (or, in general, hyperplane) orthogonal to b. Both the projection $a_{1}$ and rejection $a_{2}$ of a vector a are vectors, and their sum is equal to a,[1] which implies that the rejection is given by: \n",
    "\n",
    "$a_{2} = a − a_{1}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_hard_debias(words, index_m, index_f, w2id):\n",
    "    \"\"\"Double Hard Debias:\n",
    "    \n",
    "    words: word embeddings of some corpus\n",
    "    index_m: indices of most biased male words \n",
    "    index_f: indices of most biased female words \n",
    "    w2id:\n",
    "    \"\"\"\n",
    "    # print(words[index_m[0]])\n",
    "    males = np.asarray([words[i] for i in index_m])\n",
    "    females = np.asarray([words[i] for i in index_f])    \n",
    "    \n",
    "    # decentralize all of the embeddings\n",
    "    # first calculate mean over full vocab\n",
    "    mu = ((len(words)**(-1)) * np.sum(words, axis=0)).reshape(1,300)\n",
    "    \n",
    "    # restrict corpus for PCA due to Error:\n",
    "    # Unable to allocate 721. MiB for an array with shape (314952, 300) and data type float64\n",
    "    #x = 1000\n",
    "    \n",
    "    words_decen = np.zeros((len(words),300))\n",
    "    # then subtract mean from each word embedding\n",
    "    for index, embedding in enumerate(words):\n",
    "        #if index < len(words):\n",
    "        # print(index,\":\",embedding)\n",
    "        words_decen[index] = embedding - mu\n",
    "    \n",
    "    #print(\"decentralized:\",words_decen.shape)\n",
    "    #print(\"origin:\",words)\n",
    "        \n",
    "    # discover the frequency direction    \n",
    "    #2. for all decentralized embeddings: compute PCA\n",
    "    #princ_comp = np.asarray(pca_tft(words_decen))\n",
    "    #print(\"Principal Components:\",princ_comp)\n",
    "    pca_freq = PCA().fit(words_decen)\n",
    "    # print(\"250\",pca_freq.components_[250])\n",
    "    # print(\"251\",pca_freq.components_[251])\n",
    "    # print(pca.components_)\n",
    "    # princ_comp = pca.components_\n",
    "    #print(\"Sklearn PCs:\", pca_freq.components_.shape)\n",
    "\n",
    "    evaluations = []\n",
    "\n",
    "    #3. for all principal components:\n",
    "    # in implementation of paper only look at 20 first PCs\n",
    "    for pc in pca_freq.components_:\n",
    "        male_proj = np.zeros((len(males),300))\n",
    "        male_debias = np.zeros((len(males),300))\n",
    "        female_proj = np.zeros((len(females),300))\n",
    "        female_debias = np.zeros((len(females),300))\n",
    "    \n",
    "        \n",
    "        pc= pc.reshape(1,300)\n",
    "        \n",
    "        for index, male in enumerate(males):\n",
    "        #male embedding = decentralized embedding - projected embedding into direction of PC\n",
    "            # print(male.shape)\n",
    "            male.reshape((1,300))\n",
    "            #print(((male @np.transpose(pc.reshape(1,300)))*pc.reshape(1,300)).shape)\n",
    "            #print((male-mu).shape, ((np.transpose(pc.reshape(1,300))*male)@pc).shape)\n",
    "            #print(\"pc\", pc.shape, np.transpose(pc.reshape(300,1)).shape)\n",
    "            male_proj[index] = (male - mu) - ((male @ np.transpose(unit_vec(pc))))\n",
    "            #with all new male embeddings: HardDebias\n",
    "            #print(male_proj[index].shape)\n",
    "            male_debias[index] = hard_debias(male_proj[index])\n",
    "            #print(male_debias[index].shape)\n",
    "            \n",
    "        #print(male_proj == male_debias)\n",
    "        \n",
    "        for index, female in enumerate(females):\n",
    "        #female embedding = decentralized embedding - projected original (?) embedding into direction of PC\n",
    "            female.reshape((1,300))\n",
    "            female_proj[index] = (female - mu) - ((female @ np.transpose(unit_vec(pc))))\n",
    "            #with all new female embeddings: HardDebias\n",
    "            female_debias[index] = hard_debias(female_proj[index])\n",
    "    \n",
    "        #for all HardDebiased embeddings: KMeansClustering (2)\n",
    "        #for clustered embeddings: compute gender alignment accuracy\n",
    "        #4. store evaluations for each principal components\n",
    "        evaluations.append(align_acc(male_debias, female_debias))\n",
    "    print(evaluations)\n",
    "    \n",
    "    #5. evaluate which PC lead to most random cluster (evaluation smallest (close to 0.5), used second PC)\n",
    "    #best_eval = evaluations.index(np.min(evaluations))\n",
    "    print(np.argmin(evaluations))\n",
    "    best_pc = pca_freq.components_[np.argmin(evaluations)]\n",
    "    #print(\"Best PC:\",best_pc,\"with evaluation:\",evaluations[best_eval])\n",
    "\n",
    "    first_debias = np.zeros((words.shape))\n",
    "    #6. for all decentralized embeddings: remove that PC-direction\n",
    "    for index,word in enumerate(words_decen):\n",
    "        first_debias[index] = word - (word @ (np.transpose(unit_vec(best_pc))))\n",
    "    \n",
    "    #7. for all new embeddings: HardDebias\n",
    "    double_debias = np.zeros((words.shape))\n",
    "    for index,word in enumerate(first_debias):\n",
    "        double_debias[index] = hard_debias(word)\n",
    "\n",
    "    return double_debias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vec(vector):\n",
    "    \"\"\"calculates unit vector of passed vector\"\"\"\n",
    "    \n",
    "    unit = np.linalg.norm(vector)\n",
    "    if unit != 0 and np.isnan(unit) == False :\n",
    "        return vector/unit\n",
    "    return vector   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender alignment accuracy/ Neighborhood Metric:\n",
    "def align_acc(males, females):\n",
    "    \"\"\"bias measurement using KMeans Clustering\n",
    "    \n",
    "    takes female and male word's embeddings\n",
    "    ground truth labels:\n",
    "    0 = male,\n",
    "    1 = female\"\"\"\n",
    "    # print(males.shape, females.shape)\n",
    "    array_m_f = np.concatenate((males,females))\n",
    "    #print(array_m_f)\n",
    "    \n",
    "    #need: k (=1000) most biased female and male word's embedding (cosine similarity embedding & gender direction),\n",
    "    #1. assign ground truth gender labels: 0 = male, 1 = female\n",
    "    #2. run KMeans on embeddings\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(array_m_f)\n",
    "    split = males.shape[0]\n",
    "    \n",
    "    #for i in kmeans.labels_:\n",
    "    #    plt.scatter(np.concatenate((males, females))[kmeans.labels_ == i, 0], np.concatenate((males, females))[kmeans.labels_ == i, 1])\n",
    "    #plt.show()\n",
    "    \n",
    "    # print(kmeans.labels_)\n",
    "    # print(split)\n",
    "    correct = 0\n",
    "    #print(kmeans.labels_)\n",
    "    #3. compute alignment score: cluster assignment vs ground truth gender label\n",
    "    for i in range(array_m_f.shape[0]):\n",
    "        if i < split and kmeans.labels_[i] == 0:\n",
    "            correct+= 1\n",
    "        elif i >= split and kmeans.labels_[i] == 1:\n",
    "            correct += 1\n",
    "    \n",
    "    #4. alignment score = max(a, 1-a)\n",
    "    alignment = 1/(2*array_m_f.shape[0]) * correct\n",
    "    alignment = np.maximum(alignment, 1-alignment)\n",
    "    return alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional inputs: words to neutralize $N\\subseteq W$, family of equality sets $\\mathcal{E} = \\{E_1, E_2, ..., E_m\\}$ where each $E_i \\subseteq W$. For each word $w \\in N$, let $\\vec{w}$ be re-embedded to $\\vec{w}:=(\\vec{w}-\\vec{w}_B/||\\vec{w}-\\vec{w}_B||$. For each set $E\\in \\mathcal{E}$, let $\\mu:=\\sum_{w\\in E}w/|E|$ and $v:=\\mu-\\mu_B$. For each $w \\in E$, $\\vec{w}:=v+\\sqrt{1-||v||^2}\\frac{\\vec{w}_B-\\mu_B}{||\\vec{w}_B-\\mu_B||}$. Finally, output the subspace $B$ and the new embedding $\\{\\vec{w}\\in\\mathbb{R}^d\\}_{w\\in W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_debias (word_emb, equalize_pairs=equalize_pairs, B=pca):\n",
    "    \"\"\"performs hard debias on a word embedding to neutralize it,\n",
    "    \n",
    "    takes \n",
    "    word_emb: word embedding of the word to be neutralized,\n",
    "    equalize_pairs: equality pairs, each neutral word should be equidistant to all words in each equality set\n",
    "    B: the bias subspace\n",
    "    \n",
    "    returns\n",
    "    B: the bias subspace\n",
    "    new_word_emb: the new embedding for word_emb\n",
    "    \"\"\"\n",
    "    \n",
    "    new_word_emb = word_emb - B * (word_emb.dot(B) / B.dot(word_emb))\n",
    "    \n",
    "    return new_word_emb#, B im paper steht, dass auch B returned werden soll, aber das macht hier keinen Sinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn PCs: (300, 300)\n",
      "[0.8215, 0.7535000000000001, 0.7364999999999999, 0.8245, 0.669, 0.644, 0.8095, 0.77, 0.745, 0.5805, 0.909, 0.708, 0.593, 0.8089999999999999, 0.859, 0.959, 0.885, 0.8885, 0.659, 0.8665, 0.6685, 0.9175, 0.752, 0.704, 0.745, 0.7775, 0.758, 0.7424999999999999, 0.7195, 0.704, 0.756, 0.7515000000000001, 0.748, 0.774, 0.761, 0.7284999999999999, 0.738, 0.7215, 0.7715, 0.731, 0.7344999999999999, 0.7665, 0.7605, 0.7915, 0.775, 0.729, 0.7575000000000001, 0.7224999999999999, 0.753, 0.786, 0.743, 0.76, 0.7085, 0.717, 0.7595000000000001, 0.7384999999999999, 0.78, 0.734, 0.7685, 0.8035, 0.7665, 0.733, 0.7215, 0.7985, 0.7395, 0.694, 0.7424999999999999, 0.758, 0.712, 0.7745, 0.8089999999999999, 0.7050000000000001, 0.732, 0.724, 0.7745, 0.817, 0.713, 0.732, 0.7484999999999999, 0.7495, 0.7324999999999999, 0.7235, 0.8205, 0.713, 0.736, 0.7985, 0.7415, 0.694, 0.785, 0.7795, 0.728, 0.722, 0.753, 0.7685, 0.7755, 0.714, 0.7404999999999999, 0.714, 0.7565, 0.771, 0.7284999999999999, 0.733, 0.789, 0.7825, 0.7005, 0.8089999999999999, 0.736, 0.751, 0.738, 0.723, 0.791, 0.7905, 0.764, 0.724, 0.7615, 0.7575000000000001, 0.746, 0.738, 0.732, 0.748, 0.737, 0.7575000000000001, 0.7795, 0.72, 0.751, 0.787, 0.8049999999999999, 0.7545, 0.7885, 0.729, 0.7475, 0.7424999999999999, 0.7605, 0.7545, 0.708, 0.7515000000000001, 0.7715, 0.732, 0.7545, 0.7235, 0.7484999999999999, 0.8175, 0.7645, 0.7164999999999999, 0.7745, 0.7715, 0.7535000000000001, 0.7355, 0.7264999999999999, 0.752, 0.7785, 0.733, 0.7424999999999999, 0.753, 0.7925, 0.7645, 0.8065, 0.7295, 0.7755, 0.784, 0.765, 0.7535000000000001, 0.7045, 0.7364999999999999, 0.7545, 0.721, 0.76, 0.754, 0.6945, 0.758, 0.722, 0.7375, 0.761, 0.7685, 0.783, 0.75, 0.7595000000000001, 0.741, 0.7535000000000001, 0.7195, 0.729, 0.742, 0.739, 0.7435, 0.7424999999999999, 0.761, 0.773, 0.766, 0.7665, 0.77, 0.72, 0.786, 0.725, 0.749, 0.7615, 0.7929999999999999, 0.7455, 0.7124999999999999, 0.7195, 0.726, 0.7545, 0.72, 0.761, 0.7875, 0.756, 0.7655, 0.7475, 0.7715, 0.742, 0.7575000000000001, 0.727, 0.7135, 0.755, 0.773, 0.764, 0.7755, 0.786, 0.7595000000000001, 0.766, 0.736, 0.775, 0.7615, 0.757, 0.754, 0.7065, 0.753, 0.7215, 0.749, 0.7415, 0.7765, 0.731, 0.7404999999999999, 0.7375, 0.7315, 0.774, 0.731, 0.7364999999999999, 0.71, 0.7095, 0.7935, 0.8035, 0.7355, 0.7295, 0.7675, 0.73, 0.8195, 1.0, 0.7715, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "result = double_hard_debias(embedding, index_m, index_f, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50024265 -0.46275929  0.0821356  ... -0.04725981  0.47440758\n",
      "  -0.1332832 ]\n",
      " [ 0.363862    0.02430906  0.36200496 ...  0.03728555  0.42880194\n",
      "  -0.43616485]\n",
      " [ 0.16887998 -0.10593896  0.47302294 ... -0.02166147  0.14154992\n",
      "  -0.16788187]\n",
      " ...\n",
      " [-0.12240072 -0.15957666 -0.17676576 ... -0.00945517 -0.05294978\n",
      "  -0.18045257]\n",
      " [ 0.34891033  0.24629839  0.03828928 ...  0.37001987  0.08433326\n",
      "   0.32340648]\n",
      " [-0.65000931  0.17710975 -0.14273136 ...  0.22494923 -0.16484438\n",
      "  -0.01171416]]\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314952, 300)\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
