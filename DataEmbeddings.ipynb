{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "from sklearn import metrics as sk_m\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to retrive pre-trained 300 dimensional gloVe embedding\n",
    "embedding_300_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors.txt\"\n",
    "\n",
    "def read_embedding(url):\n",
    "    \"\"\"Function to read out an embedding\n",
    "    Input: url: url to embedding\n",
    "    \n",
    "    Returns: vocab: list of words in the embedding\n",
    "             w2id: dictionary mapping words to ids\n",
    "             embedding: array storing the word vectors,\n",
    "                           row corresponds to word id\"\"\"\n",
    "    # Open url\n",
    "    data = urllib.request.urlopen(url)\n",
    "    vocab = []\n",
    "    embedding = []\n",
    "    \n",
    "    # Each line contains one word and its embedding\n",
    "    for line in data:\n",
    "        line = line.decode()\n",
    "        # Split by spaces\n",
    "        split = line.split()\n",
    "        # First element(== the word) is added to vocabulary\n",
    "        vocab.append(split[0])\n",
    "        # All other elements(embedding vectors) are added to vectors\n",
    "        embedding.append([float(elem) for elem in split[1:]])\n",
    "    \n",
    "    # Create a dictionary with word-id pairs based on the order\n",
    "    w2id = {w: i for i, w in enumerate(vocab)}\n",
    "    # Vectors are converted into an array\n",
    "    embedding = np.array(embedding).astype(float)\n",
    "    \n",
    "    return vocab, w2id, embedding\n",
    "    \n",
    "vocab_original, w2id_original, embedding_original = read_embedding(embedding_300_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasDigit(word):\n",
    "    \"\"\"Checks if a string contains any digits\"\"\"\n",
    "    return any(char.isdigit() for char in word)\n",
    "\n",
    "def hasSpecialChar(word):\n",
    "    \"\"\"Checks if a string contains special characters(except \"_\")\"\"\"\n",
    "    special_characters = \"!@#$%^&*()-+?=,<>/.\"\n",
    "    return any(char in special_characters for char in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_vocab(vocab, w2id, embedding):\n",
    "    \"\"\"Limits the vocab by removing words containing digits or special characters\n",
    "    Input: vocab: list of words in the embedding\n",
    "           w2id: dictionary mapping words to ids\n",
    "           embedding: array storing the word vectors\n",
    "           \n",
    "    Returns: limit_vocab: list of words in vocab that do not include digits or special characters\n",
    "             limit_w2id: dictionary mapping words in limit_vocab to new ids\n",
    "             limit_embedding: array storing the word vectors of the words in limit_vocab only\"\"\"\n",
    "    limit_vocab = []\n",
    "    limit_embedding = []\n",
    "    \n",
    "    for word in vocab:\n",
    "        # If word includes either a digit or a special character move on to next word\n",
    "        if hasDigit(word) or hasSpecialChar(word):\n",
    "            continue\n",
    "        # Else add word to limit_vocab and its embedding to limit_embedding    \n",
    "        limit_vocab.append(word)\n",
    "        limit_embedding.append(embedding[w2id[word]])\n",
    "        \n",
    "    # Convert embedding into an array    \n",
    "    limit_embedding = np.array(limit_embedding).astype(float)\n",
    "    # Create new dictionary containing only the words in limit_vocab and their new ids\n",
    "    limit_w2id = {word: i for i, word in enumerate(limit_vocab)}\n",
    "    \n",
    "    return limit_vocab, limit_w2id, limit_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size:  322636\n",
      "Restricted vocab size:  314952\n"
     ]
    }
   ],
   "source": [
    "vocab, w2id, embedding = restrict_vocab(vocab_original, w2id_original, embedding_original)\n",
    "print(\"Original vocab size: \", len(vocab_original))\n",
    "print(\"Restricted vocab size: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_vocab(vocab, exclude):\n",
    "    \"\"\"Function to exclude specific words from vocabulary\n",
    "    Input: vocab: list of words in the embedding\n",
    "           exclude: list of words to exclude from the vocabulary\n",
    "           \n",
    "    Returns: limited_vocab: vocab without the words in exclude\"\"\"\n",
    "    # Create copies of vocab, word2id and word_vector\n",
    "    limited_vocab = vocab.copy()\n",
    "    # For all words that are in exclude and vocab\n",
    "    for word in exclude:\n",
    "        if word in limited_vocab:\n",
    "            # Remove word from vocab\n",
    "            limited_vocab.remove(word)\n",
    "            \n",
    "    return limited_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to female specific words as listed by the authors\n",
    "female_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/female_word_file.txt\"\n",
    "female_words_data = urllib.request.urlopen(female_words_url)\n",
    "\n",
    "# List of female words\n",
    "female_words = []\n",
    "for line in female_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    female_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to male specific words as listed by the authors\n",
    "male_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/male_word_file.txt\"\n",
    "male_words_data = urllib.request.urlopen(male_words_url)\n",
    "\n",
    "# List of male words\n",
    "male_words = []\n",
    "for line in male_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    male_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List with female - male pairs from female-male specific words\n",
    "female_male_pairs = []\n",
    "for i, female in enumerate(female_words):\n",
    "    female_male_pairs.append([female, male_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to the files storing gender specific words as listed by the authors\n",
    "gender_specific_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/gender_specific_full.json\"\n",
    "\n",
    "# Empty list to accumulate gender specific words plus additional list after lowercasing\n",
    "gender_specific_original = []\n",
    "gender_specific = []\n",
    "\n",
    "\n",
    "# Read out URL and add further gender specific words\n",
    "with urllib.request.urlopen(gender_specific_url) as f:\n",
    "    gender_specific_original.extend(json.load(f))\n",
    "\n",
    "# Add lower case words to second list\n",
    "for word in gender_specific_original:\n",
    "    gender_specific.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing definitional pairs as listed by the authors\n",
    "definitial_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/definitional_pairs.json\"\n",
    "\n",
    "# Empty list to store definitional pairs plus additional list after lowercasing\n",
    "definitional_pairs_original = []\n",
    "definitional_pairs = []\n",
    "\n",
    "\n",
    "# Read out url and add pairs in list\n",
    "with urllib.request.urlopen(definitial_pairs_url) as f:\n",
    "    definitional_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in definitional_pairs_original:\n",
    "    definitional_pairs.append([w1.lower(), w2.lower()])\n",
    "\n",
    "\n",
    "# Create list of single words instead of pairs  \n",
    "definitional_words = []\n",
    "for pair in definitional_pairs:\n",
    "    for word in pair:\n",
    "        definitional_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing the equalize pairs as listed by the authors\n",
    "equalize_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/equalize_pairs.json\"\n",
    "\n",
    "# Empty list to store equalize pairs plus additional list after lowercasing\n",
    "equalize_pairs_original = []\n",
    "equalize_pairs = []\n",
    "\n",
    "# Read out URL and add pairs to list\n",
    "with urllib.request.urlopen(equalize_pairs_url) as f:\n",
    "    equalize_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in equalize_pairs_original:\n",
    "    equalize_pairs.append([w1.lower(), w2.lower()])\n",
    "    \n",
    "# Create list of single words instead of pairs\n",
    "equalize_words = []\n",
    "for pair in equalize_pairs:\n",
    "    for word in pair:\n",
    "        equalize_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all gender specific words included in \n",
    "# female words, male words, gender specific words, equalize words and definitional words\n",
    "exclude_words = list(set(female_words + male_words + gender_specific + definitional_words + equalize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  314952\n",
      "Neutral vocab size:  314293\n"
     ]
    }
   ],
   "source": [
    "# Remove gender specific words from the embedding to obtain vocabulary of neutral words\n",
    "vocab_neutral = exclude_vocab(vocab, exclude_words)\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "print(\"Neutral vocab size: \", len(vocab_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(word, w2id=w2id, embedding=embedding):\n",
    "    return embedding[w2id[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idtfy_gender_subspace(word_sets, w2id, defining_sets, embedding, k=1):\n",
    "    \"\"\"\n",
    "    identifies the bias (gender) subspace following Bolukbasi et al. 2016\n",
    "    \n",
    "    takes\n",
    "    word_sets: vocabulary\n",
    "    w2id: a dictionary to translate words contained in the vocabulary into their corresponding IDs\n",
    "    defining_sets: N defining sets (pairs if I=2) consisting of I words that differ mainly on the bias (gender) direction\n",
    "    embedding: the embedding of the vocabulary\n",
    "    k: an integer parameters that defines how many rows of SVD(C) constitute the bias (gender) subspace B, bias (gender) direction if k=1\n",
    "    \n",
    "    returns\n",
    "    bias_subspace: linear bias (gender) subspace (direction) that is assumed to capture most of the gender bias (denoted as B in Bolukbasi et al. 2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(defining_sets) # we have 10 pairs in our defining_sets\n",
    "    I = len(defining_sets[0]) # = 2 in case of pairs (as for gender bias)\n",
    "    embedding_size = len(embedding[0]) # our embedding size is 300\n",
    "    \n",
    "    C = np.zeros((N, I, embedding_size))\n",
    "        \n",
    "    for n, d_set in enumerate(defining_sets):\n",
    "        mean_vector = np.zeros((I, embedding_size))\n",
    "        for i, word in enumerate(d_set):\n",
    "            mean_vector[i] = embed(word) / I\n",
    "            C[n][i] = (embed(word) - mean_vector[i]) * np.transpose((embed(word) - mean_vector[i])) / I\n",
    "            \n",
    "    #C = []\n",
    "    #for female_word, male_word in defining_sets:\n",
    "    #    mean = (embed(female_word) + embed(male_word)) / 2\n",
    "    #    C.append(embed(female_word) - mean)\n",
    "    #    C.append(embed(male_word) - mean)\n",
    "    \n",
    "    #C = np.array(C)\n",
    "    \n",
    "    print(\"C\",C.shape)\n",
    "    #print(C)\n",
    "    #_, SVD_C, _ = np.linalg.svd(C)\n",
    "    SVD_C = np.linalg.svd(C, compute_uv = False)\n",
    "    print(\"no uv\", SVD_C.shape)\n",
    "    B = SVD_C[:k]\n",
    "    print(\"B\",B)\n",
    "    \n",
    "    \n",
    "    print(\"cov\")\n",
    "    # as proven by Vargas & Cotterell 2020 the matrix C from Bolukbasi et al. 2016 can be written as an empirical covariance matrix for I = 2\n",
    "    matrix = []\n",
    "    for a, b in defining_sets:\n",
    "        center = (embed(a) + embed(b))/2\n",
    "        matrix.append(embed(a)-center)\n",
    "        matrix.append(embed(b)-center)\n",
    "    matrix = np.array(matrix)\n",
    "    print(\"matrix\", matrix.shape)\n",
    "    #print(matrix)\n",
    "    \n",
    "    # have to apply PCA when interpreting C as covariance matrix\n",
    "    pca = PCA(n_components = 10)\n",
    "    pca.fit(matrix)\n",
    "    print(\"pca\",pca.components_[0].shape)\n",
    "    # take the first pca\n",
    "    pca = pca.components_[0]\n",
    "    \n",
    "    SVD_matrix = np.linalg.svd(C, compute_uv=False)\n",
    "    svd = SVD_matrix[:k]\n",
    "    print(\"svd\",svd)\n",
    "\n",
    "    \n",
    "    print(\"new_B\")\n",
    "    array = np.ndarray((10,2,300))\n",
    "    #i=0\n",
    "    array_two = np.zeros((10,300,300))\n",
    "    for j, d_pair in enumerate(definitional_pairs):\n",
    "        for i, word in enumerate(d_pair):\n",
    "            # fill array with embeddings\n",
    "            array[j][i] = embedding[w2id[word]]\n",
    "            #i = i+1\n",
    "        # print(array[j][0].shape)\n",
    "        # calculate covariance between embeddings of same definitional pair?\n",
    "        array_two[j]=np.cov(np.transpose(array[j]))\n",
    "        \n",
    "    print(array_two.shape)\n",
    "    #new_C = np.cov(array)\n",
    "    #print(np.shape(new_C))\n",
    "    _, new_SVD_C, _ = np.linalg.svd(array_two)\n",
    "    print(np.shape(new_SVD_C))\n",
    "    new_B = new_SVD_C[:k]\n",
    "    print(new_B.shape)\n",
    "    \n",
    "    return B, new_B, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C (10, 2, 300)\n",
      "no uv (10, 2)\n",
      "B [[1.41470925 0.24232465]]\n",
      "cov\n",
      "matrix (20, 300)\n",
      "pca (300,)\n",
      "svd [[1.41470925 0.24232465]]\n",
      "new_B\n",
      "(10, 300, 300)\n",
      "(10, 300)\n",
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "B, new_B, pca = idtfy_gender_subspace(vocab, w2id, definitional_pairs, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most biased 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most biased male and female words\n",
    "def most_biased(embedding, B, k=500):\n",
    "    # small x, else memory issues\n",
    "    x = 50000\n",
    "    all_biased = np.ndarray((x,1))\n",
    "    for i, word in enumerate(embedding):\n",
    "        if i < x:\n",
    "            all_biased[i] = (sk_m.pairwise.cosine_similarity(word.reshape(1, 300), B.reshape(1, 300)))[0]\n",
    "            # print(sk_m.pairwise.cosine_similarity(word.reshape(1,300), B)[0])\n",
    "    #print(all_biased)\n",
    "    most_biased_f = []\n",
    "    most_biased_m = []\n",
    "    for word in range(k):\n",
    "        # female words\n",
    "        fb_index = np.argmax(all_biased)\n",
    "        most_biased_f.append(fb_index)\n",
    "        all_biased[fb_index] = 0\n",
    "        # male words\n",
    "        mb_index = np.argmin(all_biased)\n",
    "        most_biased_m.append(mb_index)\n",
    "        all_biased[mb_index] = 0\n",
    "    #print(most_biased_f, most_biased_m)\n",
    "    return most_biased_f, most_biased_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_f, index_m = most_biased(embedding, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female ['actress', 'pregnant', 'louise', 'therese', 'abbess', 'sister', 'chairwoman', 'alumna', 'princess', 'ballerina', 'maid', 'headmistress', 'pregnancy', 'josephine', 'olga', 'spinster', 'businesswoman', 'socialite', 'woman', 'heroine', 'congresswoman', 'matron', 'emmeline', 'seductive', 'uterus', 'feminist', 'actresses', 'feisty', 'princesses', 'mary', 'herself', 'suffragette', 'suffragist', 'louisa', 'ellen', 'countess', 'waitress', 'emily', 'goddess', 'girl', 'aunt', 'menstruation', 'sisters', 'governess', 'agnes', 'laura', 'duchess', 'filipina', 'archduchess', 'kuznetsova', 'daughters', 'svetlana', 'menstrual', 'noblewoman', 'katherine', 'mother', 'grandmother', 'elsa', 'nun', 'handbag', 'rebecca', 'prostitute', 'marchioness', 'sophie', 'valentina', 'irene', 'girlfriend', 'niece', 'glamorous', 'devi', 'francisca', 'manuela', 'millicent', 'nina', 'housewife', 'henriette', 'sultry', 'hilda', 'doreen', 'virgen', 'doña', 'inna', 'helene', 'thérèse', 'edith', 'baroness', 'schoolgirl', 'sophia', 'courtesan', 'tatiana', 'karolina', 'esther', 'betty', 'née', 'phoebe', 'lydia', 'miscarriage', 'emma', 'margaret', 'sheila', 'anna', 'seamstress', 'helen', 'godmother', 'patricia', 'lillian', 'breasts', 'supermodel', 'elisabeth', 'irina', 'womb', 'susan', 'christina', 'elizabeth', 'daniela', 'eliza', 'latina', 'yelena', 'needlework', 'amalia', 'frau', 'mademoiselle', 'tomboy', 'her', 'kathleen', 'lady', 'margareta', 'housekeeper', 'sorceress', 'diva', 'miss', 'consuelo', 'fatima', 'naomi', 'motherhood', 'jane', 'wollstonecraft', 'teresa', 'genevieve', 'alejandra', 'ann', 'caroline', 'lavinia', 'antonia', 'matriarch', 'bernadette', 'natalya', 'amelia', 'wilhelmina', 'lucy', 'constance', 'nurse', 'rachel', 'midwife', 'antoinette', 'magdalene', 'katie', 'alicia', 'eunice', 'cora', 'celeste', 'ekaterina', 'klara', 'eugenia', 'widowed', 'claudia', 'mildred', 'ingeborg', 'christine', 'martha', 'stepdaughter', 'blige', 'gabriela', 'annabelle', 'melanie', 'ursuline', 'diana', 'elena', 'juanita', 'rhoda', 'becky', 'eileen', 'ursula', 'marie', 'angelique', 'karin', 'patroness', 'madeleine', 'granddaughter', 'infanta', 'natalia', 'cervix', 'leona', 'dagmar', 'eva', 'fetus', 'rosalie', 'gertrude', 'lizzie', 'albertine', 'henrietta', 'bessie', 'nadezhda', 'postpartum', 'anne', 'selina', 'women', 'virginity', 'bombshell', 'maría', 'amalie', 'luise', 'frederica', 'katharina', 'tatyana', 'petrova', 'priestess', 'paulina', 'miriam', 'tamara', 'alina', 'irena', 'hildegard', 'angelica', 'magdalena', 'theresa', 'dorothea', 'julia', 'johanna', 'luisa', 'childbirth', 'femininity', 'hostess', 'blonde', 'minerva', 'erika', 'begum', 'sabrina', 'ida', 'mathilde', 'cheerleader', 'blouse', 'galina', 'sultana', 'lina', 'funnels', 'lesbian', 'nancy', 'sarah', 'amélie', 'aleksandra', 'maud', 'megan', 'elise', 'amy', 'monika', 'jenny', 'lilian', 'radwańska', 'carlotta', 'tanya', 'jeanne', 'marianne', 'sofía', 'ana', 'leah', 'contralto', 'beautiful', 'elaine', 'georgina', 'beatrice', 'harriet', 'emilie', 'earrings', 'amanda', 'ovaries', 'pankhurst', 'maharani', 'she', 'rani', 'goddesses', 'gabriella', 'cheryl', 'arabella', 'nuns', 'katharine', 'lorna', 'kumari', 'juana', 'maryam', 'thelma', 'astrid', 'edna', 'georgiana', 'ilse', 'lena', 'isabel', 'cécile', 'spokeswoman', 'bint', 'queen', 'theotokos', 'rms', 'hortense', 'sharapova', 'jessica', 'winifred', 'maternity', 'moorings', 'anita', 'dowager', 'womanhood', 'amina', 'flirtatious', 'charlotte', 'ethel', 'queenie', 'tania', 'bardot', 'lorena', 'jennifer', 'isabelle', 'hitomi', 'sorority', 'ada', 'pregnancies', 'secondly', 'heather', 'ywca', 'mothers', 'poppins', 'madame', 'vagina', 'necklace', 'susanne', 'leyla', 'beatriz', 'jacqueline', 'yvonne', 'viktoria', 'katerina', 'ramona', 'frances', 'odette', 'beaded', 'nathalie', 'topless', 'glamour', 'hadassah', 'cynthia', 'bethany', 'eleonora', 'yulia', 'augusta', 'kom', 'michelle', 'isobel', 'lucretia', 'maria', 'caterina', 'betsy', 'stepmother', 'joanne', 'hélène', 'cecilia', 'samantha', 'janie', 'femina', 'homemaker', 'aline', 'girls', 'eleanor', 'rita', 'marguerite', 'wilhelmine', 'gisela', 'eloise', 'beatrix', 'gowns', 'mme', 'valerie', 'bianca', 'lesbianism', 'annika', 'ulithi', 'bridget', 'sonia', 'maids', 'fatale', 'margarete', 'nadine', 'ilona', 'veronica', 'auntie', 'liza', 'catherine', 'adeline', 'brunette', 'lilith', 'madeline', 'faye', 'hysterical', 'kate', 'sara', 'dbe', 'amma', 'daria', 'necklaces', 'estrogen', 'rihanna', 'paola', 'jana', 'renee', 'daughter', 'alexandra', 'mrs', 'elsie', 'marija', 'pamela', 'hijab', 'alumnae', 'heiress', 'vanessa', 'receptionist', 'elisa', 'claudine', 'hera', 'marjorie', 'agatha', 'celia', 'lourdes', 'clarice', 'denise', 'ní', 'maggie', 'jelena', 'germaine', 'promiscuous', 'salome', 'ovulation', 'menopause', 'mollie', 'breastfeeding', 'ciara', 'hedwig', 'mistress', 'margrethe', 'polly', 'lynette', 'allure', 'monique', 'justine', 'donna', 'sassy', 'leda', 'majuro', 'cristina', 'chiara', 'liz', 'akiko', 'anja', 'claire', 'eugenie', 'charlene', 'fallin', 'lenore', 'gayatri', 'karen', 'susanna', 'gabrielle', 'athena', 'felicity', 'joanna', 'nora', 'inés', 'leila', 'margherita', 'fernanda', 'aurelia', 'ruth', 'susannah', 'brenda', 'feminism', 'karina', 'angela', 'debbie', 'chaste', 'faustina', 'nanny', 'kayla', 'landlady', 'ulrika', 'mimi', 'molly', 'wife', 'kristin', 'nymph', 'fiancée', 'lola', 'ayesha']\n",
      "male ['john', 'himself', 'his', 'brother', 'led', 'son', 'colonel', 'successor', 'nephew', 'footballing', 'sir', 'uncle', 'general', 'brothers', 'elway', 'he', 'tackle', 'linemen', 'nhl', 'deere', 'journeyman', 'governorship', 'brigadier', 'apprenticed', 'father', 'hooker', 'punter', 'marshal', 'captaincy', 'daud', 'generals', 'man', 'mayall', 'succeeded', 'henry', 'balliol', 'julius', 'leaguer', 'william', 'appointed', 'godfrey', 'captain', 'wingman', 'successors', 'trombonist', 'predecessor', 'mahdi', 'dibiase', 'fullback', 'domnall', 'papacy', 'inventor', 'engineer', 'military', 'defensive', 'linebacker', 'grandson', 'mcculloch', 'pontificate', 'dawkins', 'legate', 'james', 'mechanic', 'invented', 'ernest', 'surveyor', 'iii', 'muhammed', 'commander', 'czw', 'cardinal', 'irvin', 'receiver', 'sulayman', 'bulls', 'command', 'ibn', 'sayyid', 'dso', 'greats', 'under', 'plow', 'sax', 'robert', 'manager', 'explorer', 'decepticon', 'archbishop', 'grandfather', 'quarterback', 'klitschko', 'charles', 'commanded', 'sefer', 'pitchers', 'players', 'voivode', 'popper', 'antiquary', 'nfl', 'george', 'commentary', 'commanders', 'tt', 'haydn', 'usmc', 'andré', 'cricketing', 'joseph', 'hasidim', 'luthier', 'milton', 'racking', 'acumen', 'hijo', 'shocker', 'óscar', 'johann', 'associates', 'striker', 'shogun', 'jr', 'donald', 'setup', 'proto', 'ordnance', 'hannibal', 'englishman', 'xxii', 'abd', 'ernie', 'reliever', 'drafted', 'suspensions', 'playfair', 'praetorian', 'richard', 'contractor', 'episcopacy', 'snr', 'topographical', 'thomas', 'david', 'enforcers', 'starting', 'samuel', 'dewey', 'vasco', 'pacification', 'chairman', 'maestro', 'veteran', 'wayne', 'trombone', 'fought', 'bsk', 'chairmanship', 'king', 'enforcer', 'shapur', 'mercenaries', 'dalton', 'magister', 'adl', 'boyhood', 'philip', 'scoring', 'leader', 'halfback', 'shortstop', 'megatron', 'alexander', 'corps', 'ioan', 'mustapha', 'messrs', 'mullan', 'batchelor', 'army', 'drummer', 'johannes', 'relativity', 'abbot', 'khwaja', 'kamel', 'gough', 'hector', 'tex', 'franjo', 'famer', 'irl', 'edward', 'jovan', 'strongman', 'corinthians', 'fealty', 'antipope', 'linebackers', 'stonemason', 'preached', 'muhammad', 'sergeant', 'predecessors', 'positional', 'gary', 'apprenticeship', 'gottfried', 'galvatron', 'dooley', 'rebelled', 'bradman', 'electronic', 'aerospace', 'congressman', 'builder', 'stephen', 'hardline', 'ioannis', 'rule', 'johnny', 'chief', 'frusciante', 'foundry', 'albert', 'hoare', 'defensively', 'reverend', 'khl', 'midfield', 'dfc', 'mr', 'zahir', 'firm', 'corporal', 'returner', 'jonson', 'bruce', 'ironworks', 'architect', 'mechanics', 'saladin', 'frederick', 'pope', 'expeditions', 'stephenson', 'drums', 'gaspar', 'attila', 'dave', 'lieutenant', 'blacksmith', 'tsang', 'augustus', 'reginald', 'treatise', 'stuntman', 'dionysius', 'abbas', 'sons', 'mainz', 'militia', 'oberst', 'papal', 'tyndall', 'signings', 'nikola', 'wallace', 'agricola', 'abdallah', 'hap', 'galbraith', 'sabres', 'reorganizing', 'caliphate', 'carpentry', 'trumpet', 'olivares', 'businessman', 'cb', 'brian', 'beal', 'steve', 'khurasan', 'nll', 'magruder', 'muscovy', 'qasim', 'soult', 'mike', 'nexus', 'plato', 'flavius', 'antiquarian', 'mathias', 'xizong', 'monro', 'major', 'yazid', 'joe', 'backfield', 'nba', 'expedition', 'gregg', 'tim', 'leadership', 'carl', 'tenure', 'reputation', 'bass', 'partnership', 'iv', 'unix', 'saxophonist', 'marmaduke', 'capo', 'pershing', 'quintus', 'patriarch', 'resigned', 'ritter', 'benjamin', 'antigonus', 'ewald', 'byzantines', 'paul', 'ken', 'hugh', 'martyn', 'dempsey', 'jürgen', 'catcher', 'imperium', 'jeremiah', 'heenan', 'harold', 'franchises', 'maimonides', 'horsemen', 'bahadur', 'almagro', 'him', 'bud', 'virgil', 'redman', 'clement', 'aldo', 'councilman', 'bull', 'made', 'karl', 'adjutant', 'redskins', 'ahmad', 'foreman', 'appointment', 'mlb', 'harmonica', 'forwards', 'aforesaid', 'sheriff', 'governor', 'bartram', 'pipe', 'printer', 'herbert', 'lefebvre', 'contractors', 'yards', 'agro', 'colossal', 'cowboys', 'lefty', 'punting', 'cena', 'ludwig', 'johnson', 'cristiano', 'heavyweight', 'bench', 'vader', 'méliès', 'percival', 'socrates', 'horace', 'hirst', 'bassist', 'chiefs', 'gm', 'rhino', 'edwin', 'microscope', 'peter', 'burgoyne', 'kirby', 'armagh', 'goalie', 'sauron', 'óg', 'ruckus', 'signing', 'xi', 'gar', 'commentaries', 'guy', 'proconsul', 'racing', 'platoon', 'cuvier', 'aron', 'saxophone', 'babbitt', 'fairbairn', 'titus', 'hyder', 'clemons', 'career', 'marwan', 'gnaeus', 'guitar', 'taco', 'stadtholder', 'speight', 'master', 'replaced', 'xviii', 'greco', 'stearns', 'llewellyn', 'lytle', 'hisham', 'goc', 'kinsman', 'cigar', 'georg', 'shaykh', 'blaney', 'keith', 'pierre', 'rickard', 'vc', 'elwood', 'vern', 'steuart', 'rouse', 'rookie', 'leafs', 'wars', 'louis', 'bernard', 'elihu', 'dick', 'christopher', 'nasl', 'historian', 'arnold', 'máscara', 'header', 'athanasius', 'scored', 'reorganized', 'was', 'belushi', 'spengler', 'ivan', 'pistons', 'contender', 'pemberton', 'jim', 'knopfler', 'numan', 'league', 'team', 'battlefield', 'ashikaga', 'gotti', 'sturgis', 'subordinates', 'guard', 'whig', 'nishapur', 'frémont', 'rewarded', 'promoted', 'adrian', 'feudalism', 'fr', 'gottlob', 'friedrich', 'rookies', 'lineman', 'doc', 'eton', 'cymbal', 'epc', 'gotthard', 'nebuchadnezzar', 'enterprises', 'sawmill', 'outfielder', 'baronetcy']\n"
     ]
    }
   ],
   "source": [
    "female_most_biased = [vocab[i] for i in index_f]\n",
    "print(\"female\", female_most_biased)\n",
    "male_most_biased = [vocab[i] for i in index_m]\n",
    "print(\"male\", male_most_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Hard Debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_hard_debias(words, index_m, index_f, w2id):\n",
    "    \"\"\"Double Hard Debias:\n",
    "    \n",
    "    words: word embeddings of some corpus\n",
    "    males: set of most biased male words \n",
    "    females: set of most biased female words\n",
    "    w2id:\n",
    "    \"\"\"\n",
    "    \n",
    "    males = [words[i] for i in index_m]\n",
    "    females = [words[i] for i in index_f]    \n",
    "    \n",
    "    #need: Word embeddings, top 500 Male biased words set Wm, top 500 Female biased words set Wf\n",
    "    #1. for all word embeddings: decentralize all words\n",
    "    mue = (len(words)**(-1)) * np.sum(words, axis=0)\n",
    "    # print(mue)\n",
    "    words_decen = np.zeros((words.shape))\n",
    "    for index, embedding in enumerate(words):\n",
    "        # print(index,\":\",embedding)\n",
    "        words_decen[index] = embedding - mue\n",
    "    \n",
    "    #print(\"decentralized:\",words_decen)\n",
    "    #print(\"origin:\",words)\n",
    "        \n",
    "    #2. for all decentralized embeddings: compute PCA\n",
    "    #princ_comp = np.asarray(pca_tft(words_decen))\n",
    "    #print(\"Principal Components:\",princ_comp)\n",
    "    pca = PCA().fit(words_decen)\n",
    "    princ_comp = pca.components_\n",
    "\n",
    "    #print(\"Sklearn PC:\", pca.components_)\n",
    "\n",
    "    evaluations = []\n",
    "\n",
    "    #3. for all principal components:\n",
    "    for pc in princ_comp:\n",
    "        male_proj = np.zeros((len(males),300))\n",
    "        male_debias = np.zeros((len(males),300))\n",
    "        female_proj = np.zeros((len(females),300))\n",
    "        female_debias = np.zeros((len(females),300))\n",
    "   \n",
    "        for index, male in enumerate(males):\n",
    "        #male embedding = decentralized embedding - projected original (?) embedding into direction of PC\n",
    "            #print((male-mue).shape, ((np.transpose(pc)*male)*pc).shape)\n",
    "            male_proj[index] = (male - mue) - ((np.transpose(pc)*male)*pc)\n",
    "            #with all new male embeddings: HardDebias\n",
    "            male_debias[index] = hard_debias(male_proj[index])\n",
    "        \n",
    "        for index, female in enumerate(females):\n",
    "        #female embedding = decentralized embedding - projected original (?) embedding into direction of PC\n",
    "            female_proj[index] = (female - mue) - ((np.transpose(pc)*male)*pc)\n",
    "            #with all new female embeddings: HardDebias\n",
    "            female_debias[index] = hard_debias(female_proj[index])\n",
    "    \n",
    "        #for all HardDebiased embeddings: KMeansClustering (2)\n",
    "        #for clustered embeddings: compute gender alignment accuracy\n",
    "        #4. store evaluations for each principal components\n",
    "        evaluations.append(align_acc(male_debias, female_debias))\n",
    "    \n",
    "    #5. evaluate which PC lead to most random cluster (evaluation smallest (close to 0.5), used second PC)\n",
    "    best_eval = evaluations.index(np.min(evaluations))\n",
    "    best_pc = princ_comp[best_eval]\n",
    "    #print(\"Best PC:\",best_pc,\"with evaluation:\",evaluations[best_eval])\n",
    "\n",
    "    first_debias = np.zeros((words.shape))\n",
    "    #6. for all decentralized embeddings: remove that PC-direction\n",
    "    for index,word in enumerate(words_decen):\n",
    "        first_debias[index] = word - ((np.transpose(best_pc)*words[index])*best_pc)\n",
    "    \n",
    "    #7. for all new embeddings: HardDebias\n",
    "    double_debias = np.zeros((words.shape))\n",
    "    for index,word in enumerate(first_debias):\n",
    "        double_debias[index] = hard_debias(word)\n",
    "\n",
    "    return double_debias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = double_hard_debias(embedding, index_m, index_f, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender alignment accuracy/ Neighborhood Metric:\n",
    "def align_acc(males, females):\n",
    "    \"\"\"bias measurement using KMeans Clustering\n",
    "    \n",
    "    takes female and male word's embeddings\n",
    "    ground truth labels:\n",
    "    0 = male,\n",
    "    1 = female\"\"\"\n",
    "    \n",
    "    #need: k (=1000) most biased female and male word's embedding (cosine similarity embedding & gender direction),\n",
    "    #1. assign ground truth gender labels: 0 = male, 1 = female\n",
    "    #2. run KMeans on embeddings\n",
    "    kmeans = KMeans(n_clusters=2).fit(np.concatenate((males, females)))\n",
    "    split = males.shape[0]\n",
    "    correct = 0\n",
    "    #print(kmeans.labels_)\n",
    "    #3. compute alignment score: cluster assignment vs ground truth gender label\n",
    "    for i in range(np.concatenate((males, females)).shape[0]):\n",
    "        if i < split and kmeans.labels_[i] == 0:\n",
    "            correct+= 1\n",
    "        elif i >= split and kmeans.labels_[i] == 1:\n",
    "            correct += 1\n",
    "    \n",
    "    #4. alignment score = max(a, 1-a)\n",
    "    alignment = 1/(2*2) * correct\n",
    "    alignment = np.maximum(alignment, 1-alignment)\n",
    "    \n",
    "    return alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional inputs: words to neutralize $N\\subseteq W$, family of equality sets $\\mathcal{E} = \\{E_1, E_2, ..., E_m\\}$ where each $E_i \\subseteq W$. For each word $w \\in N$, let $\\vec{w}$ be re-embedded to $\\vec{w}:=(\\vec{w}-\\vec{w}_B/||\\vec{w}-\\vec{w}_B||$. For each set $E\\in \\mathcal{E}$, let $\\mu:=\\sum_{w\\in E}w/|E|$ and $v:=\\mu-\\mu_B$. For each $w \\in E$, $\\vec{w}:=v+\\sqrt{1-||v||^2}\\frac{\\vec{w}_B-\\mu_B}{||\\vec{w}_B-\\mu_B||}$. Finally, output the subspace $B$ and the new embedding $\\{\\vec{w}\\in\\mathbb{R}^d\\}_{w\\in W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_debias (word_emb, equalize_pairs=equalize_pairs, B=pca):\n",
    "    \"\"\"performs hard debias on a word embedding to neutralize it,\n",
    "    \n",
    "    takes \n",
    "    word_emb: word embedding of the word to be neutralized,\n",
    "    equalize_pairs: equality pairs, each neutral word should be equidistant to all words in each equality set\n",
    "    B: the bias subspace\n",
    "    \n",
    "    returns\n",
    "    B: the bias subspace\n",
    "    new_word_emb: the new embedding for word_emb\n",
    "    \"\"\"\n",
    "    \n",
    "    new_word_emb = word_emb - B * (word_emb.dot(B) / B.dot(word_emb))\n",
    "    \n",
    "    return new_word_emb#, B im paper steht, dass auch B returned werden soll, aber das macht hier keinen Sinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = double_hard_debias(embedding, index_m, index_f, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46401551 -0.36138454 -0.14305111 ... -0.02062871  0.49958673\n",
      "  -0.1748378 ]\n",
      " [ 0.31128478  0.10707839  0.11905837 ...  0.04600415  0.43654551\n",
      "  -0.49432505]\n",
      " [ 0.1432653   0.00264024  0.2555798  ...  0.0127982   0.17556582\n",
      "  -0.20136911]\n",
      " ...\n",
      " [-0.15505765 -0.06034876 -0.40329671 ...  0.01550556 -0.02788516\n",
      "  -0.22335757]\n",
      " [ 0.20760419  0.2392062  -0.29398582 ...  0.28803001  0.00363695\n",
      "   0.17358627]\n",
      " [-0.64096775  0.31325449 -0.33158808 ...  0.28636279 -0.10186761\n",
      "  -0.01754304]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314952, 300)\n"
     ]
    }
   ],
   "source": [
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
