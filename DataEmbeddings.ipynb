{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "from sklearn import metrics as sk_m\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "from scipy.stats import norm\n",
    "import statistics as stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to retrive pre-trained 300 dimensional gloVe embedding\n",
    "embedding_300_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors.txt\"\n",
    "\n",
    "def read_embedding(url, skip_first = False):\n",
    "    \"\"\"Function to read out an embedding\n",
    "    Input: url: url to embedding\n",
    "    \n",
    "    Returns: vocab: list of words in the embedding\n",
    "             w2id: dictionary mapping words to ids\n",
    "             embedding: array storing the word vectors,\n",
    "                           row corresponds to word id\"\"\"\n",
    "    # Open url\n",
    "    data = urllib.request.urlopen(url)\n",
    "    vocab = []\n",
    "    embedding = []\n",
    "    \n",
    "    # Each line contains one word and its embedding\n",
    "    for i, line in enumerate(data):\n",
    "        if skip_first:\n",
    "            if i == 0:\n",
    "                continue\n",
    "        #if len(line) == 301:\n",
    "        line = line.decode()\n",
    "        # Split by spaces\n",
    "        split = line.split()\n",
    "        # First element(== the word) is added to vocabulary\n",
    "        vocab.append(split[0])\n",
    "        # All other elements(embedding vectors) are added to vectors\n",
    "        embedding.append([float(elem) for elem in split[1:]])\n",
    "    \n",
    "    # Create a dictionary with word-id pairs based on the order\n",
    "    w2id = {w: i for i, w in enumerate(vocab)}\n",
    "    # Vectors are converted into an array\n",
    "    embedding = np.array(embedding).astype(float)\n",
    "    \n",
    "    return vocab, w2id, embedding\n",
    "    \n",
    "vocab_original, w2id_original, embedding_original = read_embedding(embedding_300_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasDigit(word):\n",
    "    \"\"\"Checks if a string contains any digits\"\"\"\n",
    "    return any(char.isdigit() for char in word)\n",
    "\n",
    "def hasSpecialChar(word):\n",
    "    \"\"\"Checks if a string contains special characters(except \"_\")\"\"\"\n",
    "    special_characters = \"!@#$%^&*()-+?=,<>/.\"\n",
    "    return any(char in special_characters for char in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_vocab(vocab, w2id, embedding):\n",
    "    \"\"\"Limits the vocab by removing words containing digits or special characters\n",
    "    Input: vocab: list of words in the embedding\n",
    "           w2id: dictionary mapping words to ids\n",
    "           embedding: array storing the word vectors\n",
    "           \n",
    "    Returns: limit_vocab: list of words in vocab that do not include digits or special characters\n",
    "             limit_w2id: dictionary mapping words in limit_vocab to new ids\n",
    "             limit_embedding: array storing the word vectors of the words in limit_vocab only\"\"\"\n",
    "    limit_vocab = []\n",
    "    limit_embedding = []\n",
    "    \n",
    "    for word in vocab:\n",
    "        # If word includes either a digit or a special character move on to next word\n",
    "        if hasDigit(word) or hasSpecialChar(word):\n",
    "            continue\n",
    "        # Else add word to limit_vocab and its embedding to limit_embedding    \n",
    "        limit_vocab.append(word)\n",
    "        limit_embedding.append(embedding[w2id[word]])\n",
    "        \n",
    "    # Convert embedding into an array    \n",
    "    limit_embedding = np.array(limit_embedding).astype(float)\n",
    "    # Create new dictionary containing only the words in limit_vocab and their new ids\n",
    "    limit_w2id = {word: i for i, word in enumerate(limit_vocab)}\n",
    "    \n",
    "    return limit_vocab, limit_w2id, limit_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size:  322636\n",
      "Restricted vocab size:  314952\n"
     ]
    }
   ],
   "source": [
    "vocab, w2id, embedding = restrict_vocab(vocab_original, w2id_original, embedding_original)\n",
    "print(\"Original vocab size: \", len(vocab_original))\n",
    "print(\"Restricted vocab size: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_vocab(vocab, exclude):\n",
    "    \"\"\"Function to exclude specific words from vocabulary\n",
    "    Input: vocab: list of words in the embedding\n",
    "           exclude: list of words to exclude from the vocabulary\n",
    "           \n",
    "    Returns: limited_vocab: vocab without the words in exclude\"\"\"\n",
    "    # Create copy of vocab\n",
    "    limited_vocab = vocab.copy()\n",
    "    # For all words that are in exclude and vocab\n",
    "    for word in exclude:\n",
    "        if word in limited_vocab:\n",
    "            # Remove word from vocab\n",
    "            limited_vocab.remove(word)\n",
    "            \n",
    "    return limited_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to female specific words as listed by the authors\n",
    "female_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/female_word_file.txt\"\n",
    "female_words_data = urllib.request.urlopen(female_words_url)\n",
    "\n",
    "# List of female words\n",
    "female_words = []\n",
    "for line in female_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    female_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to male specific words as listed by the authors\n",
    "male_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/male_word_file.txt\"\n",
    "male_words_data = urllib.request.urlopen(male_words_url)\n",
    "\n",
    "# List of male words\n",
    "male_words = []\n",
    "for line in male_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    male_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List with female - male pairs from female-male specific words\n",
    "female_male_pairs = []\n",
    "for i, female in enumerate(female_words):\n",
    "    female_male_pairs.append([female, male_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to the files storing gender specific words as listed by the authors\n",
    "gender_specific_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/gender_specific_full.json\"\n",
    "\n",
    "# Empty list to accumulate gender specific words plus additional list after lowercasing\n",
    "gender_specific_original = []\n",
    "gender_specific = []\n",
    "\n",
    "\n",
    "# Read out URL and add further gender specific words\n",
    "with urllib.request.urlopen(gender_specific_url) as f:\n",
    "    gender_specific_original.extend(json.load(f))\n",
    "\n",
    "# Add lower case words to second list\n",
    "for word in gender_specific_original:\n",
    "    gender_specific.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing definitional pairs as listed by the authors\n",
    "definitial_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/definitional_pairs.json\"\n",
    "\n",
    "# Empty list to store definitional pairs plus additional list after lowercasing\n",
    "definitional_pairs_original = []\n",
    "definitional_pairs = []\n",
    "\n",
    "\n",
    "# Read out url and add pairs in list\n",
    "with urllib.request.urlopen(definitial_pairs_url) as f:\n",
    "    definitional_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in definitional_pairs_original:\n",
    "    definitional_pairs.append([w1.lower(), w2.lower()])\n",
    "\n",
    "\n",
    "# Create list of single words instead of pairs  \n",
    "definitional_words = []\n",
    "for pair in definitional_pairs:\n",
    "    for word in pair:\n",
    "        definitional_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing the equalize pairs as listed by the authors\n",
    "equalize_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/equalize_pairs.json\"\n",
    "\n",
    "# Empty list to store equalize pairs plus additional list after lowercasing\n",
    "equalize_pairs_original = []\n",
    "equalize_pairs = []\n",
    "\n",
    "# Read out URL and add pairs to list\n",
    "with urllib.request.urlopen(equalize_pairs_url) as f:\n",
    "    equalize_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in equalize_pairs_original:\n",
    "    equalize_pairs.append([w1.lower(), w2.lower()])\n",
    "    \n",
    "# Create list of single words instead of pairs\n",
    "equalize_words = []\n",
    "for pair in equalize_pairs:\n",
    "    for word in pair:\n",
    "        equalize_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all gender specific words included in \n",
    "# female words, male words, gender specific words, equalize words and definitional words\n",
    "exclude_words = list(set(female_words + male_words + gender_specific + definitional_words + equalize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  314952\n",
      "Neutral vocab size:  314293\n"
     ]
    }
   ],
   "source": [
    "# Remove gender specific words from the embedding to obtain vocabulary of neutral words\n",
    "vocab_neutral = exclude_vocab(vocab, exclude_words)\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "print(\"Neutral vocab size: \", len(vocab_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(word, w2id=w2id, embedding=embedding):\n",
    "    return embedding[w2id[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load further embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gn_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors300.txt\"\n",
    "vocab_gn_original, w2id_gn_original, embedding_gn_original = read_embedding(embedding_gn_url)\n",
    "vocab_gn, w2id_gn, embedding_gn = restrict_vocab(vocab_gn_original, w2id_gn_original, embedding_gn_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_hd_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors_hd.txt\"\n",
    "vocab_hd_original, w2id_hd_original, embedding_hd_original = read_embedding(embedding_hd_url)\n",
    "vocab_hd, w2id_hd, embedding_hd = restrict_vocab(vocab_hd_original, w2id_hd_original, embedding_hd_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_hd_a_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors_hd_a.txt\"\n",
    "vocab_hd_a_original, w2id_hd_a_original, embedding_hd_a_original = read_embedding(embedding_hd_a_url)\n",
    "vocab_hd_a, w2id_hd_a, embedding_hd_a = restrict_vocab(vocab_hd_a_original, w2id_hd_a_original, embedding_hd_a_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gp_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/gp_glove.txt\"\n",
    "vocab_gp_original, w2id_gp_original, embedding_gp_original = read_embedding(embedding_gp_url, skip_first = True)\n",
    "vocab_gp, w2id_gp, embedding_gp = restrict_vocab(vocab_gp_original, w2id_gp_original, embedding_gp_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size:  322636\n",
      "Restricted vocab size:  314952\n"
     ]
    }
   ],
   "source": [
    "embedding_gp_gn_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/gp_gn_glove.txt\"\n",
    "vocab_gp_gn_original, w2id_gp_gn_original, embedding_gp_gn_original = read_embedding(embedding_gp_gn_url, skip_first = True)\n",
    "vocab_gp_gn, w2id_gp_gn, embedding_gp_gn = restrict_vocab(vocab_gp_gn_original, w2id_gp_gn_original, embedding_gp_gn_original)\n",
    "print(\"Original vocab size: \", len(vocab_gp_gn_original))\n",
    "print(\"Restricted vocab size: \", len(vocab_gp_gn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idtfy_gender_subspace(word_sets, w2id, defining_sets, embedding, k=1):\n",
    "    \"\"\"\n",
    "    identifies the bias (gender) subspace following Bolukbasi et al. 2016\n",
    "    \n",
    "    takes\n",
    "    word_sets: vocabulary\n",
    "    w2id: a dictionary to translate words contained in the vocabulary into their corresponding IDs\n",
    "    defining_sets: N defining sets (pairs if I=2) consisting of I words that differ mainly on the bias (gender) direction\n",
    "    embedding: the embedding of the vocabulary\n",
    "    k: an integer parameters that defines how many rows of SVD(C) constitute the bias (gender) subspace B, bias (gender) direction if k=1\n",
    "    \n",
    "    returns\n",
    "    bias_subspace: linear bias (gender) subspace (direction) that is assumed to capture most of the gender bias (denoted as B in Bolukbasi et al. 2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    C = []\n",
    "    for female_word, male_word in defining_sets:\n",
    "        mean = (embed(female_word) + embed(male_word)) /2\n",
    "        C.append(embed(female_word) - mean)\n",
    "        C.append(embed(male_word) - mean)\n",
    "    C = np.array(C)\n",
    "    \n",
    "    # applying PCA is the same as SVD when interpreting C as covariance matrix (Vargas & Cotterell 2020)\n",
    "    pca = PCA(n_components = 10)\n",
    "    pca.fit(C)\n",
    "\n",
    "    # take the first k pcas (first for gender direction)\n",
    "    B = []\n",
    "    for i in range(k):\n",
    "        B.append(pca.components_[i])\n",
    "    B = np.array(B).flatten()\n",
    "\n",
    "    \n",
    "    #print(\"new_B\")\n",
    "    #array = np.ndarray((10,2,300))\n",
    "    #i=0\n",
    "    #array_two = np.zeros((10,300,300))\n",
    "    #for j, d_pair in enumerate(definitional_pairs):\n",
    "    #    for i, word in enumerate(d_pair):\n",
    "    #        # fill array with embeddings\n",
    "    #        array[j][i] = embedding[w2id[word]]\n",
    "            #i = i+1\n",
    "        # print(array[j][0].shape)\n",
    "        # calculate covariance between embeddings of same definitional pair?\n",
    "    #    array_two[j]=np.cov(np.transpose(array[j]))\n",
    "        \n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_subspace = idtfy_gender_subspace(vocab, w2id, definitional_pairs, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most biased 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most biased male and female words\n",
    "def most_biased(embedding, B, k=500):\n",
    "    # small x, else memory issues\n",
    "    x = 50000\n",
    "    all_biased = np.ndarray((x,1))\n",
    "    for i, word in enumerate(embedding):\n",
    "        if i < x:\n",
    "            all_biased[i] = (sk_m.pairwise.cosine_similarity(word.reshape(1, 300), B.reshape(1, 300)))[0]\n",
    "            # print(sk_m.pairwise.cosine_similarity(word.reshape(1,300), B)[0])\n",
    "    #print(all_biased)\n",
    "    most_biased_f = []\n",
    "    most_biased_m = []\n",
    "    for word in range(k):\n",
    "        # female words\n",
    "        fb_index = np.argmin(all_biased)\n",
    "        most_biased_f.append(fb_index)\n",
    "        all_biased[fb_index] = 0\n",
    "        # male words\n",
    "        mb_index = np.argmax(all_biased)\n",
    "        most_biased_m.append(mb_index)\n",
    "        all_biased[mb_index] = 0\n",
    "    #print(most_biased_f, most_biased_m)\n",
    "    return most_biased_f, most_biased_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_f, index_m = most_biased(embedding, gender_subspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female ['john', 'himself', 'his', 'brother', 'led', 'son', 'colonel', 'successor', 'nephew', 'footballing', 'sir', 'uncle', 'general', 'brothers', 'elway', 'he', 'tackle', 'linemen', 'nhl', 'deere', 'journeyman', 'governorship', 'brigadier', 'apprenticed', 'father', 'hooker', 'punter', 'marshal', 'captaincy', 'daud', 'generals', 'man', 'mayall', 'succeeded', 'henry', 'balliol', 'julius', 'leaguer', 'william', 'appointed', 'godfrey', 'captain', 'wingman', 'successors', 'trombonist', 'predecessor', 'mahdi', 'dibiase', 'fullback', 'domnall', 'papacy', 'inventor', 'engineer', 'military', 'defensive', 'linebacker', 'grandson', 'mcculloch', 'pontificate', 'dawkins', 'legate', 'james', 'mechanic', 'invented', 'ernest', 'surveyor', 'iii', 'muhammed', 'commander', 'czw', 'cardinal', 'irvin', 'receiver', 'sulayman', 'bulls', 'command', 'ibn', 'sayyid', 'dso', 'greats', 'under', 'plow', 'sax', 'robert', 'manager', 'explorer', 'decepticon', 'archbishop', 'grandfather', 'quarterback', 'klitschko', 'charles', 'commanded', 'sefer', 'pitchers', 'players', 'voivode', 'popper', 'antiquary', 'nfl', 'george', 'commentary', 'commanders', 'tt', 'haydn', 'usmc', 'andré', 'cricketing', 'joseph', 'hasidim', 'luthier', 'milton', 'racking', 'acumen', 'hijo', 'shocker', 'óscar', 'johann', 'associates', 'striker', 'shogun', 'jr', 'donald', 'setup', 'proto', 'ordnance', 'hannibal', 'englishman', 'xxii', 'abd', 'ernie', 'reliever', 'drafted', 'suspensions', 'playfair', 'praetorian', 'richard', 'contractor', 'episcopacy', 'snr', 'topographical', 'thomas', 'david', 'enforcers', 'starting', 'samuel', 'dewey', 'vasco', 'pacification', 'chairman', 'maestro', 'veteran', 'wayne', 'trombone', 'fought', 'bsk', 'chairmanship', 'king', 'enforcer', 'shapur', 'mercenaries', 'dalton', 'magister', 'adl', 'boyhood', 'philip', 'scoring', 'leader', 'halfback', 'shortstop', 'megatron', 'alexander', 'corps', 'ioan', 'mustapha', 'messrs', 'mullan', 'batchelor', 'army', 'drummer', 'johannes', 'relativity', 'abbot', 'khwaja', 'kamel', 'gough', 'hector', 'tex', 'franjo', 'famer', 'irl', 'edward', 'jovan', 'strongman', 'corinthians', 'fealty', 'antipope', 'linebackers', 'stonemason', 'preached', 'muhammad', 'sergeant', 'predecessors', 'positional', 'gary', 'apprenticeship', 'gottfried', 'galvatron', 'dooley', 'rebelled', 'bradman', 'electronic', 'aerospace', 'congressman', 'builder', 'stephen', 'hardline', 'ioannis', 'rule', 'johnny', 'chief', 'frusciante', 'foundry', 'albert', 'hoare', 'defensively', 'reverend', 'khl', 'midfield', 'dfc', 'mr', 'zahir', 'firm', 'corporal', 'returner', 'jonson', 'bruce', 'ironworks', 'architect', 'mechanics', 'saladin', 'frederick', 'pope', 'expeditions', 'stephenson', 'drums', 'gaspar', 'attila', 'dave', 'lieutenant', 'blacksmith', 'tsang', 'augustus', 'reginald', 'treatise', 'stuntman', 'dionysius', 'abbas', 'sons', 'mainz', 'militia', 'oberst', 'papal', 'tyndall', 'signings', 'nikola', 'wallace', 'agricola', 'abdallah', 'hap', 'galbraith', 'sabres', 'reorganizing', 'caliphate', 'carpentry', 'trumpet', 'olivares', 'businessman', 'cb', 'brian', 'beal', 'steve', 'khurasan', 'nll', 'magruder', 'muscovy', 'qasim', 'soult', 'mike', 'nexus', 'plato', 'flavius', 'antiquarian', 'mathias', 'xizong', 'monro', 'major', 'yazid', 'joe', 'backfield', 'nba', 'expedition', 'gregg', 'tim', 'leadership', 'carl', 'tenure', 'reputation', 'bass', 'partnership', 'iv', 'unix', 'saxophonist', 'marmaduke', 'capo', 'pershing', 'quintus', 'patriarch', 'resigned', 'ritter', 'benjamin', 'antigonus', 'ewald', 'byzantines', 'paul', 'ken', 'hugh', 'martyn', 'dempsey', 'jürgen', 'catcher', 'imperium', 'jeremiah', 'heenan', 'harold', 'franchises', 'maimonides', 'horsemen', 'bahadur', 'almagro', 'him', 'bud', 'virgil', 'redman', 'clement', 'aldo', 'councilman', 'bull', 'made', 'karl', 'adjutant', 'redskins', 'ahmad', 'foreman', 'appointment', 'mlb', 'harmonica', 'forwards', 'aforesaid', 'sheriff', 'governor', 'bartram', 'pipe', 'printer', 'herbert', 'lefebvre', 'contractors', 'yards', 'agro', 'colossal', 'cowboys', 'lefty', 'punting', 'cena', 'ludwig', 'johnson', 'cristiano', 'heavyweight', 'bench', 'vader', 'méliès', 'percival', 'socrates', 'horace', 'hirst', 'bassist', 'chiefs', 'gm', 'rhino', 'edwin', 'microscope', 'peter', 'burgoyne', 'kirby', 'armagh', 'goalie', 'sauron', 'óg', 'ruckus', 'signing', 'xi', 'gar', 'commentaries', 'guy', 'proconsul', 'racing', 'platoon', 'cuvier', 'aron', 'saxophone', 'babbitt', 'fairbairn', 'titus', 'hyder', 'clemons', 'career', 'marwan', 'gnaeus', 'guitar', 'taco', 'stadtholder', 'speight', 'master', 'replaced', 'xviii', 'greco', 'stearns', 'llewellyn', 'lytle', 'hisham', 'goc', 'kinsman', 'cigar', 'georg', 'shaykh', 'blaney', 'keith', 'pierre', 'rickard', 'vc', 'elwood', 'vern', 'steuart', 'rouse', 'rookie', 'leafs', 'wars', 'louis', 'bernard', 'elihu', 'dick', 'christopher', 'nasl', 'historian', 'arnold', 'máscara', 'header', 'athanasius', 'scored', 'reorganized', 'was', 'belushi', 'spengler', 'ivan', 'pistons', 'contender', 'pemberton', 'jim', 'knopfler', 'numan', 'league', 'team', 'battlefield', 'ashikaga', 'gotti', 'sturgis', 'subordinates', 'guard', 'whig', 'nishapur', 'frémont', 'rewarded', 'promoted', 'adrian', 'feudalism', 'fr', 'gottlob', 'friedrich', 'rookies', 'lineman', 'doc', 'eton', 'cymbal', 'epc', 'gotthard', 'nebuchadnezzar', 'enterprises', 'sawmill', 'outfielder', 'baronetcy']\n",
      "male ['actress', 'pregnant', 'louise', 'therese', 'abbess', 'sister', 'chairwoman', 'alumna', 'princess', 'ballerina', 'maid', 'headmistress', 'pregnancy', 'josephine', 'olga', 'spinster', 'businesswoman', 'socialite', 'woman', 'heroine', 'congresswoman', 'matron', 'emmeline', 'seductive', 'uterus', 'feminist', 'actresses', 'feisty', 'princesses', 'mary', 'herself', 'suffragette', 'suffragist', 'louisa', 'ellen', 'countess', 'waitress', 'emily', 'goddess', 'girl', 'aunt', 'menstruation', 'sisters', 'governess', 'agnes', 'laura', 'duchess', 'filipina', 'archduchess', 'kuznetsova', 'daughters', 'svetlana', 'menstrual', 'noblewoman', 'katherine', 'mother', 'grandmother', 'elsa', 'nun', 'handbag', 'rebecca', 'prostitute', 'marchioness', 'sophie', 'valentina', 'irene', 'girlfriend', 'niece', 'glamorous', 'devi', 'francisca', 'manuela', 'millicent', 'nina', 'housewife', 'henriette', 'sultry', 'hilda', 'doreen', 'virgen', 'doña', 'inna', 'helene', 'thérèse', 'edith', 'baroness', 'schoolgirl', 'sophia', 'courtesan', 'tatiana', 'karolina', 'esther', 'betty', 'née', 'phoebe', 'lydia', 'miscarriage', 'emma', 'margaret', 'sheila', 'anna', 'seamstress', 'helen', 'godmother', 'patricia', 'lillian', 'breasts', 'supermodel', 'elisabeth', 'irina', 'womb', 'susan', 'christina', 'elizabeth', 'daniela', 'eliza', 'latina', 'yelena', 'needlework', 'amalia', 'frau', 'mademoiselle', 'tomboy', 'her', 'kathleen', 'lady', 'margareta', 'housekeeper', 'sorceress', 'diva', 'miss', 'consuelo', 'fatima', 'naomi', 'motherhood', 'jane', 'wollstonecraft', 'teresa', 'genevieve', 'alejandra', 'ann', 'caroline', 'lavinia', 'antonia', 'matriarch', 'bernadette', 'natalya', 'amelia', 'wilhelmina', 'lucy', 'constance', 'nurse', 'rachel', 'midwife', 'antoinette', 'magdalene', 'katie', 'alicia', 'eunice', 'cora', 'celeste', 'ekaterina', 'klara', 'eugenia', 'widowed', 'claudia', 'mildred', 'ingeborg', 'christine', 'martha', 'stepdaughter', 'blige', 'gabriela', 'annabelle', 'melanie', 'ursuline', 'diana', 'elena', 'juanita', 'rhoda', 'becky', 'eileen', 'ursula', 'marie', 'angelique', 'karin', 'patroness', 'madeleine', 'granddaughter', 'infanta', 'natalia', 'cervix', 'leona', 'dagmar', 'eva', 'fetus', 'rosalie', 'gertrude', 'lizzie', 'albertine', 'henrietta', 'bessie', 'nadezhda', 'postpartum', 'anne', 'selina', 'women', 'virginity', 'bombshell', 'maría', 'amalie', 'luise', 'frederica', 'katharina', 'tatyana', 'petrova', 'priestess', 'paulina', 'miriam', 'tamara', 'alina', 'irena', 'hildegard', 'angelica', 'magdalena', 'theresa', 'dorothea', 'julia', 'johanna', 'luisa', 'childbirth', 'femininity', 'hostess', 'blonde', 'minerva', 'erika', 'begum', 'sabrina', 'ida', 'mathilde', 'cheerleader', 'blouse', 'galina', 'sultana', 'lina', 'funnels', 'lesbian', 'nancy', 'sarah', 'amélie', 'aleksandra', 'maud', 'megan', 'elise', 'amy', 'monika', 'jenny', 'lilian', 'radwańska', 'carlotta', 'tanya', 'jeanne', 'marianne', 'sofía', 'ana', 'leah', 'contralto', 'beautiful', 'elaine', 'georgina', 'beatrice', 'harriet', 'emilie', 'earrings', 'amanda', 'ovaries', 'pankhurst', 'maharani', 'she', 'rani', 'goddesses', 'gabriella', 'cheryl', 'arabella', 'nuns', 'katharine', 'lorna', 'kumari', 'juana', 'maryam', 'thelma', 'astrid', 'edna', 'georgiana', 'ilse', 'lena', 'isabel', 'cécile', 'spokeswoman', 'bint', 'queen', 'theotokos', 'rms', 'hortense', 'sharapova', 'jessica', 'winifred', 'maternity', 'moorings', 'anita', 'dowager', 'womanhood', 'amina', 'flirtatious', 'charlotte', 'ethel', 'queenie', 'tania', 'bardot', 'lorena', 'jennifer', 'isabelle', 'hitomi', 'sorority', 'ada', 'pregnancies', 'secondly', 'heather', 'ywca', 'mothers', 'poppins', 'madame', 'vagina', 'necklace', 'susanne', 'leyla', 'beatriz', 'jacqueline', 'yvonne', 'viktoria', 'katerina', 'ramona', 'frances', 'odette', 'beaded', 'nathalie', 'topless', 'glamour', 'hadassah', 'cynthia', 'bethany', 'eleonora', 'yulia', 'augusta', 'kom', 'michelle', 'isobel', 'lucretia', 'maria', 'caterina', 'betsy', 'stepmother', 'joanne', 'hélène', 'cecilia', 'samantha', 'janie', 'femina', 'homemaker', 'aline', 'girls', 'eleanor', 'rita', 'marguerite', 'wilhelmine', 'gisela', 'eloise', 'beatrix', 'gowns', 'mme', 'valerie', 'bianca', 'lesbianism', 'annika', 'ulithi', 'bridget', 'sonia', 'maids', 'fatale', 'margarete', 'nadine', 'ilona', 'veronica', 'auntie', 'liza', 'catherine', 'adeline', 'brunette', 'lilith', 'madeline', 'faye', 'hysterical', 'kate', 'sara', 'dbe', 'amma', 'daria', 'necklaces', 'estrogen', 'rihanna', 'paola', 'jana', 'renee', 'daughter', 'alexandra', 'mrs', 'elsie', 'marija', 'pamela', 'hijab', 'alumnae', 'heiress', 'vanessa', 'receptionist', 'elisa', 'claudine', 'hera', 'marjorie', 'agatha', 'celia', 'lourdes', 'clarice', 'denise', 'ní', 'maggie', 'jelena', 'germaine', 'promiscuous', 'salome', 'ovulation', 'menopause', 'mollie', 'breastfeeding', 'ciara', 'hedwig', 'mistress', 'margrethe', 'polly', 'lynette', 'allure', 'monique', 'justine', 'donna', 'sassy', 'leda', 'majuro', 'cristina', 'chiara', 'liz', 'akiko', 'anja', 'claire', 'eugenie', 'charlene', 'fallin', 'lenore', 'gayatri', 'karen', 'susanna', 'gabrielle', 'athena', 'felicity', 'joanna', 'nora', 'inés', 'leila', 'margherita', 'fernanda', 'aurelia', 'ruth', 'susannah', 'brenda', 'feminism', 'karina', 'angela', 'debbie', 'chaste', 'faustina', 'nanny', 'kayla', 'landlady', 'ulrika', 'mimi', 'molly', 'wife', 'kristin', 'nymph', 'fiancée', 'lola', 'ayesha']\n"
     ]
    }
   ],
   "source": [
    "female_most_biased = [vocab[i] for i in index_f]\n",
    "print(\"female\", female_most_biased)\n",
    "male_most_biased = [vocab[i] for i in index_m]\n",
    "print(\"male\", male_most_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Hard Debias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector projection of a vector a on (or onto) a nonzero vector b, (also known as the vector component or vector resolution of a in the direction of b), is the orthogonal projection of a onto a straight line parallel to b. It is a vector parallel to b, defined as:\n",
    "\n",
    "${\\displaystyle \\mathbf a_{1}} =  a_{1} {\\mathbf {\\hat {b}} \\,} $\n",
    "\n",
    "where $a_{1}$ is a scalar, called the scalar projection of a onto b, and ${\\mathbf {\\hat {b}} \\,}$ is the unit vector in the direction of b.\n",
    "\n",
    "In turn, the scalar projection is defined as:[2]\n",
    "\n",
    "$    a_{1} =  a ⋅ {\\mathbf {\\hat {b}} \\,}= a ⋅ \\frac{b}{‖ b ‖} $\n",
    "\n",
    "where the operator ⋅ denotes a dot product, ‖b‖ is the length of b.\n",
    "\n",
    "The **vector component** or vector resolute of a perpendicular to b, sometimes also called the **vector rejection of a from b** is the orthogonal projection of a onto the plane (or, in general, hyperplane) orthogonal to b. Both the projection $a_{1}$ and rejection $a_{2}$ of a vector a are vectors, and their sum is equal to a,[1] which implies that the rejection is given by: \n",
    "\n",
    "$a_{2} = a − a_{1}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_hard_debias(words, index_m, index_f, w2id):\n",
    "    \"\"\"Double Hard Debias:\n",
    "    \n",
    "    words: word embeddings of some corpus\n",
    "    index_m: indices of most biased male words \n",
    "    index_f: indices of most biased female words \n",
    "    w2id:\n",
    "    \"\"\"\n",
    "    # print(words[index_m[0]])\n",
    "    males = np.asarray([words[i] for i in index_m])\n",
    "    females = np.asarray([words[i] for i in index_f])    \n",
    "    \n",
    "    # decentralize all of the embeddings\n",
    "    # first calculate mean over full vocab\n",
    "    mu = ((len(words)**(-1)) * np.sum(words, axis=0)).reshape(1,300)\n",
    "    \n",
    "    # restrict corpus for PCA due to Error:\n",
    "    # Unable to allocate 721. MiB for an array with shape (314952, 300) and data type float64\n",
    "    #x = 1000\n",
    "    \n",
    "    words_decen = np.zeros((len(words),300), dtype='float32') # chose a smaller data type due to memory error\n",
    "    # then subtract mean from each word embedding\n",
    "    for index, embedding in enumerate(words):\n",
    "        #if index < len(words):\n",
    "        # print(index,\":\",embedding)\n",
    "        words_decen[index] = embedding - mu\n",
    "    \n",
    "    #print(\"decentralized:\",words_decen.shape)\n",
    "    #print(\"origin:\",words)\n",
    "        \n",
    "    # discover the frequency direction    \n",
    "    #2. for all decentralized embeddings: compute PCA\n",
    "    #princ_comp = np.asarray(pca_tft(words_decen))\n",
    "    #print(\"Principal Components:\",princ_comp)\n",
    "    pca_freq = PCA().fit(words_decen)\n",
    "    # print(\"250\",pca_freq.components_[250])\n",
    "    # print(\"251\",pca_freq.components_[251])\n",
    "    # print(pca.components_)\n",
    "    # princ_comp = pca.components_\n",
    "    #print(\"Sklearn PCs:\", pca_freq.components_.shape)\n",
    "\n",
    "    evaluations = []\n",
    "\n",
    "    #3. for all principal components:\n",
    "    # in implementation of paper only look at 20 first PCs\n",
    "    for i, pc in enumerate(pca_freq.components_):\n",
    "        if i < 20:\n",
    "            male_proj = np.zeros((len(males),300))\n",
    "            male_debias = np.zeros((len(males),300))\n",
    "            female_proj = np.zeros((len(females),300))\n",
    "            female_debias = np.zeros((len(females),300))\n",
    "    \n",
    "        \n",
    "            #pc= pc.reshape(1,300)\n",
    "        \n",
    "            for index, male in enumerate(males):\n",
    "            #male embedding = decentralized embedding - projected embedding into direction of PC\n",
    "                # print(male.shape)\n",
    "                #male.reshape((1,300))\n",
    "                #print(((male @np.transpose(pc.reshape(1,300)))*pc.reshape(1,300)).shape)\n",
    "                #print((male-mu).shape, ((np.transpose(pc.reshape(1,300))*male)@pc).shape)\n",
    "                #print(\"pc\", pc.shape, np.transpose(pc.reshape(300,1)).shape)\n",
    "                #male_proj[index] = (male - mu) - ((male @ np.transpose(unit_vec(pc))))\n",
    "                male_proj[index] = w_orth(male, pc)\n",
    "                #with all new male embeddings: HardDebias\n",
    "                #print(male_proj[index].shape)\n",
    "                #male_debias[index] = hard_debias(male_proj[index])\n",
    "                male_debias[index] = w_orth(male_proj[index], gender_subspace)\n",
    "                #print(male_debias[index].shape)\n",
    "            \n",
    "            #print(male_proj == male_debias)\n",
    "        \n",
    "            for index, female in enumerate(females):\n",
    "            #female embedding = decentralized embedding - projected original (?) embedding into direction of PC\n",
    "                #female.reshape((1,300))\n",
    "                female_proj[index] = w_orth(female, pc)\n",
    "                #with all new female embeddings: HardDebias\n",
    "                #female_debias[index] = hard_debias(female_proj[index])\n",
    "                female_debias[index] = w_orth(female_proj[index], gender_subspace)\n",
    "    \n",
    "            #for all HardDebiased embeddings: KMeansClustering (2)\n",
    "            #for clustered embeddings: compute gender alignment accuracy\n",
    "            #4. store evaluations for each principal components\n",
    "            evaluations.append(align_acc(male_debias, female_debias))\n",
    "    print(\"eval:\",evaluations)\n",
    "    \n",
    "    #5. evaluate which PC lead to most random cluster (evaluation smallest (close to 0.5), used second PC)\n",
    "    #best_eval = evaluations.index(np.min(evaluations))\n",
    "    print(np.argmin(evaluations))\n",
    "    best_pc = pca_freq.components_[np.argmin(evaluations)]\n",
    "    #print(\"Best PC:\",best_pc,\"with evaluation:\",evaluations[best_eval])\n",
    "\n",
    "    first_debias = np.zeros((words.shape))\n",
    "    #6. for all decentralized embeddings: remove that PC-direction\n",
    "    for index,word in enumerate(words_decen):\n",
    "        first_debias[index] = w_orth(word, best_pc)\n",
    "    \n",
    "    #7. for all new embeddings: HardDebias\n",
    "    #double_debias = np.zeros((words.shape))\n",
    "    #for index,word in enumerate(first_debias):\n",
    "    #    double_debias[index] = hard_debias(word)\n",
    "    double_debias = hard_debias(first_debias)\n",
    "\n",
    "    return double_debias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vec(vector):\n",
    "    \"\"\"calculates unit vector of passed vector\"\"\"\n",
    "    \n",
    "    unit = np.linalg.norm(vector)\n",
    "    if unit != 0 and np.isnan(unit) == False :\n",
    "        return vector/unit\n",
    "    return vector   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender alignment accuracy/ Neighborhood Metric:\n",
    "def align_acc(males, females):\n",
    "    \"\"\"bias measurement using KMeans Clustering\n",
    "    \n",
    "    takes female and male word's embeddings\n",
    "    ground truth labels:\n",
    "    0 = male,\n",
    "    1 = female\"\"\"\n",
    "    # print(males.shape, females.shape)\n",
    "    array_m_f = np.concatenate((males,females))\n",
    "    #print(array_m_f)\n",
    "    \n",
    "    #need: k (=1000) most biased female and male word's embedding (cosine similarity embedding & gender direction),\n",
    "    #1. assign ground truth gender labels: 0 = male, 1 = female\n",
    "    #2. run KMeans on embeddings\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(array_m_f)\n",
    "    split = males.shape[0]\n",
    "    \n",
    "    #for i in kmeans.labels_:\n",
    "    #    plt.scatter(np.concatenate((males, females))[kmeans.labels_ == i, 0], np.concatenate((males, females))[kmeans.labels_ == i, 1])\n",
    "    #plt.show()\n",
    "    \n",
    "    # print(kmeans.labels_)\n",
    "    # print(split)\n",
    "    correct = 0\n",
    "    #print(kmeans.labels_)\n",
    "    #3. compute alignment score: cluster assignment vs ground truth gender label\n",
    "    for i in range(array_m_f.shape[0]):\n",
    "        if i < split and kmeans.labels_[i] == 0:\n",
    "            correct+= 1\n",
    "        elif i >= split and kmeans.labels_[i] == 1:\n",
    "            correct += 1\n",
    "    \n",
    "    #4. alignment score = max(a, 1-a)\n",
    "    alignment = 1/(2*array_m_f.shape[0]) * correct\n",
    "    alignment = np.maximum(alignment, 1-alignment)\n",
    "    return alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional inputs: words to neutralize $N\\subseteq W$, family of equality sets $\\mathcal{E} = \\{E_1, E_2, ..., E_m\\}$ where each $E_i \\subseteq W$. For each word $w \\in N$, let $\\vec{w}$ be re-embedded to $\\vec{w}:=(\\vec{w}-\\vec{w}_B/||\\vec{w}-\\vec{w}_B||$. For each set $E\\in \\mathcal{E}$, let $\\mu:=\\sum_{w\\in E}w/|E|$ and $v:=\\mu-\\mu_B$. For each $w \\in E$, $\\vec{w}:=v+\\sqrt{1-||v||^2}\\frac{\\vec{w}_B-\\mu_B}{||\\vec{w}_B-\\mu_B||}$. Finally, output the subspace $B$ and the new embedding $\\{\\vec{w}\\in\\mathbb{R}^d\\}_{w\\in W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_debias (word_emb, equality_sets=equalize_pairs, B=gender_subspace):\n",
    "    \"\"\"performs hard debias on a word embedding to neutralize it,\n",
    "    \n",
    "    takes \n",
    "    word_emb: word embedding of the word to be neutralized,\n",
    "    equalize_pairs: equality pairs, each neutral word should be equidistant to all words in each equality set\n",
    "    B: the bias subspace\n",
    "    \n",
    "    returns\n",
    "    B: the bias subspace\n",
    "    new_word_emb: the new embedding for word_emb\n",
    "    \"\"\"\n",
    "    \n",
    "    # if word_emb is a single embedding:\n",
    "        # w_orth(word_emb)\n",
    "        \n",
    "    # if word_emb is the embeddings of all words to be neutralized:\n",
    "    \n",
    "    new_word_emb = np.zeros((word_emb.shape))\n",
    "    for i, embedding in enumerate(word_emb):\n",
    "        new_word_emb[i] = w_orth(embedding, B)\n",
    "        \n",
    "    for equal_set in equality_sets:\n",
    "        if equal_set[0] in w2id and equal_set[1] in w2id:\n",
    "            mean = (embed(equal_set[0]) + embed(equal_set[1])) / 2\n",
    "            mean_biased = mean - w_orth(mean, B)\n",
    "            v = mean - mean_biased # what is the biased mean?\n",
    "            for word in equal_set:\n",
    "                print(word)\n",
    "                word_biased = embed(word) - w_orth(embed(word), B)\n",
    "                new_embed = v + np.sqrt(1 - (np.linalg.norm(v)) ** 2) * ((word_biased - mean_biased) / unit_vec(word_biased - mean_biased))\n",
    "    \n",
    "    return new_word_emb#, B im paper steht, dass auch B returned werden soll, aber das macht hier keinen Sinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_orth (word_emb, direction):\n",
    "    \"\"\"removes direction from word embedding by calculating the orthogonal word vector\n",
    "    \n",
    "    w_orth = w - (projection of w onto direction)\n",
    "    w_orth = w - (direction * (w dot direction))\n",
    "    \n",
    "    takes \n",
    "    word_emb: word to remove the direction from\n",
    "    direction: the direction to remove\n",
    "    \n",
    "    returns:\n",
    "    embedding orthogonal to direction   \n",
    "    \"\"\"\n",
    "    \n",
    "    #new_word_emb_two = (word_emb) - ((word_emb @ np.transpose(unit_vec(direction))))\n",
    "    \n",
    "    # leads to evaluations of 0.5 and 1\n",
    "    #new_word_emb = unit_vec(word_emb) - (direction *  unit_vec(word_emb).dot(direction))\n",
    "    #print((np.dot(word_emb,direction)).shape)\n",
    "    \n",
    "    # formula from Bolukbasi et al. (2016)\n",
    "    new_word = word_emb - ((word_emb.dot(direction)) * direction)\n",
    "    \n",
    "\n",
    "    # print(new_word_emb_two, new_word_emb, new)\n",
    "    #print(new.shape)\n",
    "    \n",
    "    return unit_vec(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval: [0.757, 0.768, 0.7404999999999999, 0.7090000000000001, 0.7515000000000001, 0.745, 0.751, 0.7515000000000001, 0.7515000000000001, 0.748, 0.748, 0.7515000000000001, 0.7484999999999999, 0.7525, 0.753, 0.747, 0.748, 0.752, 0.7505, 0.7484999999999999]\n",
      "3\n",
      "monastery\n",
      "convent\n",
      "spokesman\n",
      "spokeswoman\n",
      "dad\n",
      "mom\n",
      "men\n",
      "women\n",
      "councilman\n",
      "councilwoman\n",
      "grandpa\n",
      "grandma\n",
      "grandsons\n",
      "granddaughters\n",
      "testosterone\n",
      "estrogen\n",
      "uncle\n",
      "aunt\n",
      "husbands\n",
      "wives\n",
      "father\n",
      "mother\n",
      "grandpa\n",
      "grandma\n",
      "he\n",
      "she\n",
      "boy\n",
      "girl\n",
      "boys\n",
      "girls\n",
      "brother\n",
      "sister\n",
      "brothers\n",
      "sisters\n",
      "businessman\n",
      "businesswoman\n",
      "chairman\n",
      "chairwoman\n",
      "colt\n",
      "filly\n",
      "congressman\n",
      "congresswoman\n",
      "dad\n",
      "mom\n",
      "dads\n",
      "moms\n",
      "dudes\n",
      "gals\n",
      "father\n",
      "mother\n",
      "fatherhood\n",
      "motherhood\n",
      "fathers\n",
      "mothers\n",
      "fella\n",
      "granny\n",
      "fraternity\n",
      "sorority\n",
      "gelding\n",
      "mare\n",
      "gentleman\n",
      "lady\n",
      "gentlemen\n",
      "ladies\n",
      "grandfather\n",
      "grandmother\n",
      "grandson\n",
      "granddaughter\n",
      "he\n",
      "she\n",
      "himself\n",
      "herself\n",
      "his\n",
      "her\n",
      "king\n",
      "queen\n",
      "kings\n",
      "queens\n",
      "male\n",
      "female\n",
      "males\n",
      "females\n",
      "man\n",
      "woman\n",
      "men\n",
      "women\n",
      "nephew\n",
      "niece\n",
      "prince\n",
      "princess\n",
      "schoolboy\n",
      "schoolgirl\n",
      "son\n",
      "daughter\n",
      "sons\n",
      "daughters\n"
     ]
    }
   ],
   "source": [
    "result_equal = double_hard_debias(embedding, index_m, index_f, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10805317 -0.08766379  0.01061857 ... -0.04900763  0.07792142\n",
      "  -0.01665246]\n",
      " [ 0.07214007 -0.01246045  0.04986387 ... -0.02734615  0.06203784\n",
      "  -0.06135967]\n",
      " [ 0.05096261 -0.02703599  0.07235463 ... -0.03247229  0.02815472\n",
      "  -0.02323581]\n",
      " ...\n",
      " [-0.03249714 -0.03440436 -0.07950904 ... -0.00769115 -0.01051873\n",
      "  -0.0550563 ]\n",
      " [ 0.0695056   0.04708673 -0.05388755 ...  0.06078932 -0.00027821\n",
      "   0.05233467]\n",
      " [-0.16114553  0.06672763 -0.06129586 ...  0.06207156 -0.02941251\n",
      "   0.0016239 ]]\n"
     ]
    }
   ],
   "source": [
    "print(result_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314952, 300)\n"
     ]
    }
   ],
   "source": [
    "print(result_equal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Association Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weat(object):\n",
    "\n",
    "    def __init__(self, concept1,concept2,stereotype1,stereotype2, iterations, embedding, w2id):\n",
    "        self.concept1 = concept1\n",
    "        self.concept2 = concept2\n",
    "        self.stereotype1 = stereotype1\n",
    "        self.stereotype2 = stereotype2\n",
    "        self.iterations = iterations\n",
    "        self.embedding = embedding\n",
    "        self.w2id = w2id\n",
    "\n",
    "    def getPValueAndEffect(self):\n",
    "        pvalue = 0\n",
    "        effect_size = 0\n",
    "        sd = 0\n",
    "        testStatistic = self.getTestStatistic(self.concept1,self.concept2,self.stereotype1,self.stereotype2, self.embedding, self.w2id)\n",
    "        nullDist = self.nullDistribution(self.concept1, self.concept2, self.stereotype1, self.stereotype2, self.iterations, self.embedding, self.w2id)\n",
    "        entireDistribution = self.getEntireDistribution(self.concept1, self.concept2, self.stereotype1, self.stereotype2, self.iterations, self.embedding, self.w2id)\n",
    "\n",
    "        pvalue = 1-self.calculateCumulativeProbability(nullDist, testStatistic)\n",
    "        effect_size = self.effectSize(entireDistribution, testStatistic)\n",
    "        sd = stat.stdev(nullDist)\n",
    "        return pvalue, effect_size, sd\n",
    "\n",
    "    def nullDistribution(self, concept1, concept2, stereotype1, stereotype2, iterations, embedding, w2id):\n",
    "\n",
    "        # permute concepts and for each permutation calculate getTestStatistic and save it in your distribution\n",
    "        bothConcepts = concept1 + concept2\n",
    "        print(\"Generating null distribution...\")\n",
    "\n",
    "        stereotype1NullMatrix = []\n",
    "        stereotype2NullMatrix = []\n",
    "\n",
    "        for attribute in stereotype1:\n",
    "            similarity_list = []\n",
    "            stereotype1Embedding = embedding[w2id[attribute]]\n",
    "\n",
    "            for word in bothConcepts:\n",
    "                nullEmbedding = embedding[w2id[word]]\n",
    "                similarity = self.cosineSimilarity(nullEmbedding, stereotype1Embedding)\n",
    "                similarity_list.append(similarity)\n",
    "            stereotype1NullMatrix.append(similarity_list)\n",
    "\n",
    "\n",
    "        for attribute in stereotype2:\n",
    "            similarity_list = []\n",
    "            stereotype2Embedding = embedding[w2id[attribute]]\n",
    "\n",
    "            for word in bothConcepts:\n",
    "                nullEmbedding = embedding[w2id[word]]\n",
    "                similarity = self.cosineSimilarity(nullEmbedding, stereotype2Embedding)\n",
    "                similarity_list.append(similarity)\n",
    "            stereotype2NullMatrix.append(similarity_list)\n",
    "\n",
    "        #Assuming both concepts have the same length\n",
    "        setSize = int(len(bothConcepts)/2)\n",
    "        print(\"Number of permutations \", iterations)\n",
    "        toShuffle = list(range(0, len(bothConcepts)))\n",
    "        distribution = []\n",
    "\n",
    "        for iter in range(iterations):\n",
    "            random.shuffle(toShuffle)\n",
    "        \t#calculate mean for each null shuffle\n",
    "            meanSimilaritycon1str1 = 0\n",
    "            meanSimilaritycon1str2 = 0\n",
    "            meanSimilaritycon2str1 = 0\n",
    "            meanSimilaritycon2str2 = 0\n",
    "\n",
    "            for i in range(len(stereotype1)):\n",
    "                for j in range(setSize):\n",
    "                    meanSimilaritycon1str1 = meanSimilaritycon1str1 + stereotype1NullMatrix[i][toShuffle[j]]\n",
    "\n",
    "            for i in range(len(stereotype2)):\n",
    "                for j in range(setSize):\n",
    "                    meanSimilaritycon1str2 = meanSimilaritycon1str2 + stereotype2NullMatrix[i][toShuffle[j]]\n",
    "\n",
    "            for i in range(len(stereotype1)):\n",
    "                for j in range(setSize):\n",
    "                    meanSimilaritycon2str1 = meanSimilaritycon2str1 + stereotype1NullMatrix[i][toShuffle[j+setSize]]\n",
    "\n",
    "            for i in range(len(stereotype2)):\n",
    "                for j in range(setSize):\n",
    "                    meanSimilaritycon2str2 = meanSimilaritycon2str2 + stereotype2NullMatrix[i][toShuffle[j+setSize]]\n",
    "\n",
    "            meanSimilaritycon1str1 = meanSimilaritycon1str1/(len(stereotype1)*setSize)\n",
    "            meanSimilaritycon1str2 = meanSimilaritycon1str2/(len(stereotype2)*setSize)\n",
    "            meanSimilaritycon2str1 = meanSimilaritycon2str1/(len(stereotype1)*setSize)\n",
    "            meanSimilaritycon2str2 = meanSimilaritycon2str2/(len(stereotype2)*setSize)\n",
    "\n",
    "            #come back here later\n",
    "            distribution.append((meanSimilaritycon1str1 - meanSimilaritycon1str2) - meanSimilaritycon2str1 + meanSimilaritycon2str2)\n",
    "\n",
    "        return distribution\n",
    "\n",
    "    def calculateCumulativeProbability(self,nullDistribution, testStatistic):\n",
    "        cumulative = -100\n",
    "        nullDistribution.sort()\n",
    "\n",
    "        \n",
    "        d = norm(loc = stat.mean(nullDistribution), scale = stat.stdev(nullDistribution))\n",
    "        cumulative = d.cdf(testStatistic)\n",
    "\n",
    "        return cumulative\n",
    "\n",
    "    def effectSize(self,array, mean):\n",
    "        effect = mean/stat.stdev(array)\n",
    "        return effect\n",
    "\n",
    "    def getTestStatistic(self, concept1, concept2, stereotype1, stereotype2, embedding, w2id):\n",
    "\n",
    "        differenceOfMeans =0\n",
    "        differenceOfMeansConcept1 =0\n",
    "        differenceOfMeansConcept2 =0\n",
    "\n",
    "        #concept 1 computations\n",
    "        for word in concept1:\n",
    "            concept1_embedding = embedding[w2id[word]]\n",
    "\n",
    "            meanConcept1Stereotype1=0\n",
    "            for attribute in stereotype1:\n",
    "                stereotype1_embedding = embedding[w2id[attribute]]\n",
    "                similarity = self.cosineSimilarity(concept1_embedding, stereotype1_embedding)\n",
    "                meanConcept1Stereotype1 = meanConcept1Stereotype1 + similarity\n",
    "\n",
    "            meanConcept1Stereotype1 = meanConcept1Stereotype1/len(stereotype1)\n",
    "\n",
    "\n",
    "            meanConcept1Stereotype2=0\n",
    "            for attribute in stereotype2:\n",
    "                stereotype2_embedding = embedding[w2id[attribute]]\n",
    "                similarity = self.cosineSimilarity(concept1_embedding, stereotype2_embedding)\n",
    "                meanConcept1Stereotype2 = meanConcept1Stereotype2 + similarity\n",
    "\n",
    "            meanConcept1Stereotype2 = meanConcept1Stereotype2/len(stereotype2)\n",
    "\n",
    "            differenceOfMeansConcept1 = differenceOfMeansConcept1+ meanConcept1Stereotype1 - meanConcept1Stereotype2\n",
    "\n",
    "        #effect size computations mean S(x,A,B)\n",
    "        differenceOfMeansConcept1 = differenceOfMeansConcept1/len(concept1)\n",
    "\n",
    "        #concept 2 computations\n",
    "        for word in concept2:\n",
    "            concept2_embedding = embedding[w2id[word]]\n",
    "\n",
    "            meanConcept2Stereotype1=0\n",
    "            for attribute in stereotype1:\n",
    "                stereotype1_embedding = embedding[w2id[attribute]]\n",
    "                similarity = self.cosineSimilarity(concept2_embedding, stereotype1_embedding)\n",
    "                meanConcept2Stereotype1 = meanConcept2Stereotype1 + similarity\n",
    "\n",
    "            meanConcept2Stereotype1 = meanConcept2Stereotype1/len(stereotype1)\n",
    "\n",
    "            meanConcept2Stereotype2=0\n",
    "            for attribute in stereotype2:\n",
    "                stereotype2_embedding = embedding[w2id[attribute]]\n",
    "                similarity = self.cosineSimilarity(concept2_embedding, stereotype2_embedding)\n",
    "                meanConcept2Stereotype2 = meanConcept2Stereotype2 + similarity\n",
    "\n",
    "            meanConcept2Stereotype2 = meanConcept2Stereotype2/len(stereotype2)\n",
    "\n",
    "            differenceOfMeansConcept2 = differenceOfMeansConcept2+ meanConcept2Stereotype1 - meanConcept2Stereotype2\n",
    "\n",
    "        #effect size computations mean S(x,A,B)\n",
    "        differenceOfMeansConcept2 = differenceOfMeansConcept2/len(concept2)\n",
    "        differenceOfMeans = differenceOfMeansConcept1 - differenceOfMeansConcept2\n",
    "\n",
    "        #used for effect size computations before dividing by standard deviation\n",
    "        print(\"The difference of means is \", differenceOfMeans)\n",
    "        return differenceOfMeans\n",
    "\n",
    "    def getEntireDistribution(self, concept1, concept2, stereotype1, stereotype2, iterations, embedding, w2id):\n",
    "\n",
    "        bothConcepts = concept1 + concept2\n",
    "        distribution = []\n",
    "        print(\"Getting the entire distribution\")\n",
    "\n",
    "        for word in bothConcepts:\n",
    "            conceptEmbedding = embedding[w2id[word]]\n",
    "            similarityToStereotype1 = 0\n",
    "            similarityToStereotype2 = 0\n",
    "\n",
    "            for attribute in stereotype1:\n",
    "                stereotype1Embedding = embedding[w2id[attribute]]\n",
    "                similarityToStereotype1 = similarityToStereotype1 + self.cosineSimilarity(conceptEmbedding, stereotype1Embedding)\n",
    "            similarityToStereotype1 = similarityToStereotype1/len(stereotype1)\n",
    "\n",
    "            for attribute in stereotype2:\n",
    "                stereotype2Embedding = embedding[w2id[attribute]]\n",
    "                similarityToStereotype2 = similarityToStereotype2 + self.cosineSimilarity(conceptEmbedding, stereotype2Embedding)\n",
    "            similarityToStereotype2 = similarityToStereotype2/len(stereotype2)\n",
    "\n",
    "            distribution.append(similarityToStereotype1 - similarityToStereotype2)\n",
    "\n",
    "        return distribution\n",
    "\n",
    "    def cosineSimilarity(self,a, b):\n",
    "        a = [a]\n",
    "        b = [b]\n",
    "        r = sk_m.pairwise.cosine_similarity(a,b)\n",
    "        return r[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Career and family\n",
    "# Change from Bill to Tom as in paper to avoid ambiguity\n",
    "male_names = [\"john\", \"paul\", \"mike\", \"kevin\", \"steve\", \"greg\", \"jeff\", \"tom\"]\n",
    "female_names = [\"amy\", \"joan\", \"lisa\", \"sarah\", \"diana\", \"kate\", \"ann\", \"donna\"]\n",
    "career_attributes = [\"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"office\", \"business\", \"career\"]\n",
    "family_attributes = [\"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math and arts\n",
    "math_words = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\"]\n",
    "arts_words1 = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"sculpture\"]\n",
    "male_attributes1 = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
    "female_attributes1 = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science and arts\n",
    "science_words = [\"science\", \"technology\", \"pyhsics\", \"chemistry\", \"einstein\", \"nasa\", \"experiment\", \"astronomy\"]\n",
    "arts_words2 = [\"poetry\", \"art\", \"shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\"]\n",
    "male_attributes2 = [\"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\"]\n",
    "female_attributes2 = [\"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference of means is  0.14427444139552204\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "p-value:  0.00015898145565074184\n",
      "effect size:  1.805996189486473\n",
      "standard deviation:  0.04008756771386324\n"
     ]
    }
   ],
   "source": [
    "iterations = 100000\n",
    "wea_test = weat(male_names, female_names, career_attributes, family_attributes, iterations, embedding, w2id)\n",
    "pvalue, effect_size, sd = wea_test.getPValueAndEffect()\n",
    "print(\"p-value: \", pvalue)\n",
    "print(\"effect size: \", effect_size)\n",
    "print(\"standard deviation: \", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'brother'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6ed34859059e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"brother\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'brother'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
