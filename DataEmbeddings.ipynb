{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn import metrics as sk_m\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to retrive pre-trained 300 dimensional gloVe embedding\n",
    "embedding_300_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors.txt\"\n",
    "\n",
    "def read_embedding(url):\n",
    "    \"\"\"Function to read out an embedding\n",
    "    Input: url: url to embedding\n",
    "    \n",
    "    Returns: vocab: list of words in the embedding\n",
    "             word2id: dictionary mapping words to ids\n",
    "             word_vectors: array storing the embeddings,\n",
    "                           row corresponds to word id\"\"\"\n",
    "    # Open url\n",
    "    data = urllib.request.urlopen(url)\n",
    "    vocab = []\n",
    "    word_vectors = []\n",
    "    \n",
    "    # Each line contains one word and its embedding\n",
    "    for line in data:\n",
    "        line = line.decode()\n",
    "        # Split by spaces\n",
    "        split = line.split()\n",
    "        # First element(== the word) is added to vocabulary\n",
    "        vocab.append(split[0])\n",
    "        # All other elements(embedding vectors) are added to vectors\n",
    "        word_vectors.append([float(elem) for elem in split[1:]])\n",
    "    \n",
    "    # Create a dictionary with word-id pairs based on the order\n",
    "    word2id = {w: i for i, w in enumerate(vocab)}\n",
    "    # Vectors are converted into an array\n",
    "    word_vectors = np.array(word_vectors).astype(float)\n",
    "    \n",
    "    return vocab, word2id, word_vectors\n",
    "    \n",
    "embedding_300_vocab, embedding_300_word2id, embedding_300_word_vector = read_embedding(embedding_300_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_vocab(vocab, exclude):\n",
    "    \"\"\"Function to exclude specific words from vocabulary\n",
    "    Input: vocab: list of words in the embedding\n",
    "           exclude: list of words to exclude from the vocabulary\n",
    "           \n",
    "    Returns: limited_vocab: vocab without the words in exclude\"\"\"\n",
    "    # Create copies of vocab, word2id and word_vector\n",
    "    limited_vocab = vocab.copy()\n",
    "    # For all words that are in exclude and vocab\n",
    "    for word in exclude:\n",
    "        if word in limited_vocab:\n",
    "            # Remove word from vocab\n",
    "            limited_vocab.remove(word)\n",
    "            \n",
    "    return limited_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to female specific words\n",
    "female_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/female_word_file.txt\"\n",
    "female_words_data = urllib.request.urlopen(female_words_url)\n",
    "\n",
    "# List of female words\n",
    "female_words = []\n",
    "for line in female_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    female_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to male specific words\n",
    "male_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/male_word_file.txt\"\n",
    "male_words_data = urllib.request.urlopen(male_words_url)\n",
    "\n",
    "# List of male words\n",
    "male_words = []\n",
    "for line in male_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    male_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List with female - male pairs from female-male specific words\n",
    "female_male_pairs = []\n",
    "for i, female in enumerate(female_words):\n",
    "    female_male_pairs.append([female, male_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to the files storing gender specific words\n",
    "gender_specific_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/gender_specific_full.json\"\n",
    "\n",
    "# Empty list to accumulate gender specific words plus additional list after lowercasing\n",
    "gender_specific_original = []\n",
    "gender_specific = []\n",
    "\n",
    "\n",
    "# Read out URL and add further gender specific words\n",
    "with urllib.request.urlopen(gender_specific_url) as f:\n",
    "    gender_specific_original.extend(json.load(f))\n",
    "\n",
    "# Add lower case words to second list\n",
    "for word in gender_specific_original:\n",
    "    gender_specific.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing definitional pairs\n",
    "definitial_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/definitional_pairs.json\"\n",
    "\n",
    "# Empty list to store definitional pairs plus additional list after lowercasing\n",
    "definitional_pairs_original = []\n",
    "definitional_pairs = []\n",
    "\n",
    "\n",
    "# Read out url and add pairs in list\n",
    "with urllib.request.urlopen(definitial_pairs_url) as f:\n",
    "    definitional_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in definitional_pairs_original:\n",
    "    definitional_pairs.append([w1.lower(), w2.lower()])\n",
    "\n",
    "\n",
    "# Create list of single words instead of pairs  \n",
    "definitional_words = []\n",
    "for pair in definitional_pairs:\n",
    "    for word in pair:\n",
    "        definitional_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing the equalize pairs\n",
    "equalize_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/equalize_pairs.json\"\n",
    "\n",
    "# Empty list to store equalize pairs plus additional list after lowercasing\n",
    "equalize_pairs_original = []\n",
    "equalize_pairs = []\n",
    "\n",
    "# Read out URL and add pairs to list\n",
    "with urllib.request.urlopen(equalize_pairs_url) as f:\n",
    "    equalize_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in equalize_pairs_original:\n",
    "    equalize_pairs.append([w1.lower(), w2.lower()])\n",
    "    \n",
    "# Create list of single words instead of pairs\n",
    "equalize_words = []\n",
    "for pair in equalize_pairs:\n",
    "    for word in pair:\n",
    "        equalize_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all gender specific words included in \n",
    "# female words, male words, gender specific words, equalize words and definitional words\n",
    "exclude_words = list(set(female_words + male_words + gender_specific + definitional_words + equalize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  322636\n",
      "limited vocab size:  321977\n"
     ]
    }
   ],
   "source": [
    "# Remove gender specific words from the embedding to obtain vocabulary of neutral words\n",
    "embedding_300_vocab_neutral = exclude_vocab(embedding_300_vocab, exclude_words)\n",
    "print(\"vocab size: \", len(embedding_300_vocab))\n",
    "print(\"limited vocab size: \", len(embedding_300_vocab_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idtfy_gender_subspace(W, w2id, definitional_pairs, embedding, k=1):\n",
    "    C = np.ndarray((10, 2, 300))\n",
    "    for i, d_pair in enumerate(definitional_pairs):\n",
    "        for j, word in enumerate(d_pair):\n",
    "            mean_i = embedding[w2id[word]] / len(d_pair) # should be 2\n",
    "            C[i][j] = ((np.transpose(embedding[w2id[word]] - mean_i)*(embedding[w2id[word]] - mean_i))/len(d_pair))\n",
    "    C = C.reshape(10, 300, 2)\n",
    "    \n",
    "    \n",
    "    # is C supposed to be the covariance matrix of the word embeddings?\n",
    "    array = np.ndarray((10,2,300))\n",
    "    #i=0\n",
    "    array_two = np.zeros((10,300,300))\n",
    "    for j, d_pair in enumerate(definitional_pairs):\n",
    "        for i, word in enumerate(d_pair):\n",
    "            # fill array with embeddings\n",
    "            array[j][i] = embedding[w2id[word]]\n",
    "            #i = i+1\n",
    "        # print(array[j][0].shape)\n",
    "        # calculate covariance between embeddings of same definitional pair?\n",
    "        array_two[j]=np.cov(np.transpose(array[j]))\n",
    "        \n",
    "    # print(array_two.shape)\n",
    "    #new_C = np.cov(array)\n",
    "    #print(np.shape(new_C))\n",
    "    _, new_SVD_C, _ = np.linalg.svd(array_two)\n",
    "    print(np.shape(new_SVD_C))\n",
    "    new_B = new_SVD_C[:k]\n",
    "    print(new_B.shape)\n",
    "    \n",
    "    _, SVD_C, _ = np.linalg.svd(C, full_matrices = True)\n",
    "    # print(np.shape(SVD_C))\n",
    "    B = SVD_C[:k]\n",
    "    return new_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300)\n",
      "(1, 300)\n",
      "[[1.22104626e+01 3.75520940e-15 3.71054662e-15 3.69942786e-15\n",
      "  3.34145689e-15 2.98664685e-15 2.46348792e-15 2.16273247e-15\n",
      "  1.98006134e-15 1.66441679e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.22005497e-15\n",
      "  1.22005497e-15 1.22005497e-15 1.22005497e-15 1.20924187e-15\n",
      "  6.91355500e-16 6.50362362e-16 4.97099590e-16 6.50669335e-18]]\n"
     ]
    }
   ],
   "source": [
    "B = idtfy_gender_subspace(embedding_300_vocab, embedding_300_word2id, definitional_pairs, embedding_300_word_vector)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most biased male and female words\n",
    "def most_biased(embedding, B, k=500):\n",
    "    # small x, else memory issues\n",
    "    x = 50000\n",
    "    all_biased = np.ndarray((x,1))\n",
    "    for i, word in enumerate(embedding):\n",
    "        if i < x:\n",
    "            all_biased[i] = (sk_m.pairwise.cosine_similarity(word.reshape(1,300), B))[0]\n",
    "            # print(sk_m.pairwise.cosine_similarity(word.reshape(1,300), B)[0])\n",
    "    #print(all_biased)\n",
    "    most_biased_m = []\n",
    "    most_biased_f = []\n",
    "    for word in range(k):\n",
    "        # male words\n",
    "        mb_index = np.argmax(all_biased)\n",
    "        most_biased_m.append(mb_index)\n",
    "        all_biased[mb_index] = 0\n",
    "        # female words\n",
    "        fb_index = np.argmin(all_biased)\n",
    "        most_biased_f.append(fb_index)\n",
    "        all_biased[fb_index] = 0\n",
    "    #print(most_biased_m, most_biased_f)\n",
    "    return most_biased_m, most_biased_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mauro\n",
      "docudrama\n",
      "astana\n",
      "gijón\n",
      "revue\n",
      "conti\n",
      "nightclub\n",
      "dino\n",
      "wicket\n",
      "cloudy\n",
      "presenter\n",
      "despicable\n",
      "debutant\n",
      "luxembourg\n",
      "dirty\n",
      "goon\n",
      "almaty\n",
      "cavern\n",
      "windy\n",
      "hyena\n",
      "cobbled\n",
      "uae\n",
      "napoli\n",
      "kr\n",
      "fables\n",
      "animated\n",
      "comedians\n",
      "monaco\n",
      "terri\n",
      "basket\n",
      "cave\n",
      "gilan\n",
      "taipei\n",
      "reboot\n",
      "chilly\n",
      "ganguly\n",
      "debut\n",
      "itv1\n",
      "footballers\n",
      "sevilla\n",
      "lawman\n",
      "kalmar\n",
      "hb\n",
      "thoroughbreds\n",
      "dijon\n",
      "chalk\n",
      "dung\n",
      "potok\n",
      "mazarin\n",
      "asymptomatic\n",
      "vitória\n",
      "mallorca\n",
      "autobahn\n",
      "raced\n",
      "quay\n",
      "underbelly\n",
      "ponies\n",
      "albacete\n",
      "loach\n",
      "pit\n",
      "dubai\n",
      "métro\n",
      "cabaret\n",
      "petah\n",
      "allard\n",
      "highschool\n",
      "shameless\n",
      "lara\n",
      "substitute\n",
      "beetle\n",
      "série\n",
      "berra\n",
      "rté\n",
      "thief\n",
      "boland\n",
      "darkest\n",
      "gemstone\n",
      "amber\n",
      "scriptwriter\n",
      "balls\n",
      "lionheart\n",
      "shillong\n",
      "muscat\n",
      "d1\n",
      "ostrava\n",
      "szczecin\n",
      "aurelio\n",
      "adaption\n",
      "sesame\n",
      "viktoria\n",
      "246\n",
      "cricketing\n",
      "cricket\n",
      "taker\n",
      "cinderella\n",
      "appearances\n",
      "pantomime\n",
      "taichung\n",
      "getafe\n",
      "revues\n",
      "2021\n",
      "cartoon\n",
      "bouncer\n",
      "cbbc\n",
      "sloth\n",
      "shelly\n",
      "crawling\n",
      "tashkent\n",
      "cellar\n",
      "forecourt\n",
      "clube\n",
      "gritty\n",
      "dirt\n",
      "flavio\n",
      "erfurt\n",
      "minute\n",
      "grave\n",
      "equalizer\n",
      "maserati\n",
      "début\n",
      "universitario\n",
      "stage\n",
      "quesada\n",
      "comedian\n",
      "qatar\n",
      "itv\n",
      "marrakech\n",
      "rohan\n",
      "morelia\n",
      "sidewalk\n",
      "appearing\n",
      "skeletons\n",
      "puppeteer\n",
      "marquee\n",
      "faraway\n",
      "notoriously\n",
      "fabulous\n",
      "deepest\n",
      "pauper\n",
      "kazakhstan\n",
      "bannister\n",
      "cordoba\n",
      "stade\n",
      "mares\n",
      "timer\n",
      "aired\n",
      "vaudeville\n",
      "banfield\n",
      "cricketers\n",
      "fabled\n",
      "rectangle\n",
      "miniseries\n",
      "hertha\n",
      "fy\n",
      "resistor\n",
      "jar\n",
      "dancer\n",
      "heist\n",
      "skylight\n",
      "città\n",
      "bergerac\n",
      "zlín\n",
      "freddy\n",
      "portraying\n",
      "creamy\n",
      "margate\n",
      "malappuram\n",
      "showcasing\n",
      "comedy\n",
      "keeper\n",
      "spirited\n",
      "zane\n",
      "bollywood\n",
      "ridden\n",
      "bunny\n",
      "donkey\n",
      "cosmopterigidae\n",
      "stepping\n",
      "sketch\n",
      "dhabi\n",
      "houdini\n",
      "grosseto\n",
      "bowling\n",
      "guimarães\n",
      "ol\n",
      "loess\n",
      "drayton\n",
      "thieves\n",
      "köln\n",
      "chipped\n",
      "ranji\n",
      "documentary\n",
      "ide\n",
      "python\n",
      "viva\n",
      "eishockey\n",
      "penning\n",
      "ground\n",
      "jed\n",
      "brasileiro\n",
      "magician\n",
      "facilitator\n",
      "ellipse\n",
      "cup\n",
      "ince\n",
      "olimpia\n",
      "feces\n",
      "schoolboys\n",
      "batsmen\n",
      "swordsman\n",
      "urchin\n",
      "cosby\n",
      "globes\n",
      "crumbling\n",
      "103\n",
      "stag\n",
      "101\n",
      "lad\n",
      "rickshaw\n",
      "comedies\n",
      "gem\n",
      "dwellers\n",
      "utv\n",
      "stuntman\n",
      "cozy\n",
      "plzeň\n",
      "188\n",
      "théâtre\n",
      "shari\n",
      "forbidden\n",
      "2b\n",
      "anthropomorphic\n",
      "youngsters\n",
      "maidan\n",
      "neighbours\n",
      "wretched\n",
      "doha\n",
      "youngster\n",
      "increment\n",
      "tv\n",
      "cheetahs\n",
      "fossilized\n",
      "angolan\n",
      "daring\n",
      "abbottabad\n",
      "lola\n",
      "j2\n",
      "filth\n",
      "dug\n",
      "merlin\n",
      "bbc\n",
      "arabian\n",
      "chronicling\n",
      "laughing\n",
      "galloping\n",
      "deen\n",
      "futsal\n",
      "driveway\n",
      "striker\n",
      "fabio\n",
      "circus\n",
      "2019\n",
      "chemnitz\n",
      "camelot\n",
      "indonesia\n",
      "valletta\n",
      "legendary\n",
      "hannover\n",
      "roadside\n",
      "fifa\n",
      "micromollusk\n",
      "bounding\n",
      "2018\n",
      "fairground\n",
      "barbieri\n",
      "hedge\n",
      "187\n",
      "entertainers\n",
      "esporte\n",
      "92\n",
      "bilbao\n",
      "bailout\n",
      "equaliser\n",
      "racing\n",
      "zürich\n",
      "afternoons\n",
      "ceará\n",
      "dummies\n",
      "2022\n",
      "show\n",
      "loretta\n",
      "musik\n",
      "gaborone\n",
      "incline\n",
      "freshly\n",
      "telenovelas\n",
      "changchun\n",
      "pavement\n",
      "crawl\n",
      "lunch\n",
      "exponent\n",
      "eupithecia\n",
      "fk\n",
      "pinocchio\n",
      "puppet\n",
      "antiques\n",
      "zoey\n",
      "necropolis\n",
      "nurturing\n",
      "qualifier\n",
      "euro\n",
      "namibian\n",
      "scaly\n",
      "kale\n",
      "keeler\n",
      "struggling\n",
      "hsv\n",
      "indy\n",
      "goalscorer\n",
      "dismal\n",
      "cartoons\n",
      "laxman\n",
      "unpaved\n",
      "valenzuela\n",
      "television\n",
      "transgression\n",
      "gwynne\n",
      "zug\n",
      "dinosaur\n",
      "cupboard\n",
      "graveyard\n",
      "secrets\n",
      "terriers\n",
      "lanka\n",
      "smooth\n",
      "alam\n",
      "pits\n",
      "tee\n",
      "loophole\n",
      "bandung\n",
      "maze\n",
      "nala\n",
      "kabaddi\n",
      "haunted\n",
      "contacting\n",
      "kanal\n",
      "uefa\n",
      "saeed\n",
      "goalkeeper\n",
      "marionette\n",
      "volkswagen\n",
      "marl\n",
      "123\n",
      "nights\n",
      "europa\n",
      "semen\n",
      "sinbad\n",
      "showcase\n",
      "played\n",
      "breakfast\n",
      "eastenders\n",
      "troupe\n",
      "bridgetown\n",
      "kinder\n",
      "scored\n",
      "syphilis\n",
      "rtv\n",
      "filthy\n",
      "españa\n",
      "rogues\n",
      "hilly\n",
      "caen\n",
      "debuts\n",
      "mulligan\n",
      "rtl\n",
      "disappearing\n",
      "confessions\n",
      "tamer\n",
      "lisboa\n",
      "exit\n",
      "unlucky\n",
      "roasted\n",
      "ulaanbaatar\n",
      "jungle\n",
      "brothel\n",
      "handball\n",
      "roadshow\n",
      "oman\n",
      "peril\n",
      "pescara\n",
      "series\n",
      "kaunas\n",
      "sideshow\n",
      "tilly\n",
      "brasília\n",
      "tuff\n",
      "aw\n",
      "jeddah\n",
      "badminton\n",
      "performer\n",
      "riyadh\n",
      "broadcaster\n",
      "morehead\n",
      "auvergne\n",
      "pelvis\n",
      "preferably\n",
      "pebbles\n",
      "alley\n",
      "mundi\n",
      "seedy\n",
      "talib\n",
      "aged\n",
      "gif\n",
      "showroom\n",
      "getaway\n",
      "39\n",
      "liga\n",
      "brasil\n",
      "babu\n",
      "scarab\n",
      "byrnes\n",
      "dir\n",
      "walking\n",
      "orchestre\n",
      "carrera\n",
      "shaun\n",
      "mummy\n",
      "109\n",
      "radwańska\n",
      "beginner\n",
      "soak\n",
      "krypton\n",
      "witches\n",
      "tough\n",
      "puss\n",
      "pub\n",
      "jolly\n",
      "pig\n",
      "animators\n",
      "cafe\n",
      "attractions\n",
      "cow\n",
      "mumbai\n",
      "mccourt\n",
      "hound\n",
      "mistresses\n",
      "chişinău\n",
      "seasoned\n",
      "club\n",
      "wiesbaden\n",
      "horses\n",
      "imaginary\n",
      "jos\n",
      "rickshaws\n",
      "oval\n",
      "div\n",
      "goalkeepers\n",
      "zoo\n",
      "102\n",
      "spotlight\n",
      "penultimate\n",
      "málaga\n",
      "city\n",
      "stockbroker\n",
      "inhabitant\n",
      "asian\n",
      "ernie\n",
      "alleys\n",
      "swine\n",
      "185\n",
      "yogi\n",
      "ansari\n",
      "artiste\n",
      "cahill\n",
      "186\n",
      "rusticated\n",
      "pearls\n",
      "countdown\n",
      "disney\n",
      "doordarshan\n",
      "movies\n",
      "innings\n",
      "wiener\n",
      "obec\n",
      "mx\n",
      "policeman\n",
      "paulo\n",
      "236\n",
      "batsman\n",
      "alte\n",
      "1s\n",
      "bogotá\n",
      "lua\n",
      "mineiro\n",
      "friendlies\n",
      "beggar\n",
      "sock\n",
      "zdf\n",
      "exiting\n",
      "chicks\n"
     ]
    }
   ],
   "source": [
    "index_m, index_f = most_biased(embedding_300_word_vector, B)\n",
    "for i in index_m:\n",
    "    print(embedding_300_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"female words:\")\n",
    "for i in index_f:\n",
    "    print(embedding_300_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_most_biased = [embedding_300_vocab[i] for i in index_m]\n",
    "female_most_biased = [embedding_300_vocab[i] for i in index_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_hard_debias(words, index_m, index_f, w2id):\n",
    "    \"\"\"Double Hard Debias:\n",
    "    \n",
    "    words: word embeddings of some corpus\n",
    "    males: set of most biased male words \n",
    "    females: set of most biased female words\n",
    "    w2id:\n",
    "    \"\"\"\n",
    "    \n",
    "    males = [words[i] for i in index_m]\n",
    "    females = [words[i] for i in index_f]    \n",
    "    \n",
    "    #need: Word embeddings, top 500 Male biased words set Wm, top 500 Female biased words set Wf\n",
    "    #1. for all word embeddings: decentralize all words\n",
    "    mue = (len(words)**(-1)) * np.sum(words, axis=0)\n",
    "    # print(mue)\n",
    "    words_decen = np.zeros((words.shape))\n",
    "    for index, embedding in enumerate(words):\n",
    "        # print(index,\":\",embedding)\n",
    "        words_decen[index] = embedding - mue\n",
    "    \n",
    "    #print(\"decentralized:\",words_decen)\n",
    "    #print(\"origin:\",words)\n",
    "        \n",
    "    #2. for all decentralized embeddings: compute PCA\n",
    "    #princ_comp = np.asarray(pca_tft(words_decen))\n",
    "    #print(\"Principal Components:\",princ_comp)\n",
    "    pca = PCA().fit(words_decen)\n",
    "    princ_comp = pca.components_\n",
    "\n",
    "    #print(\"Sklearn PC:\", pca.components_)\n",
    "\n",
    "    evaluations = []\n",
    "\n",
    "    #3. for all principal components:\n",
    "    for pc in princ_comp:\n",
    "        male_proj = np.zeros((len(males),300))\n",
    "        male_debias = np.zeros((len(males),300))\n",
    "        female_proj = np.zeros((len(females),300))\n",
    "        female_debias = np.zeros((len(females),300))\n",
    "   \n",
    "        for index, male in enumerate(males):\n",
    "        #male embedding = decentralized embedding - projected original (?) embedding into direction of PC\n",
    "            print((male-mue).shape, ((np.transpose(pc)*male)*pc).shape)\n",
    "            male_proj[index] = (male - mue) - ((np.transpose(pc)*male)*pc)\n",
    "            #with all new male embeddings: HardDebias\n",
    "            male_debias[index] = hard_debias(male_proj[index])\n",
    "        \n",
    "        for index, female in enumerate(females):\n",
    "        #female embedding = decentralized embedding - projected original (?) embedding into direction of PC\n",
    "            female_proj[index] = (female - mue) - ((np.transpose(pc)*male)*pc)\n",
    "            #with all new female embeddings: HardDebias\n",
    "            female_debias[index] = hard_debias(female_proj[index])\n",
    "    \n",
    "        #for all HardDebiased embeddings: KMeansClustering (2)\n",
    "        #for clustered embeddings: compute gender alignment accuracy\n",
    "        #4. store evaluations for each principal components\n",
    "        evaluations.append(align_acc(male_debias, female_debias))\n",
    "    \n",
    "    #5. evaluate which PC lead to most random cluster (evaluation smallest (close to 0.5), used second PC)\n",
    "    best_eval = evaluations.index(np.min(evaluations))\n",
    "    best_pc = princ_comp[best_eval]\n",
    "    #print(\"Best PC:\",best_pc,\"with evaluation:\",evaluations[best_eval])\n",
    "\n",
    "    first_debias = np.zeros((words.shape))\n",
    "    #6. for all decentralized embeddings: remove that PC-direction\n",
    "    for index,word in enumerate(words_decen):\n",
    "        first_debias[index] = word - ((np.transpose(best_pc)*words[index])*best_pc)\n",
    "    \n",
    "    #7. for all new embeddings: HardDebias\n",
    "    double_debias = np.zeros((words.shape))\n",
    "    for index,word in enumerate(first_debias):\n",
    "        double_debias[index] = hard_debias(word)\n",
    "\n",
    "    return double_debias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-d1b2fd0d0ccd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdouble_hard_debias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_300_word_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_300_word2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-0bd0adab0e9a>\u001b[0m in \u001b[0;36mdouble_hard_debias\u001b[1;34m(words, index_m, index_f, w2id)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mmale_proj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmale\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m#with all new male embeddings: HardDebias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mmale_debias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhard_debias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmale_proj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfemale\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfemales\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-c92f658d7cf2>\u001b[0m in \u001b[0;36mhard_debias\u001b[1;34m(N, equalize_pairs, embedding, B)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me_pair\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mequalize_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0me_pair\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m             \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me_pair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m             \u001b[0msimple_average_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean_B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mw_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple_average_v\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m^\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_B\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean_B\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_B\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean_B\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#normed, v should also be normed (||v||)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "result = double_hard_debias(embedding_300_word_vector, index_m, index_f, embedding_300_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender alignment accuracy/ Neighborhood Metric:\n",
    "def align_acc(males, females):\n",
    "    \"\"\"bias measurement using KMeans Clustering\n",
    "    \n",
    "    takes female and male word's embeddings\n",
    "    ground truth labels:\n",
    "    0 = male,\n",
    "    1 = female\"\"\"\n",
    "    \n",
    "    #need: k (=1000) most biased female and male word's embedding (cosine similarity embedding & gender direction),\n",
    "    #1. assign ground truth gender labels: 0 = male, 1 = female\n",
    "    #2. run KMeans on embeddings\n",
    "    kmeans = KMeans(n_clusters=2).fit(np.concatenate((males, females)))\n",
    "    split = males.shape[0]\n",
    "    correct = 0\n",
    "    #print(kmeans.labels_)\n",
    "    #3. compute alignment score: cluster assignment vs ground truth gender label\n",
    "    for i in range(np.concatenate((males, females)).shape[0]):\n",
    "        if i < split and kmeans.labels_[i] == 0:\n",
    "            correct+= 1\n",
    "        elif i >= split and kmeans.labels_[i] == 1:\n",
    "            correct += 1\n",
    "    \n",
    "    #4. alignment score = max(a, 1-a)\n",
    "    alignment = 1/(2*2) * correct\n",
    "    alignment = np.maximum(alignment, 1-alignment)\n",
    "    \n",
    "    return alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_debias (N, equalize_pairs=equalize_pairs, embedding=embedding_300_word_vector, B=B):\n",
    "    for i, e_pair in enumerate (equalize_pairs):\n",
    "        for word in e_pair:\n",
    "            mean = word/len(e_pair)\n",
    "            simple_average_v = mean - mean_B\n",
    "            w_embedding = simple_average_v + np.sqrt(1-v^2) * ((embedding_B - mean_B)/(embedding_B - mean_B)) #normed, v should also be normed (||v||)\n",
    "    return B, w_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d1b2fd0d0ccd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdouble_hard_debias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_300_word_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_300_word2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-e9558502fa62>\u001b[0m in \u001b[0;36mdouble_hard_debias\u001b[1;34m(words, index_m, index_f, w2id)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m#3. for all principal components:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprinc_comp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mmale_proj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmales\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mmale_debias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmales\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mfemale_proj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfemales\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
