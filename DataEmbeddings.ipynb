{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "from sklearn import metrics as sk_m\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "from scipy.stats import norm\n",
    "import statistics as stat\n",
    "from weat import weat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to retrive pre-trained 300 dimensional gloVe embedding\n",
    "embedding_300_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors.txt\"\n",
    "\n",
    "def read_embedding(url, skip_first = False):\n",
    "    \"\"\"Function to read out an embedding\n",
    "    Input: url: url to embedding\n",
    "    \n",
    "    Returns: vocab: list of words in the embedding\n",
    "             w2id: dictionary mapping words to ids\n",
    "             embedding: array storing the word vectors,\n",
    "                           row corresponds to word id\"\"\"\n",
    "    # Open url\n",
    "    data = urllib.request.urlopen(url)\n",
    "    vocab = []\n",
    "    embedding = []\n",
    "    \n",
    "    # Each line contains one word and its embedding\n",
    "    for i, line in enumerate(data):\n",
    "        if skip_first:\n",
    "            if i == 0:\n",
    "                continue\n",
    "        #if len(line) == 301:\n",
    "        line = line.decode()\n",
    "        # Split by spaces\n",
    "        split = line.split()\n",
    "        # First element(== the word) is added to vocabulary\n",
    "        vocab.append(split[0])\n",
    "        # All other elements(embedding vectors) are added to vectors\n",
    "        embedding.append([float(elem) for elem in split[1:]])\n",
    "    \n",
    "    # Create a dictionary with word-id pairs based on the order\n",
    "    w2id = {w: i for i, w in enumerate(vocab)}\n",
    "    # Vectors are converted into an array\n",
    "    embedding = np.array(embedding).astype(float)\n",
    "    \n",
    "    return vocab, w2id, embedding\n",
    "    \n",
    "vocab_original, w2id_original, embedding_original = read_embedding(embedding_300_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasDigit(word):\n",
    "    \"\"\"Checks if a string contains any digits\"\"\"\n",
    "    return any(char.isdigit() for char in word)\n",
    "\n",
    "def hasSpecialChar(word):\n",
    "    \"\"\"Checks if a string contains special characters(except \"_\")\"\"\"\n",
    "    special_characters = \"!@#$%^&*()-+?=,<>/.\"\n",
    "    return any(char in special_characters for char in word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_vocab(vocab, w2id, embedding):\n",
    "    \"\"\"Limits the vocab by removing words containing digits or special characters\n",
    "    Input: vocab: list of words in the embedding\n",
    "           w2id: dictionary mapping words to ids\n",
    "           embedding: array storing the word vectors\n",
    "           \n",
    "    Returns: limit_vocab: list of words in vocab that do not include digits or special characters\n",
    "             limit_w2id: dictionary mapping words in limit_vocab to new ids\n",
    "             limit_embedding: array storing the word vectors of the words in limit_vocab only\"\"\"\n",
    "    limit_vocab = []\n",
    "    limit_embedding = []\n",
    "    \n",
    "    for i, word in enumerate(vocab[:50000]): # hoping that this gives us the most common words\n",
    "        # If word includes either a digit or a special character move on to next word\n",
    "        if (hasDigit(word) or hasSpecialChar(word)):\n",
    "            continue\n",
    "        # Else add word to limit_vocab and its embedding to limit_embedding    \n",
    "        limit_vocab.append(word)\n",
    "        limit_embedding.append(embedding[w2id[word]])\n",
    "        \n",
    "    # Convert embedding into an array    \n",
    "    limit_embedding = np.array(limit_embedding).astype(float)\n",
    "    # Create new dictionary containing only the words in limit_vocab and their new ids\n",
    "    limit_w2id = {word: i for i, word in enumerate(limit_vocab)}\n",
    "    \n",
    "    return limit_vocab, limit_w2id, limit_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size:  322636\n",
      "Restricted vocab size:  47974\n"
     ]
    }
   ],
   "source": [
    "vocab, w2id, embedding = restrict_vocab(vocab_original, w2id_original, embedding_original)\n",
    "print(\"Original vocab size: \", len(vocab_original))\n",
    "print(\"Restricted vocab size: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_vocab(vocab, exclude):\n",
    "    \"\"\"Function to exclude specific words from vocabulary\n",
    "    Input: vocab: list of words in the embedding\n",
    "           exclude: list of words to exclude from the vocabulary\n",
    "           \n",
    "    Returns: limited_vocab: vocab without the words in exclude\"\"\"\n",
    "    # Create copy of vocab\n",
    "    limited_vocab = vocab.copy()\n",
    "    # For all words that are in exclude and vocab\n",
    "    for word in exclude:\n",
    "        if word in limited_vocab:\n",
    "            # Remove word from vocab\n",
    "            limited_vocab.remove(word)\n",
    "            \n",
    "    return limited_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to female specific words as listed by the authors\n",
    "female_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/female_word_file.txt\"\n",
    "female_words_data = urllib.request.urlopen(female_words_url)\n",
    "\n",
    "# List of female words\n",
    "female_words = []\n",
    "for line in female_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    female_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to male specific words as listed by the authors\n",
    "male_words_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/male_word_file.txt\"\n",
    "male_words_data = urllib.request.urlopen(male_words_url)\n",
    "\n",
    "# List of male words\n",
    "male_words = []\n",
    "for line in male_words_data:\n",
    "    line = line.decode()\n",
    "    line = line.split()\n",
    "    male_words.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List with female - male pairs from female-male specific words\n",
    "female_male_pairs = []\n",
    "for i, female in enumerate(female_words):\n",
    "    female_male_pairs.append([female, male_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to the files storing gender specific words as listed by the authors\n",
    "gender_specific_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/gender_specific_full.json\"\n",
    "\n",
    "# Empty list to accumulate gender specific words plus additional list after lowercasing\n",
    "gender_specific_original = []\n",
    "gender_specific = []\n",
    "\n",
    "\n",
    "# Read out URL and add further gender specific words\n",
    "with urllib.request.urlopen(gender_specific_url) as f:\n",
    "    gender_specific_original.extend(json.load(f))\n",
    "\n",
    "# Add lower case words to second list\n",
    "for word in gender_specific_original:\n",
    "    gender_specific.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing definitional pairs as listed by the authors\n",
    "definitial_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/definitional_pairs.json\"\n",
    "\n",
    "# Empty list to store definitional pairs plus additional list after lowercasing\n",
    "definitional_pairs_original = []\n",
    "definitional_pairs = []\n",
    "\n",
    "\n",
    "# Read out url and add pairs in list\n",
    "with urllib.request.urlopen(definitial_pairs_url) as f:\n",
    "    definitional_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in definitional_pairs_original:\n",
    "    definitional_pairs.append([w1.lower(), w2.lower()])\n",
    "\n",
    "\n",
    "# Create list of single words instead of pairs  \n",
    "definitional_words = []\n",
    "for pair in definitional_pairs:\n",
    "    for word in pair:\n",
    "        definitional_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the file storing the equalize pairs as listed by the authors\n",
    "equalize_pairs_url = \"https://raw.githubusercontent.com/uvavision/Double-Hard-Debias/master/data/equalize_pairs.json\"\n",
    "\n",
    "# Empty list to store equalize pairs plus additional list after lowercasing\n",
    "equalize_pairs_original = []\n",
    "equalize_pairs = []\n",
    "\n",
    "# Read out URL and add pairs to list\n",
    "with urllib.request.urlopen(equalize_pairs_url) as f:\n",
    "    equalize_pairs_original.extend(json.load(f))\n",
    "    \n",
    "# Add lower case pairs to second list\n",
    "for [w1, w2] in equalize_pairs_original:\n",
    "    equalize_pairs.append([w1.lower(), w2.lower()])\n",
    "    \n",
    "# Create list of single words instead of pairs\n",
    "equalize_words = []\n",
    "for pair in equalize_pairs:\n",
    "    for word in pair:\n",
    "        equalize_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all gender specific words included in \n",
    "# female words, male words, gender specific words, equalize words and definitional words\n",
    "exclude_words = list(set(female_words + male_words + gender_specific + definitional_words + equalize_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(word, w2id=w2id, embedding=embedding):\n",
    "    return embedding[w2id[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  47974\n",
      "Neutral vocab size:  47597 (47597, 300) 47597\n"
     ]
    }
   ],
   "source": [
    "# Remove gender specific words from the embedding to obtain vocabulary of neutral words\n",
    "vocab_neutral = exclude_vocab(vocab, exclude_words)\n",
    "# save both neutral embeddings and the indices of the neutral words in original vocab\n",
    "embedding_neutral = np.asarray([embed(word) for word in vocab_neutral])\n",
    "id_neutral = [w2id[neutral] for neutral in vocab_neutral]\n",
    "\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "print(\"Neutral vocab size: \", len(vocab_neutral), embedding_neutral.shape, len(id_neutral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load further embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gn_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors300.txt\"\n",
    "vocab_gn_original, w2id_gn_original, embedding_gn_original = read_embedding(embedding_gn_url)\n",
    "vocab_gn, w2id_gn, embedding_gn = restrict_vocab(vocab_gn_original, w2id_gn_original, embedding_gn_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias_gn(wv):\n",
    "    for v in wv:\n",
    "        assert(len(v) == 300)\n",
    "    \n",
    "    wv = wv[:,:-1]\n",
    "\n",
    "    for v in wv:\n",
    "        assert(len(v) == 299)\n",
    "    return wv\n",
    "\n",
    "vocab_gn_a = vocab_gn\n",
    "w2id_gn_a = w2id_gn\n",
    "embedding_gn_a = debias_gn(embedding_gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_hd_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors_hd.txt\"\n",
    "vocab_hd_original, w2id_hd_original, embedding_hd_original = read_embedding(embedding_hd_url)\n",
    "vocab_hd, w2id_hd, embedding_hd = restrict_vocab(vocab_hd_original, w2id_hd_original, embedding_hd_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_hd_a_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/vectors_hd_a.txt\"\n",
    "vocab_hd_a_original, w2id_hd_a_original, embedding_hd_a_original = read_embedding(embedding_hd_a_url)\n",
    "vocab_hd_a, w2id_hd_a, embedding_hd_a = restrict_vocab(vocab_hd_a_original, w2id_hd_a_original, embedding_hd_a_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gp_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/gp_glove.txt\"\n",
    "vocab_gp_original, w2id_gp_original, embedding_gp_original = read_embedding(embedding_gp_url, skip_first = True)\n",
    "vocab_gp, w2id_gp, embedding_gp = restrict_vocab(vocab_gp_original, w2id_gp_original, embedding_gp_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size:  322636\n",
      "Restricted vocab size:  314952\n"
     ]
    }
   ],
   "source": [
    "embedding_gp_gn_url = \"http://www.cs.virginia.edu/~tw8cb/word_embeddings/gp_gn_glove.txt\"\n",
    "vocab_gp_gn_original, w2id_gp_gn_original, embedding_gp_gn_original = read_embedding(embedding_gp_gn_url, skip_first = True)\n",
    "vocab_gp_gn, w2id_gp_gn, embedding_gp_gn = restrict_vocab(vocab_gp_gn_original, w2id_gp_gn_original, embedding_gp_gn_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idtfy_gender_subspace(word_sets, w2id, defining_sets, k=1):\n",
    "    \"\"\"\n",
    "    identifies the bias (gender) subspace following Bolukbasi et al. 2016\n",
    "    \n",
    "    takes\n",
    "    word_sets: vocabulary\n",
    "    w2id: a dictionary to translate words contained in the vocabulary into their corresponding IDs\n",
    "    defining_sets: N defining sets (pairs if I=2) consisting of I words that differ mainly on the bias (gender) direction\n",
    "    embedding: the embedding of the vocabulary\n",
    "    k: an integer parameters that defines how many rows of SVD(C) constitute the bias (gender) subspace B, bias (gender) direction if k=1\n",
    "    \n",
    "    returns\n",
    "    bias_subspace: linear bias (gender) subspace (direction) that is assumed to capture most of the gender bias (denoted as B in Bolukbasi et al. 2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    C = []\n",
    "    for female_word, male_word in defining_sets:\n",
    "        mean = (embed(female_word) + embed(male_word)) /2\n",
    "        C.append(embed(female_word) - mean)\n",
    "        C.append(embed(male_word) - mean)\n",
    "    C = np.array(C)\n",
    "    \n",
    "    # applying PCA is the same as SVD when interpreting C as covariance matrix (Vargas & Cotterell 2020)\n",
    "    pca = PCA(n_components = 10)\n",
    "    pca.fit(C)\n",
    "\n",
    "    # take the first k pcas (first for gender direction)\n",
    "    B = []\n",
    "    for i in range(k):\n",
    "        B.append(pca.components_[i])\n",
    "    B = np.array(B).flatten()\n",
    "\n",
    "    \n",
    "    #print(\"new_B\")\n",
    "    #array = np.ndarray((10,2,300))\n",
    "    #i=0\n",
    "    #array_two = np.zeros((10,300,300))\n",
    "    #for j, d_pair in enumerate(definitional_pairs):\n",
    "    #    for i, word in enumerate(d_pair):\n",
    "    #        # fill array with embeddings\n",
    "    #        array[j][i] = embedding[w2id[word]]\n",
    "            #i = i+1\n",
    "        # print(array[j][0].shape)\n",
    "        # calculate covariance between embeddings of same definitional pair?\n",
    "    #    array_two[j]=np.cov(np.transpose(array[j]))\n",
    "        \n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_subspace = idtfy_gender_subspace(vocab, w2id, definitional_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most biased 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most biased male and female words\n",
    "def most_biased(embedding, B, k=500):\n",
    "    # small x, else memory issues\n",
    "    #x = 50000\n",
    "    all_biased = np.ndarray((len(embedding),1))\n",
    "    for i, word in enumerate(embedding):\n",
    "        #if i < x:\n",
    "        all_biased[i] = (sk_m.pairwise.cosine_similarity(word.reshape(1, 300), B.reshape(1, 300)))[0]\n",
    "            # print(sk_m.pairwise.cosine_similarity(word.reshape(1,300), B)[0])\n",
    "    #print(all_biased)\n",
    "    most_biased_f = []\n",
    "    most_biased_m = []\n",
    "    for word in range(k):\n",
    "        # female words\n",
    "        fb_index = np.argmin(all_biased)\n",
    "        most_biased_f.append(fb_index)\n",
    "        all_biased[fb_index] = 0\n",
    "        # male words\n",
    "        mb_index = np.argmax(all_biased)\n",
    "        most_biased_m.append(mb_index)\n",
    "        all_biased[mb_index] = 0\n",
    "    #print(most_biased_f, most_biased_m)\n",
    "    return most_biased_f, most_biased_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_f, index_m = most_biased(embedding_neutral, gender_subspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female ['sean', 'ibm', 'liberalization', 'ghazni', 'comparable', 'sails', 'prevalence', 'workstations', 'decomposing', 'burners', 'boatswain', 'spruce', 'websites', 'graves', 'businessperson', 'instituted', 'shaped', 'desolation', 'elven', 'rebuke', 'offenbach', 'rudi', 'rapids', 'reflex', 'aotearoa', 'correctly', 'replication', 'victorious', 'llewellyn', 'mercer', 'basra', 'alternating', 'pontiff', 'firsthand', 'tippett', 'exciting', 'sore', 'creeds', 'nk', 'reggaeton', 'deathly', 'selkirk', 'practicality', 'epidemics', 'disseminating', 'dive', 'unprecedented', 'sookie', 'typeface', 'brownian', 'statutes', 'nationals', 'volleyball', 'authorship', 'ct', 'floodplain', 'consumers', 'cleveland', 'sparta', 'lawrence', 'beer', 'concentrate', 'suspicions', 'unattractive', 'masculine', 'angolan', 'lords', 'decorations', 'draft', 'excitation', 'thor', 'boyer', 'jihadist', 'puritans', 'gautama', 'phage', 'realtime', 'liang', 'vocations', 'widescreen', 'winnings', 'campeonato', 'sedative', 'charging', 'bianchi', 'broadsides', 'plate', 'nato', 'cowper', 'selby', 'teamwork', 'nyasaland', 'adjust', 'casing', 'equity', 'tuning', 'inquiry', 'corridor', 'overtake', 'haskell', 'erskine', 'protocols', 'benin', 'huntingdonshire', 'cohort', 'foreclosure', 'darth', 'alleles', 'scanning', 'commencement', 'unlock', 'cathay', 'expired', 'welcome', 'slobodan', 'antiochus', 'overbearing', 'picasso', 'anyang', 'burma', 'generator', 'mamluks', 'roadshow', 'utilization', 'johnstone', 'eureka', 'circumstances', 'legrand', 'meteorology', 'dex', 'supplementary', 'lswr', 'wilcox', 'mitral', 'meltdown', 'planktonic', 'congressman', 'zimmerman', 'stork', 'reproduce', 'genital', 'realities', 'bancroft', 'huw', 'bishkek', 'retained', 'pencils', 'collide', 'cumbia', 'transmitter', 'handset', 'sacha', 'clapp', 'fsu', 'reinhart', 'bibliotheca', 'levinson', 'favorites', 'longing', 'fenner', 'bouvier', 'disguising', 'baer', 'mari', 'velvet', 'trujillo', 'bulk', 'banknotes', 'fukuoka', 'bharatiya', 'aquarius', 'shotgun', 'adjectives', 'librarians', 'lorentz', 'gall', 'benefited', 'reorganize', 'pinkerton', 'prieto', 'stelae', 'argento', 'ade', 'treat', 'phone', 'bogged', 'virgen', 'downloaded', 'xiao', 'mullins', 'entries', 'samadhi', 'aluminum', 'leamington', 'khans', 'anhalt', 'dysfunction', 'dangers', 'anxious', 'bentinck', 'helsinki', 'nouveau', 'ewa', 'composition', 'congregational', 'aiken', 'unfair', 'risen', 'predictor', 'oshkosh', 'verbal', 'sixtus', 'rincon', 'hel', 'maynard', 'offbeat', 'lausanne', 'lithgow', 'constructs', 'seriousness', 'disorganized', 'fugitives', 'crumbled', 'precincts', 'piloting', 'knicks', 'sind', 'ratan', 'widowed', 'incorporation', 'abomination', 'pittsburg', 'hermetic', 'peng', 'wahl', 'pussy', 'specialty', 'doomsday', 'hec', 'radial', 'flip', 'porfirio', 'caster', 'patterns', 'honourable', 'emt', 'woodworking', 'zygmunt', 'objectives', 'parchment', 'greenlandic', 'diocesan', 'fer', 'winding', 'lyrical', 'sprays', 'tou', 'islamabad', 'dou', 'mattered', 'resigns', 'nfc', 'macbeth', 'achievable', 'phonology', 'mild', 'malawian', 'japonica', 'sanremo', 'kootenay', 'unfavorable', 'lora', 'graded', 'tracey', 'palliative', 'nantwich', 'trevor', 'booted', 'worcester', 'prevails', 'zhongshu', 'daniel', 'umbria', 'literate', 'champ', 'depaul', 'bathurst', 'wit', 'chaste', 'canard', 'padding', 'impersonator', 'fisheries', 'perfectly', 'matthias', 'sinfonietta', 'ricard', 'atholl', 'moser', 'stationery', 'steadfast', 'medial', 'vanity', 'ru', 'interspersed', 'antrim', 'nigger', 'unusable', 'projecting', 'armaments', 'interpretation', 'unpopularity', 'docudrama', 'biddle', 'dividend', 'overcame', 'cartels', 'pete', 'determining', 'ula', 'lambton', 'brontë', 'arbuckle', 'guernsey', 'atheists', 'lancers', 'inertia', 'colombian', 'diagnosis', 'dem', 'fabre', 'pt', 'galloping', 'unjustified', 'pour', 'wcc', 'io', 'æthelred', 'markup', 'albanians', 'mcintosh', 'frontline', 'elusive', 'leggett', 'weimar', 'exposé', 'namibia', 'enzymatic', 'handily', 'comprehensively', 'encyclopaedia', 'chs', 'vitis', 'yakuza', 'mer', 'perjury', 'septimus', 'papacy', 'locomotion', 'azarenka', 'presbyterians', 'buttresses', 'vowel', 'mondale', 'yano', 'mist', 'individualistic', 'garrett', 'piling', 'ha', 'classifier', 'astrologer', 'oude', 'hateful', 'demarcated', 'convent', 'leung', 'kant', 'cfa', 'iterations', 'parted', 'groundwater', 'durrell', 'operationally', 'sakamoto', 'navigated', 'wept', 'philosophical', 'ashoka', 'sap', 'randal', 'underneath', 'counselor', 'madera', 'pdc', 'bonsai', 'expatriate', 'framed', 'calderón', 'sequoia', 'accompaniment', 'gagarin', 'omnivorous', 'ceos', 'levine', 'foal', 'blair', 'equitable', 'relics', 'cheers', 'ironclad', 'gecko', 'wavelength', 'osborn', 'thorny', 'organisms', 'ballerina', 'nunatak', 'barbican', 'laughter', 'rifts', 'møller', 'lovat', 'compassion', 'amiable', 'reversal', 'hawks', 'inhibits', 'motivation', 'southgate', 'batten', 'kpmg', 'caracalla', 'gosford', 'garter', 'abuts', 'pantera', 'reproducing', 'placid', 'harlequin', 'moravia', 'dazzling', 'uneducated', 'caldwell', 'tolerated', 'additive', 'endures', 'margraviate', 'swirl', 'daddy', 'waltham', 'harney', 'subs', 'lepage', 'vernon', 'avid', 'dedication', 'almonds', 'raja', 'parables', 'aphids', 'lecithoceridae', 'riker', 'seceded', 'parliaments', 'rit', 'oxide', 'assuming', 'livery', 'imposition', 'mistral', 'ewan', 'muscogee', 'grandfathered', 'compulsory', 'mons', 'elgar', 'jagiellonian', 'coverings', 'fathom', 'licensing', 'turntables', 'freezing', 'talkies', 'degas', 'corinth', 'bartók', 'democratization', 'accompany', 'tvs', 'hossain', 'sworn', 'mayes', 'fuerza', 'sainte', 'travellers', 'staunch', 'clad', 'sagan', 'afi']\n",
      "male ['different', 'calls', 'tank', 'balmain', 'public', 'allosteric', 'availability', 'appellant', 'facing', 'ingeborg', 'benefice', 'shutout', 'lengths', 'notwithstanding', 'catering', 'margot', 'butterfly', 'props', 'laryngeal', 'chilean', 'lifetimes', 'fans', 'outside', 'alp', 'pottery', 'timorese', 'northern', 'race', 'blunt', 'bill', 'breeches', 'conceptual', 'syfy', 'logo', 'arad', 'exporters', 'delicate', 'dharwad', 'veil', 'contacted', 'regarding', 'player', 'charts', 'stairs', 'pip', 'recoverable', 'sprang', 'untreated', 'appeared', 'vectors', 'relating', 'reconnaissance', 'habsburg', 'races', 'seo', 'albums', 'camila', 'organic', 'fixation', 'burial', 'frederica', 'foreign', 'christianity', 'josie', 'hegel', 'verbandsgemeinde', 'national', 'deduce', 'blanca', 'arts', 'highway', 'kumar', 'cystic', 'crowd', 'grove', 'staffel', 'front', 'posted', 'patrimony', 'rotor', 'training', 'soups', 'starfish', 'deadlines', 'herself', 'common', 'hamlet', 'blade', 'outlaws', 'tyrol', 'mombasa', 'celebrities', 'perce', 'helped', 'matchups', 'rossa', 'edges', 'jellicoe', 'sphingidae', 'monopolies', 'blotches', 'jacobean', 'congo', 'self', 'researcher', 'engel', 'drives', 'concentration', 'mariners', 'abolition', 'zack', 'yearbook', 'kirov', 'deco', 'veto', 'embellished', 'decides', 'nanda', 'bermondsey', 'whittle', 'culture', 'leigh', 'haughey', 'telegrams', 'josephus', 'instead', 'despite', 'galápagos', 'movie', 'starred', 'gum', 'bulgarians', 'nubian', 'telescopes', 'graduation', 'endemic', 'bearings', 'kind', 'grigory', 'organises', 'multiples', 'epp', 'safer', 'persona', 'asquith', 'riaj', 'hamilton', 'champions', 'municipality', 'competence', 'bangladeshi', 'romantically', 'defeat', 'workers', 'brooding', 'fgm', 'haploid', 'hindmarsh', 'diacritics', 'species', 'categories', 'commuter', 'recipes', 'undated', 'swelling', 'hearings', 'ecuadorian', 'poli', 'mains', 'khosrow', 'minor', 'waterproof', 'torpedoed', 'godley', 'omni', 'shona', 'potent', 'holiday', 'jacques', 'cab', 'loftus', 'detailed', 'benton', 'isaiah', 'legendre', 'dwayne', 'whoever', 'annan', 'launch', 'overlooking', 'aa', 'target', 'actuators', 'honoree', 'whole', 'agents', 'usually', 'celestial', 'evening', 'godmother', 'quadrilateral', 'mit', 'dunham', 'yeshiva', 'macroeconomic', 'organ', 'tunstall', 'facilities', 'loops', 'theseus', 'shakes', 'malaysia', 'ova', 'shared', 'auto', 'laptops', 'medieval', 'soccer', 'dominance', 'resembled', 'christopher', 'dandy', 'soe', 'l', 'labour', 'patel', 'bwf', 'eleventh', 'nickelodeon', 'overwhelming', 'piedras', 'ayatollah', 'engagements', 'pigeon', 'sword', 'tohoku', 'kenny', 'burney', 'altars', 'polarization', 'axis', 'conjoined', 'rosalie', 'monotone', 'negligent', 'cured', 'lingam', 'renumbered', 'whitechapel', 'homosexuality', 'penetrate', 'turkish', 'martens', 'institutions', 'bucknell', 'edirne', 'pallet', 'clumsy', 'seafront', 'purpose', 'snack', 'chestnut', 'apologies', 'pei', 'platte', 'bridegroom', 'due', 'glare', 'yards', 'antioquia', 'agencies', 'singapore', 'denny', 'hungary', 'bbc', 'honour', 'attending', 'dream', 'animal', 'regarded', 'plot', 'padres', 'coding', 'caliphs', 'grunge', 'zelda', 'ferocious', 'negative', 'banished', 'buffalo', 'pours', 'lockers', 'deteriorating', 'books', 'exit', 'convicted', 'berliner', 'fontaine', 'landscaped', 'deny', 'extramarital', 'cassidy', 'bookmakers', 'affiliate', 'uniquely', 'magister', 'mohawk', 'pappas', 'arrange', 'bandits', 'busts', 'fk', 'cloister', 'about', 'doubles', 'limitation', 'blown', 'photograph', 'brownish', 'designer', 'compromise', 'monty', 'unfortunate', 'gerda', 'seeds', 'semigroup', 'accordance', 'molly', 'difficulty', 'equipping', 'spears', 'expanded', 'concepción', 'cta', 'squarepants', 'explode', 'dissolution', 'rules', 'emilie', 'committees', 'handling', 'conceive', 'devaluation', 'nuggets', 'spacious', 'jaw', 'iaf', 'promise', 'google', 'jenny', 'optic', 'spacecraft', 'antilles', 'money', 'braithwaite', 'desktop', 'prohibits', 'salaam', 'hotline', 'sandhya', 'coffers', 'lists', 'anglican', 'madero', 'jasper', 'reach', 'administrators', 'iman', 'forgives', 'leningrad', 'remodelling', 'hashtag', 'stabbed', 'barbosa', 'development', 'bruises', 'rostam', 'conducted', 'itc', 'allegiances', 'balthazar', 'canadian', 'biathlon', 'prost', 'wuhan', 'houseguests', 'collett', 'devotions', 'gaozu', 'concurred', 'meade', 'cherokee', 'balinese', 'belton', 'improvements', 'enemies', 'discontinuity', 'grenades', 'ile', 'perverse', 'corgan', 'urbanization', 'wheels', 'spice', 'rivers', 'br', 'sample', 'phil', 'mix', 'percival', 'spirit', 'reconstruction', 'geraldton', 'affirmed', 'splendor', 'future', 'debris', 'was', 'idiots', 'faustus', 'permanently', 'creeks', 'criterion', 'cloudy', 'prevent', 'arrondissements', 'aylmer', 'music', 'united', 'telugu', 'mtn', 'rhea', 'davos', 'breached', 'color', 'invade', 'connick', 'christiane', 'touched', 'elections', 'anonymous', 'kublai', 'passages', 'smart', 'saraswati', 'unwanted', 'prisons', 'northamptonshire', 'praxis', 'respectability', 'swivel', 'rivalry', 'rye', 'liable', 'kickoff', 'lupton', 'research', 'subsection', 'ceded', 'columbus', 'vodka', 'befriended', 'ziggler', 'pernambuco', 'hay', 'thickly', 'azov', 'compact', 'centerline', 'rudra', 'cultures', 'kde', 'scientifically', 'zone', 'assessed', 'pins', 'farm', 'eventual', 'kenyon', 'motorcycles', 'kepler', 'ideologies', 'contemporaries', 'earlier', 'collateral', 'fugitive', 'bloch', 'fahrenheit', 'lid', 'sphere', 'upton', 'faculties', 'oper', 'ajay', 'locating', 'suburban', 'whatever', 'endearing', 'pim', 'weeks']\n"
     ]
    }
   ],
   "source": [
    "female_most_biased = [vocab[i] for i in index_f]\n",
    "print(\"female\", female_most_biased) #switched again???\n",
    "male_most_biased = [vocab[i] for i in index_m]\n",
    "print(\"male\", male_most_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Hard Debias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector projection of a vector a on (or onto) a nonzero vector b, (also known as the vector component or vector resolution of a in the direction of b), is the orthogonal projection of a onto a straight line parallel to b. It is a vector parallel to b, defined as:\n",
    "\n",
    "${\\displaystyle \\mathbf a_{1}} =  a_{1} {\\mathbf {\\hat {b}} \\,} $\n",
    "\n",
    "where $a_{1}$ is a scalar, called the scalar projection of a onto b, and ${\\mathbf {\\hat {b}} \\,}$ is the unit vector in the direction of b.\n",
    "\n",
    "In turn, the scalar projection is defined as:[2]\n",
    "\n",
    "$    a_{1} =  a ⋅ {\\mathbf {\\hat {b}} \\,}= a ⋅ \\frac{b}{‖ b ‖} $\n",
    "\n",
    "where the operator ⋅ denotes a dot product, ‖b‖ is the length of b.\n",
    "\n",
    "The **vector component** or vector resolute of a perpendicular to b, sometimes also called the **vector rejection of a from b** is the orthogonal projection of a onto the plane (or, in general, hyperplane) orthogonal to b. Both the projection $a_{1}$ and rejection $a_{2}$ of a vector a are vectors, and their sum is equal to a,[1] which implies that the rejection is given by: \n",
    "\n",
    "$a_{2} = a − a_{1}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_hard_debias(embedding, embedding_neutral, id_neutral, index_m, index_f):\n",
    "    \"\"\"\n",
    "    Double Hard Debias as proposed by Wang et. al.:\n",
    "    \n",
    "    takes\n",
    "    embedding: all embeddings\n",
    "    embedding_neutral: subset of embeddings that the bias should be removed from\n",
    "    index_m: indices of most biased male words \n",
    "    index_f: indices of most biased female words \n",
    "    \n",
    "    returns\n",
    "    double_debias: full set of double-debiased embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    # create word lists of most biased male and female words\n",
    "    males = np.asarray([embedding[i] for i in index_m])\n",
    "    females = np.asarray([embedding[i] for i in index_f])    \n",
    "    \n",
    "    # decentralize all of the embeddings and store seperately\n",
    "    words_decen = np.zeros((len(embedding),300), dtype='float32') # chose a smaller data type due to memory error\n",
    "    words_neutral_decen = np.zeros((len(embedding_neutral),300), dtype='float32')\n",
    "    # first calculate mean over full vocab\n",
    "    mu = ((len(embedding)**(-1)) * np.sum(embedding, axis=0)).reshape(1,300)\n",
    "    # then subtract mean from each word embedding\n",
    "    for index, emb in enumerate(embedding):\n",
    "        words_decen[index] = emb - mu\n",
    "        \n",
    "    for index, emb in enumerate(embedding_neutral):\n",
    "        words_neutral_decen[index] = emb - mu\n",
    "    \n",
    "    #print(\"decentralized:\",words_decen.shape)\n",
    "    #print(\"origin:\",words)\n",
    "        \n",
    "    # discover the frequency direction    \n",
    "    # for all decentralized embeddings: compute PCA\n",
    "    #print(\"Principal Components:\",princ_comp)\n",
    "    pca_freq = PCA().fit(words_decen)\n",
    "    # print(pca.components_)\n",
    "    \n",
    "    evaluations = []\n",
    "\n",
    "    # in implementation of paper only consider 20 first PCs\n",
    "    for i, pc in enumerate(pca_freq.components_):\n",
    "        if i < 20:\n",
    "            male_proj = np.zeros((len(males),300))\n",
    "            male_debias = np.zeros((len(males),300))\n",
    "            female_proj = np.zeros((len(females),300))\n",
    "            female_debias = np.zeros((len(females),300))\n",
    "    \n",
    "        \n",
    "            #pc= pc.reshape(1,300)\n",
    "        \n",
    "            # remove PC direction and gender direction from all embeddings\n",
    "            for index, male in enumerate(males):\n",
    "                \n",
    "                # remove direction of current PC\n",
    "                male_proj[index] = w_orth(male, pc)\n",
    "                # remove gender direction: hard debias\n",
    "                male_debias[index] = w_orth(male_proj[index], gender_subspace)\n",
    "            \n",
    "            # repeat for female-biased words\n",
    "            for index, female in enumerate(females):\n",
    "            \n",
    "                female_proj[index] = w_orth(female, pc)\n",
    "                female_debias[index] = w_orth(female_proj[index], gender_subspace)\n",
    "    \n",
    "            # apply Neighbourhood Metric\n",
    "            # compute gender alignment accuracy for each PC\n",
    "            evaluations.append(align_acc(male_debias, female_debias))\n",
    "            \n",
    "    print(\"eval:\",evaluations)\n",
    "    \n",
    "    # evaluate which PC-rejection leads to most random cluster = evaluation smallest (closest to 0.5) \n",
    "    # in original paper corresponded to second PC    \n",
    "    print(np.argmin(evaluations))\n",
    "    best_pc = pca_freq.components_[np.argmin(evaluations)]\n",
    "\n",
    "    first_debias = np.zeros((embedding.shape))\n",
    "    first_neutral_debias = []\n",
    "    # remove best PC-direction from all neutral, decentralized words\n",
    "    for index,word in enumerate(words_decen):\n",
    "        \n",
    "        if index in id_neutral:\n",
    "            first_debias[index] = w_orth(word, best_pc)\n",
    "            first_neutral_debias.append(w_orth(word, best_pc))\n",
    "        else:\n",
    "            first_debias[index] = unit_vec(word)\n",
    "           \n",
    "    first_neutral_debias = np.asarray(first_neutral_debias)\n",
    "    # print(first_neutral_debias.shape, first_neutral_debias.shape == embedding_neutral.shape)\n",
    "    #double_debias = np.zeros((words.shape))\n",
    "    #for index,word in enumerate(first_debias):\n",
    "    # double_debias[index] = hard_debias(word)\n",
    "    \n",
    "    # apply HardDebias to all neutral, once debiased, words\n",
    "    double_neutral_debias = hard_debias_2(first_neutral_debias)\n",
    "    \n",
    "    double_debias = first_debias.copy()\n",
    "    \n",
    "    for index, word in enumerate(double_neutral_debias):\n",
    "        double_debias[id_neutral[index]] = word\n",
    "\n",
    "    return double_debias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vec(vector):\n",
    "    \"\"\"calculates unit vector of passed vector\"\"\"\n",
    "    \n",
    "    unit = np.linalg.norm(vector)\n",
    "    if unit != 0 and np.isnan(unit) == False :\n",
    "        return vector/unit\n",
    "    return vector   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender alignment accuracy/ Neighborhood Metric:\n",
    "def align_acc(males, females, k=2):\n",
    "    \"\"\"bias measurement using KMeans Clustering\n",
    "    \n",
    "    takes\n",
    "    males: male words' embeddings\n",
    "    females: female words'embeddings\n",
    "    k: number of clusters to create\n",
    "    ground truth labels:\n",
    "    0 = male,\n",
    "    1 = female\n",
    "    \n",
    "    returns\n",
    "    alignment: alignment accuracy after clustering the embeddings according to gender\n",
    "    \"\"\"\n",
    "    # print(males.shape, females.shape)\n",
    "    array_m_f = np.concatenate((males,females))\n",
    "    #print(array_m_f)\n",
    "    \n",
    "    #need: k (=1000) most biased female and male word's embedding (cosine similarity embedding & gender direction),\n",
    "    # perform KMeans on embeddings with k=2\n",
    "    kmeans = KMeans(n_clusters=k).fit(array_m_f)\n",
    "    split = males.shape[0]\n",
    "    \n",
    "    # print(kmeans.labels_)\n",
    "    # print(split)\n",
    "    correct = 0\n",
    "    #print(kmeans.labels_)\n",
    "   \n",
    "    # compute alignment score: cluster assignment vs ground truth gender label\n",
    "    for i in range(array_m_f.shape[0]):\n",
    "        # correct clustering if word was assigned its ground truth label\n",
    "        if i < split and kmeans.labels_[i] == 0:\n",
    "            correct+= 1\n",
    "        elif i >= split and kmeans.labels_[i] == 1:\n",
    "            correct += 1\n",
    "    \n",
    "    # alignment score = max(a, 1-a)\n",
    "    alignment = 1/(2*array_m_f.shape[0]) * correct\n",
    "    alignment = np.maximum(alignment, 1-alignment)\n",
    "    \n",
    "    return alignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional inputs: words to neutralize $N\\subseteq W$, family of equality sets $\\mathcal{E} = \\{E_1, E_2, ..., E_m\\}$ where each $E_i \\subseteq W$. For each word $w \\in N$, let $\\vec{w}$ be re-embedded to $\\vec{w}:=(\\vec{w}-\\vec{w}_B/||\\vec{w}-\\vec{w}_B||$. For each set $E\\in \\mathcal{E}$, let $\\mu:=\\sum_{w\\in E}w/|E|$ and $v:=\\mu-\\mu_B$. For each $w \\in E$, $\\vec{w}:=v+\\sqrt{1-||v||^2}\\frac{\\vec{w}_B-\\mu_B}{||\\vec{w}_B-\\mu_B||}$. Finally, output the subspace $B$ and the new embedding $\\{\\vec{w}\\in\\mathbb{R}^d\\}_{w\\in W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_debias (word_emb, equality_sets=equalize_pairs, B=gender_subspace):\n",
    "    \"\"\"performs hard debias on a word embedding to neutralize it,\n",
    "    \n",
    "    takes \n",
    "    word_emb: word embedding of the word to be neutralized,\n",
    "    equalize_pairs: equality pairs, each neutral word should be equidistant to all words in each equality set\n",
    "    B: the bias subspace\n",
    "    \n",
    "    returns\n",
    "    B: the bias subspace\n",
    "    new_word_emb: the new embedding for word_emb\n",
    "    \"\"\"\n",
    "    \n",
    "    # if word_emb is a single embedding:\n",
    "        # w_orth(word_emb)\n",
    "        \n",
    "    # if word_emb is the embeddings of all words to be neutralized:\n",
    "    \n",
    "    new_word_emb = np.zeros((word_emb.shape))\n",
    "    for i, embedding in enumerate(word_emb):\n",
    "        new_word_emb[i] = w_orth(embedding, B)\n",
    "        \n",
    "    for equal_set in equality_sets:\n",
    "        if equal_set[0] in w2id and equal_set[1] in w2id:\n",
    "            mean = (embed(equal_set[0]) + embed(equal_set[1])) / 2\n",
    "            mean_biased = mean - w_orth(mean, B)\n",
    "            v = mean - mean_biased # what is the biased mean?\n",
    "            for word in equal_set:\n",
    "                # print(word)\n",
    "                word_biased = embed(word) - w_orth(embed(word), B)\n",
    "                # new_embed = v + np.sqrt(1 - (np.linalg.norm(v)) ** 2) * ((word_biased - mean_biased) / unit_vec(word_biased - mean_biased))\n",
    "                new_embed = v * ((word_biased - mean_biased) / unit_vec(word_biased - mean_biased))\n",
    "    \n",
    "    return new_word_emb#, B im paper steht, dass auch B returned werden soll, aber das macht hier keinen Sinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_debias_2 (word_emb, equality_sets=female_male_pairs, B=gender_subspace):\n",
    "    \"\"\"performs hard debias on a word embedding to neutralize it,\n",
    "    \n",
    "    takes \n",
    "    word_emb: word embedding of the word to be neutralized,\n",
    "    equalize_pairs: equality pairs, each neutral word should be equidistant to all words in each equality set\n",
    "    B: the bias subspace\n",
    "    \n",
    "    returns\n",
    "    B: the bias subspace\n",
    "    new_word_emb: the new embedding for word_emb\n",
    "    \"\"\"\n",
    "    \n",
    "    # if word_emb is a single embedding:\n",
    "        # w_orth(word_emb)\n",
    "        \n",
    "    # if word_emb is the embeddings of all words to be neutralized:\n",
    "    \n",
    "    new_word_emb = np.zeros((word_emb.shape))\n",
    "    for i, embedding in enumerate(word_emb):\n",
    "        new_word_emb[i] = w_orth(embedding, B)\n",
    "        \n",
    "    for equal_set in equality_sets:\n",
    "        if equal_set[0] in w2id and equal_set[1] in w2id:\n",
    "            mean = (embed(equal_set[0]) + embed(equal_set[1])) / 2\n",
    "            mean_biased = mean - w_orth(mean, B)\n",
    "            v = mean - mean_biased # what is the biased mean?\n",
    "            # print(v)\n",
    "            for word in equal_set:\n",
    "                # print(word)\n",
    "                word_biased = embed(word) - w_orth(embed(word), B)\n",
    "                # print(np.linalg.norm(v))\n",
    "                # new_embed = v + np.sqrt(1 - (np.linalg.norm(v)) ** 2) * ((word_biased - mean_biased) / unit_vec(word_biased - mean_biased))\n",
    "                new_embed = v * ((word_biased - mean_biased) / unit_vec(word_biased - mean_biased))\n",
    "                \n",
    "    return new_word_emb#, B im paper steht, dass auch B returned werden soll, aber das macht hier keinen Sinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_orth (word_emb, direction):\n",
    "    \"\"\"\n",
    "    removes direction from word embedding by calculating the orthogonal word vector\n",
    "    \n",
    "    w_orth = w - (projection of w onto direction)\n",
    "    w_orth = w - (direction * (w dot direction))\n",
    "    \n",
    "    takes \n",
    "    word_emb: word to remove the direction from\n",
    "    direction: the direction to remove\n",
    "    \n",
    "    returns\n",
    "    unit vector embedding orthogonal to direction   \n",
    "    \"\"\"\n",
    "    \n",
    "    # formula from Bolukbasi et al. (2016)\n",
    "    new_word = word_emb - ((word_emb.dot(direction)) * direction)\n",
    "    \n",
    "\n",
    "    # print(new_word_emb_two, new_word_emb, new)\n",
    "    #print(new.shape)\n",
    "    \n",
    "    return unit_vec(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval: [0.7335, 0.8055, 0.6955, 0.806, 0.8055, 0.806, 0.806, 0.6945, 0.8055, 0.694, 0.6935, 0.806, 0.6955, 0.6945, 0.8055, 0.8055, 0.694, 0.694, 0.8049999999999999, 0.694]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "result_equal = double_hard_debias(embedding, embedding_neutral, id_neutral, index_m, index_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06019437 -0.06884434 -0.00223788 ...  0.01110984  0.0742712\n",
      "  -0.01431799]\n",
      " [ 0.03205562  0.01141418  0.04538446 ...  0.02068558  0.05873342\n",
      "  -0.06412243]\n",
      " [ 0.00690539 -0.00280085  0.07322954 ...  0.01731567  0.02018292\n",
      "  -0.01262799]\n",
      " ...\n",
      " [-0.02292979 -0.10056282  0.03456785 ... -0.00971185 -0.00824303\n",
      "   0.02183745]\n",
      " [-0.07453527 -0.01284823 -0.05090731 ...  0.0090728  -0.03314008\n",
      "   0.11597508]\n",
      " [ 0.17736915  0.08583836 -0.04222819 ...  0.0998133  -0.04829274\n",
      "  -0.01744922]]\n"
     ]
    }
   ],
   "source": [
    "print(result_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47974, 300)\n"
     ]
    }
   ],
   "source": [
    "print(result_equal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval: [0.7315, 0.8055, 0.8055, 0.694, 0.8075, 0.8069999999999999, 0.8065, 0.6955, 0.6945, 0.8055, 0.8065, 0.8055, 0.6945, 0.806, 0.8065, 0.6930000000000001, 0.8065, 0.694, 0.8045, 0.806]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "result_fem_male = double_hard_debias(embedding, embedding_neutral, id_neutral, index_m, index_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06065416 -0.07001813 -0.00137307 ...  0.01204657  0.07658218\n",
      "  -0.01018279]\n",
      " [ 0.03100812  0.00826786  0.04700006 ...  0.02320207  0.06229564\n",
      "  -0.06091475]\n",
      " [ 0.00488846 -0.00370413  0.07313233 ...  0.01804288  0.01926631\n",
      "  -0.01720095]\n",
      " ...\n",
      " [-0.02000681 -0.09549544  0.03232551 ... -0.01399882 -0.01292501\n",
      "   0.0201582 ]\n",
      " [-0.07017762 -0.00678335 -0.053362   ...  0.00423113 -0.03725102\n",
      "   0.11793896]\n",
      " [ 0.17678637  0.07927758 -0.03869781 ...  0.10594854 -0.03980051\n",
      "  -0.00765143]]\n"
     ]
    }
   ],
   "source": [
    "print(result_fem_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n"
     ]
    }
   ],
   "source": [
    "print(result_equal == result_fem_male)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Association Test\n",
    "Implementation taken from \"https://github.com/shivaomrani/HumanBiasInSemantics\". \n",
    "With minor adjustments such as variable names for readability, see file \"weat.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Career and family\n",
    "# Change from Bill to Tom as in paper to avoid ambiguity\n",
    "male_names = [\"john\", \"paul\", \"mike\", \"kevin\", \"steve\", \"greg\", \"jeff\", \"tom\"]\n",
    "female_names = [\"amy\", \"joan\", \"lisa\", \"sarah\", \"diana\", \"kate\", \"ann\", \"donna\"]\n",
    "career_attributes = [\"executive\", \"management\", \"professional\", \"corporation\", \"salary\", \"office\", \"business\", \"career\"]\n",
    "family_attributes = [\"home\", \"parents\", \"children\", \"family\", \"cousins\", \"marriage\", \"wedding\", \"relatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math and arts\n",
    "math_words = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\"]\n",
    "arts_words1 = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"sculpture\"]\n",
    "male_attributes1 = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
    "female_attributes1 = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Science and arts\n",
    "science_words = [\"science\", \"technology\", \"pyhsics\", \"chemistry\", \"einstein\", \"nasa\", \"experiment\", \"astronomy\"]\n",
    "arts_words2 = [\"poetry\", \"art\", \"shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\"]\n",
    "male_attributes2 = [\"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\"]\n",
    "female_attributes2 = [\"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"her\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference of means is  0.14427444139552204\n",
      "Generating null distribution...\n",
      "Number of permutations  100000\n",
      "Getting the entire distribution\n",
      "p-value:  0.00015728444615481507\n",
      "effect size:  1.805996189486473\n",
      "standard deviation:  0.04001119565930456\n"
     ]
    }
   ],
   "source": [
    "iterations = 100000\n",
    "wea_test = weat(male_names, female_names, career_attributes, family_attributes, iterations, embedding, w2id)\n",
    "pvalue, effect_size, sd = wea_test.getPValueAndEffect()\n",
    "print(\"p-value: \", pvalue)\n",
    "print(\"effect size: \", effect_size)\n",
    "print(\"standard deviation: \", sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word_analogy import analogy_tasks as ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 98.95% (283/286)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 94.69% (1409/1488)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 7.63% (18/236)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 77.49% (1855/2394)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 71.67% (301/420)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 9.25% (86/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 29.22% (135/462)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 78.68% (1048/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 46.55% (378/812)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 47.42% (441/930)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 93.23% (1418/1521)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 34.87% (544/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 75.46% (898/1190)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 45.57% (370/812)\n",
      "Questions seen/total: 73.54% (14373/19544)\n",
      "Semantic accuracy: 80.14%  (3866/4824)\n",
      "Syntactic accuracy: 55.69%  (5318/9549)\n",
      "Total accuracy: 63.90%  (9184/14373)\n"
     ]
    }
   ],
   "source": [
    "ana.evaluate_analogy_google(embedding, vocab, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 96.50% (276/286)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 97.31% (1448/1488)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 7.63% (18/236)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 73.43% (1758/2394)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 90.48% (380/420)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 20.32% (189/930)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 36.36% (168/462)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 85.44% (1138/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 56.03% (455/812)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 62.04% (577/930)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 94.41% (1436/1521)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 56.79% (886/1560)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 80.84% (962/1190)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 58.87% (478/812)\n",
      "Questions seen/total: 73.54% (14373/19544)\n",
      "Semantic accuracy: 80.43%  (3880/4824)\n",
      "Syntactic accuracy: 65.86%  (6289/9549)\n",
      "Total accuracy: 70.75%  (10169/14373)\n"
     ]
    }
   ],
   "source": [
    "ana.evaluate_analogy_google(result_equal, vocab, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4884\n",
      "ACCURACY TOP1-MSR: 54.40% (2657/4884)\n"
     ]
    }
   ],
   "source": [
    "ana.evaluate_analogy_msr(embedding, vocab, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
